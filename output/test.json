["Language","Models","are","Few-Shot","Learners","Tom","B.","Brown","%03","Benjamin","Mann","%03","Nick","Ryder","%03","Melanie","Subbiah","%03","Jared","Kaplan","y","Prafulla","Dhariwal","Arvind","Neelakantan","Pranav","Shyam","Girish","Sastry","Amanda","Askell","Sandhini","Agarwal","Ariel","Herbert-Voss","Gretchen","Krueger","Tom","Henighan","Rewon","Child","Aditya","Ramesh","Daniel","M.","Ziegler","Jeffrey","Wu","Clemens","Winter","Christopher","Hesse","Mark","Chen","Eric","Sigler","Mateusz","Litwin","Scott","Gray","Benjamin","Chess","Jack","Clark","Christopher","Berner","Sam","McCandlish","Alec","Radford","Ilya","Sutskever","Dario","Amodei","OpenAI","Abstract","Recent","work","has","demonstrated","substantial","gains","on","many","NLP","tasks","and","benchmarks","by","pre-training","on","a","large","corpus","of","text","followed","by","%EF%AC%81ne-tuning","on","a","speci%EF%AC%81c","task.","While","typically","task-agnostic","in","architecture%2C","this","method","still","requires","task-speci%EF%AC%81c","%EF%AC%81ne-tuning","datasets","of","thousands","or","tens","of","thousandsofexamples.","Bycontrast%2Chumanscangenerallyperformanewlanguagetaskfromonly","a","few","examples","or","from","simple","instructions","%E2%80%93","something","which","current","NLP","systems","still","largely","struggle","to","do.","Here","we","show","that","scaling","up","language","models","greatly","improves","task-agnostic%2C","few-shot","performance%2C","sometimes","even","reaching","competitiveness","with","prior","state-of-the-art","%EF%AC%81ne-","tuning","approaches.","Speci%EF%AC%81cally%2C","we","train","GPT-3%2C","an","autoregressive","language","model","with","175","billion","parameters%2C","10x","more","than","any","previous","non-sparse","language","model%2C","and","test","its","performance","in","the","few-shot","setting.","For","all","tasks%2C","GPT-3","is","applied","without","any","gradient","updates","or","%EF%AC%81ne-tuning%2C","withtasksandfew-shotdemonstrationsspeci%EF%AC%81edpurelyviatextinteractionwiththemodel.","GPT-3","achievesstrongperformanceon","manyNLPdatasets%2Cincludingtranslation%2C","question-answering%2Cand","clozetasks%2Caswellasseveraltasksthatrequireon-the-%EF%AC%82yreasoningordomainadaptation%2Csuchas","unscrambling","words%2C","using","a","novel","word","in","a","sentence%2C","or","performing","3-digit","arithmetic.","At","the","same","time%2C","we","also","identify","some","datasets","where","GPT-3%E2%80%99s","few-shot","learning","still","struggles%2C","as","well","as","some","datasets","whereGPT-3faces","methodological","issuesrelated","to","trainingon","large","web","corpora.","Finally%2C","we","%EF%AC%81nd","that","GPT-3","can","generate","samples","of","news","articles","which","human","evaluators","have","dif%EF%AC%81culty","distinguishingfromarticleswrittenbyhumans.","Wediscussbroadersocietalimpactsofthis%EF%AC%81nding","and","of","GPT-3","in","general.","%03","Equal","contribution","y","Johns","Hopkins","University%2C","OpenAI","Author","contributions","listed","at","end","of","paper",".","arXiv%3A2005.14165v4%20%20%5Bcs.CL%5D%20%2022%20Jul%202020","Contents","1","Introduction","3","2","Approach","6","2.1","Model","and","Architectures",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","8","2.2","Training","Dataset",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","8","2.3","Training","Process",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","9","2.4","Evaluation",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","10","3","Results","10","3.1","Language","Modeling%2C","Cloze%2C","and","Completion","Tasks",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","11","3.2","Closed","Book","Question","Answering",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","13","3.3","Translation",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","14","3.4","Winograd-Style","Tasks",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","16","3.5","Common","Sense","Reasoning",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","17","3.6","Reading","Comprehension",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","18","3.7","SuperGLUE",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","18","3.8","NLI",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","20","3.9","Synthetic","and","Qualitative","Tasks",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","21","4","Measuring","and","Preventing","Memorization","Of","Benchmarks","29","5","Limitations","33","6","Broader","Impacts","34","6.1","Misuse","of","Language","Models",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","35","6.2","Fairness%2C","Bias%2C","and","Representation",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","36","6.3","Energy","Usage",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".",".","39","7","Related","Work","39","8","Conclusion","40","A","Details","of","Common","Crawl","Filtering","43","B","Details","of","Model","Training","43","%20C%20","Details","of","Test","Set","Contamination","Studies","43","D","Total","Compute","Used","to","Train","Language","Models","46","E","Human","Quality","Assessment","of","Synthetic","News","Articles","46","F","Additional","Samples","from","GPT-3","48","%20G%20","Details","of","Task","Phrasing","and","Speci%EF%AC%81cations","50","H","Results","on","All","Tasks","for","All","Model","Sizes","63","2","1","Introduction","Recentyearshavefeaturedatrendtowardspre-trainedlanguagerepresentationsinNLPsystems%2Cappliedinincreasingly","%EF%AC%82exibleandtask-agnosticwaysfordownstreamtransfer.","First%2Csingle-layerrepresentationswerelearnedusingword","vectors%5B","MCCD13","%2C","PSM14","%5Dandfedtotask-speci%EF%AC%81carchitectures%2CthenRNNswithmultiplelayersofrepresentations","andcontextualstatewereusedtoformstrongerrepresentations%5B","DL15","%2C","MBXS17","%2C","PNZtY18","%5D(thoughstillappliedto","task-speci%EF%AC%81carchitectures)%2Candmorerecentlypre-trainedrecurrentortransformerlanguagemodels%5B","VSP","%2B","17","%5Dhave","been","directly","%EF%AC%81ne-tuned%2C","entirely","removing","the","need","for","task-speci%EF%AC%81c","architectures","%5B","RNSS18","%2C","DCLT18","%2C","HR18","%5D.","This","last","paradigm","has","led","to","substantial","progress","on","many","challenging","NLP","tasks","such","as","reading","comprehension%2C","question","answering%2C","textual","entailment%2C","and","many","others%2C","and","has","continued","to","advance","based","on","new","architectures","andalgorithms%5B","RSR","%2B","19","%2C","LOG","%2B","19","%2C","YDY","%2B","19","%2C","LCG","%2B","19","%5D.","However%2Camajorlimitationtothisapproachisthatwhile","the","architecture","is","task-agnostic%2C","there","is","still","a","need","for","task-speci%EF%AC%81c","datasets","and","task-speci%EF%AC%81c","%EF%AC%81ne-tuning%3A","to","achieve","strong","performance","on","a","desired","task","typically","requires","%EF%AC%81ne-tuning","on","a","dataset","of","thousands","to","hundreds","of","thousands","of","examples","speci%EF%AC%81c","to","that","task.","Removing","this","limitation","would","be","desirable%2C","for","several","reasons.","First%2C","from","a","practical","perspective%2C","the","need","for","a","large","dataset","of","labeled","examples","for","every","new","task","limits","the","applicability","of","language","models.","There","exists","a","very","wide","range","of","possible","useful","language","tasks%2C","encompassing","anything","from","correcting","grammar%2C","to","generating","examples","of","an","abstract","concept%2C","to","critiquing","a","short","story.","For","many","ofthesetasksitisdif%EF%AC%81culttocollectalargesupervisedtrainingdataset%2Cespeciallywhentheprocessmustberepeated","for","every","new","task.","Second%2C","the","potential","to","exploit","spurious","correlations","in","training","data","fundamentally","grows","with","the","expressiveness","of","the","model","and","the","narrowness","of","the","training","distribution.","This","can","create","problems","for","the","pre-training","plus","%EF%AC%81ne-tuningparadigm%2Cwheremodelsaredesignedtobelargetoabsorbinformationduringpre-training%2Cbutarethen","%EF%AC%81ne-tuned","on","very","narrow","task","distributions.","For","instance","%5B","HLW","%2B","20","%5D","observe","that","larger","models","do","not","necessarily","generalizebetterout-of-distribution.","Thereisevidencethatsuggeststhatthegeneralizationachievedunderthisparadigm","can","be","poor","because","the","model","is","overly","speci%EF%AC%81c","to","the","training","distribution","and","does","not","generalize","well","outside","it","%5B","YdC","%2B","19","%2C","MPL19","%5D.","Thus%2C","the","performance","of","%EF%AC%81ne-tuned","models","on","speci%EF%AC%81c","benchmarks%2C","even","when","it","is","nominally","at","human-level%2C","may","exaggerate","actual","performance","on","the","underlying","task","%5B","GSL","%2B","18","%2C","NK19","%5D.","Third%2C","humans","do","not","require","large","supervised","datasets","to","learn","most","language","tasks","%E2%80%93","a","brief","directive","in","natural","language","(e.g.","%E2%80%9Cplease","tell","me","if","this","sentence","describes","something","happy","or","something","sad%E2%80%9D)","or","at","most","a","tiny","number","of","demonstrations","(e.g.","%E2%80%9Chere","are","two","examples","of","people","acting","brave%3B","please","give","a","third","example","of","bravery%E2%80%9D)","is","often","Figure","1.1%3A","Language","model","meta-learning.","Duringunsupervisedpre-training%2Calanguagemodeldevelopsabroad","setofskillsandpatternrecognitionabilities.","Itthenusestheseabilitiesatinferencetimetorapidlyadapttoorrecognize","the","desiredtask.","Weuse","the","term%E2%80%9Cin-context","learning%E2%80%9Dto","describethe","innerloop","ofthis","process%2C","whichoccurs","within","the","forward-pass","upon","each","sequence.","The","sequences","in","this","diagram","are","not","intended","to","be","representative","of","the","data","a","modelwouldseeduringpre-training%2Cbutareintendedtoshowthattherearesometimesrepeatedsub-tasksembedded","within","a","single","sequence.","3","Figure1.2%3A","Largermodelsmakeincreasinglyef%EF%AC%81cientuseofin-contextinformation.","Weshowin-contextlearning","performanceonasimpletaskrequiringthemodeltoremoverandomsymbolsfromaword%2Cbothwithandwithouta","naturallanguagetaskdescription(seeSec.","3.9.2",").","Thesteeper%E2%80%9Cin-contextlearningcurves%E2%80%9Dforlargemodelsdemonstrate","improved","ability","to","learn","a","task","from","contextual","information.","We","see","qualitatively","similar","behavior","across","a","wide","range","of","tasks.","suf%EF%AC%81cient","to","enable","a","human","to","perform","a","new","task","to","at","least","a","reasonable","degree","of","competence.","Aside","from","pointing","to","a","conceptual","limitation","in","our","current","NLP","techniques%2C","this","adaptabilityhas","practical","advantages","%E2%80%93","it","allows","humans","to","seamlessly","mix","together","or","switch","between","many","tasks","and","skills%2C","for","example","performing","addition","during","a","lengthy","dialogue.","To","be","broadly","useful%2C","we","would","someday","like","our","NLP","systems","to","have","this","same","%EF%AC%82uidity","and","generality.","One","potential","route","towards","addressing","these","issues","is","meta-learning","1","%E2%80%93","which","in","the","context","of","language","models","means","themodeldevelopsabroadsetofskillsandpatternrecognitionabilitiesattrainingtime%2Candthenusesthoseabilities","atinferencetimetorapidlyadapttoorrecognizethedesiredtask(illustratedinFigure","1.1",").","Recentwork%5B","RWC","%2B","19","%5D","attempts","to","do","this","via","what","we","call","%E2%80%9Cin-context","learning%E2%80%9D%2C","using","the","text","input","of","a","pretrained","language","model","as","a","form","of","task","speci%EF%AC%81cation%3A","the","model","is","conditioned","on","a","natural","language","instruction","and%2For","a","few","demonstrations","of","the","task","and","is","then","expected","to","complete","further","instances","of","the","task","simply","by","predicting","what","comes","next.","Whileithasshownsomeinitialpromise%2Cthisapproachstillachievesresultsfarinferiorto%EF%AC%81ne-tuning%E2%80%93forexample","%5B","RWC","%2B","19","%5D","achieves","only","4%25","on","Natural","Questions%2C","and","even","its","55","F1","CoQa","result","is","now","more","than","35","points","behind","thestateoftheart.","Meta-learningclearlyrequiressubstantialimprovementinordertobeviableasapracticalmethodof","solving","language","tasks.","Another","recent","trend","in","language","modeling","may","offer","a","way","forward.","In","recent","years","the","capacity","of","transformer","language","models","has","increased","substantially%2C","from","100","million","parameters","%5B","RNSS18","%5D%2C","to","300","million","parameters","%5B","DCLT18","%5D%2Cto1.5billionparameters%5B","RWC","%2B","19","%5D%2Cto8billionparameters%5B","SPP","%2B","19","%5D%2C11billionparameters%5B","RSR","%2B","19","%5D%2C","and","%EF%AC%81nally","17","billion","parameters","%5B","Tur20","%5D.","Each","increase","has","broughtimprovements","in","text","synthesis","and%2For","downstream","NLP","tasks%2C","and","there","is","evidence","suggesting","that","log","loss%2C","which","correlates","well","with","many","downstream","tasks%2C","follows","a","smooth","trend","of","improvement","with","scale","%5B","KMH","%2B","20","%5D.","Since","in-context","learning","involves","absorbing","many","skills","and","taskswithintheparametersofthemodel%2Citisplausiblethatin-contextlearningabilitiesmightshowsimilarlystrong","gains","with","scale.","1","Inthecontextoflanguagemodelsthishassometimesbeencalled%E2%80%9Czero-shottransfer%E2%80%9D%2Cbutthistermispotentiallyambiguous%3A","the","method","is","%E2%80%9Czero-shot%E2%80%9D","in","the","sense","that","no","gradient","updates","are","performed%2C","but","it","often","involves","providing","inference-time","demonstrations","to","the","model%2C","so","is","not","truly","learning","from","zero","examples.","To","avoid","this","confusion%2C","we","use","the","term","%E2%80%9Cmeta-learning%E2%80%9D","to","capture","the","inner-loop","%2F","outer-loop","structure","of","the","general","method%2C","and","the","term","%E2%80%9Cin","context-learning%E2%80%9D","to","refer","to","the","inner","loop","of","meta-learning.","We","further","specialize","the","description","to","%E2%80%9Czero-shot%E2%80%9D%2C","%E2%80%9Cone-shot%E2%80%9D%2C","or","%E2%80%9Cfew-shot%E2%80%9D","depending","on","how","many","demonstrationsareprovidedatinferencetime.","Thesetermsareintendedtoremainagnosticonthequestionofwhetherthemodel","learns","new","tasks","from","scratch","at","inference","time","or","simply","recognizes","patterns","seen","during","training","%E2%80%93","this","is","an","important","issue","which","wediscusslaterinthepaper%2Cbut%E2%80%9Cmeta-learning%E2%80%9Disintendedtoencompassbothpossibilities%2Candsimplydescribestheinner-outer","loop","structure.","4","Figure","1.3%3A","Aggregate","performance","for","all","42","accuracy-denominated","benchmarks","While","zero-shot","performance","improves","steadily","with","model","size%2C","few-shot","performance","increases","more","rapidly%2C","demonstrating","that","larger","models","are","more","pro%EF%AC%81cient","at","in-context","learning.","See","Figure","3.8","for","a","more","detailed","analysis","on","SuperGLUE%2C","a","standard","NLP","benchmark","suite.","In","this","paper%2C","we","test","this","hypothesis","by","training","a","175","billion","parameter","autoregressive","language","model%2C","which","we","call","GPT-3%2Candmeasuringitsin-contextlearningabilities.","Speci%EF%AC%81cally%2CweevaluateGPT-3onovertwodozenNLPdatasets%2C","as","well","as","several","novel","tasks","designed","to","test","rapid","adaptation","to","tasks","unlikely","to","be","directly","contained","in","the","training","set.","For","each","task%2C","we","evaluate","GPT-3","under","3","conditions%3A","(a)","%E2%80%9Cfew-shot","learning%E2%80%9D%2C","or","in-context","learning","where","we","allow","as","many","demonstrations","as","will","%EF%AC%81t","into","the","model%E2%80%99s","context","window","(typically","10","to","100)%2C","(b)","%E2%80%9Cone-shot","learning%E2%80%9D%2C","whereweallowonlyonedemonstration%2Cand(c)%E2%80%9Czero-shot%E2%80%9Dlearning%2Cwherenodemonstrationsareallowedandonly","aninstructioninnaturallanguageisgiventothemodel.","GPT-3couldalsoinprinciplebeevaluatedinthetraditional","%EF%AC%81ne-tuning","setting%2C","but","we","leave","this","to","future","work.","Figure","1.2","illustrates","the","conditions","we","study%2C","and","shows","few-shot","learning","of","a","simple","task","requiring","the","model","to","removeextraneoussymbolsfromaword.","Modelperformanceimproveswiththeadditionofanaturallanguagetask","description%2C","and","with","the","number","of","examples","in","the","model%E2%80%99s","context%2C","K",".","Few-shot","learning","also","improves","dramatically","withmodelsize.","Thoughtheresultsinthiscaseareparticularlystriking%2Cthegeneraltrendswithbothmodelsizeand","numberofexamplesin-contextholdformosttaskswestudy.","Weemphasizethatthese%E2%80%9Clearning%E2%80%9Dcurvesinvolveno","gradient","updates","or","%EF%AC%81ne-tuning%2C","just","increasing","numbers","of","demonstrations","given","as","conditioning.","Broadly%2C","on","NLP","tasks","GPT-3","achieves","promising","results","in","the","zero-shot","and","one-shot","settings%2C","and","in","the","the","few-shot","setting","is","sometimes","competitive","with","or","even","occasionally","surpasses","state-of-the-art","(despite","state-of-the-art","being","held","by%EF%AC%81ne-tunedmodels).","Forexample%2CGPT-3achieves81.5F1onCoQAinthezero-shotsetting%2C84.0F1onCoQAin","the","one-shot","setting%2C","85.0","F1","in","the","few-shot","setting.","Similarly%2C","GPT-3","achieves","64.3%25","accuracy","on","TriviaQA","in","the","zero-shot","setting%2C","68.0%25","in","the","one-shot","setting%2C","and","71.2%25","in","the","few-shot","setting%2C","the","last","of","which","is","state-of-the-art","relative","to","%EF%AC%81ne-tuned","models","operating","in","the","same","closed-book","setting.","GPT-3","also","displays","one-shot","and","few-shot","pro%EF%AC%81ciency","at","tasks","designed","to","test","rapid","adaption","or","on-the-%EF%AC%82y","reasoning%2C","which","include","unscrambling","words%2C","performing","arithmetic%2C","and","using","novel","words","in","a","sentence","after","seeing","them","de%EF%AC%81ned","only","once.","We","also","show","that","in","the","few-shot","setting%2C","GPT-3","can","generate","synthetic","news","articles","which","human","evaluators","have","dif%EF%AC%81culty","distinguishing","from","human-generated","articles.","At","thesame","time%2C","we","also%EF%AC%81nd","some","taskson","which","few-shot","performancestruggles%2C","even","at","thescale","of","GPT-3.","This","includes","natural","language","inference","tasks","like","the","ANLI","dataset%2C","and","some","reading","comprehension","datasets","like","RACE","or","QuAC.","By","presenting","a","broad","characterization","of","GPT-3%E2%80%99s","strengths","and","weaknesses%2C","including","these","limitations%2C","we","hope","to","stimulate","study","of","few-shot","learning","in","language","models","and","draw","attention","to","where","progress","is","most","needed.","A","heuristic","sense","of","the","overall","results","can","be","seen","in","Figure","1.3","%2C","which","aggregates","the","various","tasks","(though","it","should","not","be","seen","as","a","rigorous","or","meaningful","benchmark","in","itself).","5","We","also","undertake","a","systematic","study","of","%E2%80%9Cdata","contamination%E2%80%9D","%E2%80%93","a","growing","problem","when","training","high","capacity","models","on","datasets","such","as","Common","Crawl%2C","which","can","potentially","include","content","from","test","datasets","simply","because","such","content","often","exists","on","the","web.","In","this","paper","we","develop","systematic","tools","tomeasure","data","contamination","and","quantify","its","distorting","effects.","Although","we","%EF%AC%81nd","that","data","contaminationhas","a","minimal","effect","on","GPT-3%E2%80%99s","performance","on","most","datasets%2Cwedoidentifyafewdatasetswhereitcouldbein%EF%AC%82atingresults%2Candweeitherdonotreportresultsonthese","datasets","or","we","note","them","with","an","asterisk%2C","depending","on","the","severity.","In","addition","to","all","the","above%2C","we","also","train","a","series","of","smaller","models","(ranging","from","125","million","parameters","to","13","billion","parameters)","inorder","to","comparetheir","performance","to","GPT-3","in","thezero%2C","one","and","few-shotsettings.","Broadly%2Cfor","most","tasks","we","%EF%AC%81nd","relatively","smooth","scaling","with","model","capacity","in","all","three","settings%3B","one","notable","pattern","is","that","the","gap","between","zero-%2C","one-%2C","and","few-shot","performance","often","grows","with","model","capacity%2C","perhaps","suggesting","that","larger","models","are","more","pro%EF%AC%81cient","meta-learners.","Finally%2C","given","the","broad","spectrum","of","capabilities","displayed","by","GPT-3%2C","we","discuss","concerns","about","bias%2C","fairness%2C","and","broader","societal","impacts%2C","and","attempt","a","preliminary","analysis","of","GPT-3%E2%80%99s","characteristics","in","this","regard.","Theremainderofthispaperisorganizedasfollows.","InSection","2","%2Cwedescribeourapproachandmethodsfortraining","GPT-3andevaluatingit.","Section","3","presentsresultsonthefullrangeoftasksinthezero-%2Cone-andfew-shotsettings.","Section","4","addresses","questions","of","data","contamination","(train-test","overlap).","Section","5","discusses","limitations","of","GPT-3.","Section","6","discusses","broader","impacts.","Section","7","reviews","related","work","and","Section","8","concludes.","2","Approach","Our","basic","pre-training","approach%2C","including","model%2C","data%2C","and","training%2C","is","similar","to","the","process","described","in","%5B","RWC","%2B","19","%5D%2C","with","relatively","straightforward","scaling","up","of","the","model","size%2C","dataset","size","and","diversity%2C","and","length","of","training.","Our","use","of","in-context","learning","is","also","similar","to","%5B","RWC","%2B","19","%5D%2C","but","in","this","work","we","systematically","explore","different","settings","for","learning","within","the","context.","Therefore%2C","we","start","this","section","by","explicitly","de%EF%AC%81ning","and","contrasting","the","different","settings","thatwewillbeevaluatingGPT-3onorcouldinprincipleevaluateGPT-3on.","Thesesettingscanbeseenaslyingona","spectrum","of","howmuch","task-speci%EF%AC%81c","data","they","tendto","rely","on.","Speci%EF%AC%81cally%2C","we","canidentify","at","least","four","pointson","this","spectrum","(see","Figure","2.1","for","an","illustration)%3A","%E2%80%A2","Fine-Tuning(FT)","has","been","the","most","common","approach","in","recent","years%2C","and","involves","updating","the","weights","of","a","pre-trained","model","by","training","on","a","supervised","dataset","speci%EF%AC%81c","to","the","desired","task.","Typically","thousands","to","hundreds","of","thousands","of","labeled","examples","are","used.","The","main","advantage","of","%EF%AC%81ne-tuning","is","strong","performance","on","many","benchmarks.","The","main","disadvantages","are","the","need","for","a","newlarge","dataset","for","every","task%2C","the","potential","for","poor","generalization","out-of-distribution","%5B","MPL19","%5D%2C","and","the","potential","to","exploit","spurious","features","of","the","training","data","%5B","GSL","%2B","18","%2C","NK19","%5D%2C","potentially","resulting","in","an","unfair","comparison","with","human","performance.","In","this","work","we","do","not","%EF%AC%81ne-tune","GPT-3","because","our","focus","is","on","task-agnostic","performance%2C","but","GPT-3","can","be","%EF%AC%81ne-tuned","in","principle","and","this","is","a","promising","direction","for","future","work.","%E2%80%A2","Few-Shot","(FS)","is","the","term","we","will","use","in","this","work","to","refer","to","the","setting","where","the","model","is","given","a","few","demonstrationsofthetaskatinferencetimeasconditioning%5B","RWC","%2B","19","%5D%2Cbutnoweightupdatesareallowed.","As","shown","in","Figure","2.1","%2C","for","a","typical","dataset","an","example","has","a","context","and","a","desired","completion","(for","example","an","English","sentence","and","the","French","translation)%2C","and","few-shot","works","by","giving","K","examples","of","context","and","completion%2Candthenone%EF%AC%81nalexampleofcontext%2Cwiththemodelexpectedtoprovidethecompletion.","We","typically","set","K","in","the","range","of","10","to","100","as","this","is","how","many","examples","can","%EF%AC%81t","in","the","model%E2%80%99s","context","window","(","n","ctx","%3D","2048",").","Themainadvantages","offew-shotare","amajorreductionin","theneedfor","task-speci%EF%AC%81c","dataand","reduced","potential","to","learn","an","overly","narrow","distribution","from","a","large","but","narrow","%EF%AC%81ne-tuning","dataset.","The","main","disadvantage","is","that","results","from","this","method","have","so","far","been","much","worse","than","state-of-the-art","%EF%AC%81ne-tuned","models.","Also%2C","a","small","amount","of","task","speci%EF%AC%81c","data","is","still","required.","As","indicated","by","the","name%2C","few-shot","learning","as","described","here","for","language","models","is","related","to","few-shot","learning","as","used","in","other","contexts","in","ML","%5B","HYC01","%2C","VBL","%2B","16","%5D","%E2%80%93","both","involve","learning","based","on","a","broad","distribution","of","tasks","(in","this","case","implicit","in","the","pre-training","data)","and","then","rapidly","adapting","to","a","new","task.","%E2%80%A2","One-Shot","(1S)","is","the","same","as","few-shot","except","that","only","one","demonstration","is","allowed%2C","in","addition","to","a","natural","language","description","of","the","task%2C","as","shown","in","Figure","1.","The","reason","to","distinguish","one-shot","from","few-shot","and","zero-shot(below)isthatitmostcloselymatchesthewayinwhichsometasksarecommunicatedtohumans.","For","example%2C","when","asking","humans","to","generate","a","dataset","on","a","human","worker","service","(for","example","Mechanical","Turk)%2Citiscommontogiveonedemonstrationofthetask.","Bycontrastitissometimesdif%EF%AC%81culttocommunicate","the","content","or","format","of","a","task","if","no","examples","are","given.","6","Figure","2.1%3A","Zero-shot%2C","one-shot","and","few-shot%2C","contrasted","with","traditional","%EF%AC%81ne-tuning",".","The","panels","above","show","four","methods","for","performing","a","task","with","a","language","model","%E2%80%93","%EF%AC%81ne-tuning","is","the","traditional","method%2C","whereas","zero-%2C","one-%2C","and","few-shot%2C","which","we","study","in","this","work%2C","require","the","model","to","perform","the","task","with","only","forward","passes","at","test","time.","Wetypicallypresentthemodelwithafewdozenexamplesinthefewshotsetting.","Exactphrasingsforalltask","descriptions%2C","examples","and","prompts","can","be","found","in","Appendix","%20G%20",".","%E2%80%A2","Zero-Shot(0S)","isthesameasone-shotexceptthatnodemonstrationsareallowed%2Candthemodelisonlygiven","a","natural","language","instruction","describing","the","task.","This","method","provides","maximum","convenience%2C","potential","for","robustness%2C","andavoidance","ofspurious","correlations(unless","they","occurvery","broadlyacross","thelarge","corpus","of","pre-training","data)%2C","but","is","also","the","most","challenging","setting.","In","some","cases","it","may","even","be","dif%EF%AC%81cult","for","humans","tounderstandtheformatofthetaskwithoutpriorexamples%2Csothissettingisinsomecases%E2%80%9Cunfairlyhard%E2%80%9D.","Forexample%2Cifsomeoneisaskedto%E2%80%9Cmakeatableofworldrecordsforthe200mdash%E2%80%9D%2Cthisrequestcanbe","ambiguous%2C","as","it","may","not","be","clear","exactly","what","format","the","table","should","have","or","what","should","be","included","(and","evenwith","careful","clari%EF%AC%81cation%2Cunderstandingpreciselywhatisdesired","canbedif%EF%AC%81cult).","Nevertheless%2Cforat","least","some","settings","zero-shot","is","closest","to","how","humans","perform","tasks","%E2%80%93","for","example%2C","in","the","translation","example","in","Figure","2.1","%2C","a","human","would","likely","know","what","to","do","from","just","the","text","instruction.","Figure","2.1","shows","the","four","methods","using","the","example","of","translating","English","to","French.","In","this","paper","we","focus","on","zero-shot%2C","one-shot","and","few-shot%2C","with","the","aim","of","comparing","them","not","as","competing","alternatives%2C","but","as","different","problemsettingswhichofferavaryingtrade-offbetweenperformanceonspeci%EF%AC%81cbenchmarksandsampleef%EF%AC%81ciency.","We","especially","highlight","the","few-shot","results","as","many","of","them","are","only","slightly","behind","state-of-the-art","%EF%AC%81ne-tuned","models.","Ultimately%2C","however%2C","one-shot%2C","or","even","sometimes","zero-shot%2C","seem","like","the","fairest","comparisons","to","human","performance%2C","and","are","important","targets","for","future","work.","Sections","2.1","-","2.3","belowgivedetailsonourmodels%2Ctrainingdata%2Candtrainingprocessrespectively.","Section","2.4","discusses","the","details","of","how","we","do","few-shot%2C","one-shot%2C","and","zero-shot","evaluations.","7","Model","Name","n","params","n","layers","d","model","n","heads","d","head","Batch","Size","Learning","Rate","GPT-3","Small","125M","12","768","12","64","0.5M","6","%3A","0","%02","10","%00","4","GPT-3","Medium","350M","24","1024","16","64","0.5M","3","%3A","0","%02","10","%00","4","GPT-3","Large","760M","24","1536","16","96","0.5M","2","%3A","5","%02","10","%00","4","GPT-3","XL","1.3B","24","2048","24","128","1M","2","%3A","0","%02","10","%00","4","GPT-3","2.7B","2.7B","32","2560","32","80","1M","1","%3A","6","%02","10","%00","4","GPT-3","6.7B","6.7B","32","4096","32","128","2M","1","%3A","2","%02","10","%00","4","GPT-3","13B","13.0B","40","5140","40","128","2M","1","%3A","0","%02","10","%00","4","GPT-3","175B","or","%E2%80%9CGPT-3%E2%80%9D","175.0B","96","12288","96","128","3.2M","0","%3A","6","%02","10","%00","4","Table","2.1%3A","Sizes%2Carchitectures%2Candlearninghyper-parameters(batchsizeintokensandlearningrate)ofthemodels","which","we","trained.","All","models","were","trained","for","a","total","of","300","billion","tokens.","2.1","Model","and","Architectures","We","use","the","same","model","and","architecture","as","GPT-2","%5B","RWC","%2B","19","%5D%2C","including","the","modi%EF%AC%81ed","initialization%2C","pre-normalization%2C","andreversibletokenizationdescribedtherein%2Cwiththeexceptionthatweusealternatingdenseandlocallybandedsparse","attentionpatternsinthelayersofthetransformer%2CsimilartotheSparseTransformer%5B","CGRS19","%5D.","Tostudythedependence","of","ML","performance","on","model","size%2C","we","train","8","differentsizes","of","model%2C","ranging","over","three","orders","ofmagnitude","from","125","million","parameters","to","175","billion","parameters%2C","with","the","last","being","the","model","we","call","GPT-3.","Previous","work","%5B","KMH","%2B","20","%5D","suggeststhatwithenoughtrainingdata%2Cscalingofvalidationlossshouldbeapproximatelyasmoothpowerlawasa","function","of","size%3B","training","models","of","many","different","sizes","allows","us","to","test","this","hypothesis","both","for","validation","loss","and","for","downstream","language","tasks.","Table","2.1","showsthesizesandarchitecturesofour8models.","Here","n","params","isthetotalnumberoftrainableparameters%2C","n","layers","is","the","total","number","of","layers%2C","d","model","is","the","number","of","units","in","each","bottleneck","layer","(we","always","have","the","feedforward","layer","four","times","the","size","of","the","bottleneck","layer%2C","d","%3D","4","%03","d","model",")%2C","and","d","head","is","the","dimension","of","each","attentionhead.","Allmodelsuseacontextwindowof","n","ctx","%3D","2048","tokens.","WepartitionthemodelacrossGPUsalong","both","the","depth","and","width","dimension","in","order","to","minimize","data-transfer","between","nodes.","The","precise","architectural","parametersforeachmodelarechosenbasedoncomputationalef%EF%AC%81ciencyandload-balancinginthelayoutofmodels","across","GPU%E2%80%99s.","Previous","work","%5B","KMH","%2B","20","%5D","suggests","that","validation","loss","is","not","strongly","sensitive","to","these","parameters","within","a","reasonably","broad","range.","2.2","Training","Dataset","Datasetsforlanguagemodelshaverapidlyexpanded%2CculminatingintheCommonCrawldataset","2","%5B","RSR","%2B","19","%5Dconstituting","nearlyatrillionwords.","Thissize","ofdatasetissuf%EF%AC%81cienttotrainourlargest","modelswithouteverupdatingonthesame","sequence","twice.","However%2C","we","have","found","that","un%EF%AC%81ltered","or","lightly","%EF%AC%81ltered","versions","of","Common","Crawl","tend","to","have","lower","quality","than","more","curated","datasets.","Therefore%2C","we","took","3","steps","to","improve","the","average","quality","of","our","datasets%3A","(1)","we","downloaded","and","%EF%AC%81ltered","a","version","of","CommonCrawl","based","on","similarity","to","a","range","of","high-quality","reference","corpora%2C","(2)","we","performed","fuzzy","deduplication","at","the","document","level%2C","within","and","across","datasets%2C","to","prevent","redundancy","andpreservetheintegrityofourheld-outvalidationsetasanaccuratemeasureofover%EF%AC%81tting%2Cand(3)wealsoadded","known","high-quality","reference","corpora","to","the","training","mix","to","augment","CommonCrawl","and","increase","its","diversity.","Detailsofthe%EF%AC%81rsttwopoints(processingofCommonCrawl)aredescribedinAppendix","A",".Forthethird%2Cweadded","several","curated","high-quality","datasets%2C","including","an","expanded","version","of","the","WebText","dataset","%5B","RWC","%2B","19","%5D%2C","collected","by","scraping","links","over","a","longer","period","of","time%2C","and","%EF%AC%81rst","described","in","%5B","KMH","%2B","20","%5D%2C","two","internet-based","books","corpora","(Books1","and","Books2)","and","English-language","Wikipedia.","Table","2.2","shows","the","%EF%AC%81nal","mixture","of","datasets","that","we","used","in","training.","The","CommonCrawl","data","was","downloaded","from","41","shards","of","monthly","CommonCrawl","covering","2016","to","2019%2C","constituting","45TB","of","compressed","plaintext","before","%EF%AC%81ltering","and570GBafter%EF%AC%81ltering%2Croughlyequivalentto400billionbyte-pair-encodedtokens.","Notethatduringtraining%2Cdatasets","arenot","sampledinproportionto","theirsize%2Cbut","ratherdatasetswe","viewashigher-quality","aresampledmore","frequently%2C","such","that","CommonCrawl","and","Books2","datasets","are","sampled","less","than","once","during","training%2C","but","the","other","datasets","are","sampled","2-3","times.","This","essentially","accepts","a","small","amount","of","over%EF%AC%81tting","in","exchange","for","higher","quality","training","data.","2","https%3A%2F%2Fcommoncrawl.org%2Fthe-data%2F","8","Figure2.2%3A","Totalcomputeusedduringtraining",".","BasedontheanalysisinScalingLawsForNeuralLanguageModels","%5B","KMH","%2B","20","%5D","we","train","much","larger","models","on","many","fewer","tokens","than","is","typical.","As","a","consequence%2C","although","GPT-3","3B","isalmost10xlargerthanRoBERTa-Large(355Mparams)%2Cbothmodelstookroughly50peta%EF%AC%82op%2Fs-daysofcompute","during","pre-training.","Methodology","for","these","calculations","can","be","found","in","Appendix","D",".","Dataset","Quantity","(tokens)","Weight","in","training","mix","Epochs","elapsed","when","training","for","300B","tokens","Common","Crawl","(%EF%AC%81ltered)","410","billion","60%25","0.44","WebText2","19","billion","22%25","2.9","Books1","12","billion","8%25","1.9","Books2","55","billion","8%25","0.43","Wikipedia","3","billion","3%25","3.4","Table","2.2%3A","Datasets","used","to","train","GPT-3",".","%E2%80%9CWeight","in","training","mix%E2%80%9D","refers","to","the","fraction","of","examples","during","training","thataredrawnfromagivendataset%2Cwhichweintentionallydonotmakeproportionaltothesizeofthedataset.","Asa","result%2C","when","we","train","for","300","billion","tokens%2C","some","datasets","are","seen","up","to","3.4","times","during","training","while","other","datasets","are","seen","less","than","once.","A","major","methodological","concern","with","language","models","pretrained","on","a","broad","swath","of","internet","data%2C","particularly","large","models","with","the","capacity","to","memorize","vast","amounts","of","content%2C","is","potential","contamination","of","downstream","tasks","by","havingtheirtestordevelopmentsetsinadvertentlyseenduringpre-training.","Toreducesuchcontamination%2Cwesearched","forandattemptedtoremoveanyoverlapswiththedevelopmentandtestsetsofallbenchmarksstudiedinthispaper.","Unfortunately%2Cabuginthe%EF%AC%81lteringcausedustoignoresomeoverlaps%2Candduetothecostoftrainingitwasnotfeasible","to","retrain","the","model.","In","Section","4","we","characterize","the","impact","of","the","remaining","overlaps%2C","and","in","future","work","we","will","more","aggressively","remove","data","contamination.","2.3","Training","Process","Asfoundin%5B","KMH","%2B","20","%2C","MKAT18","%5D%2Clargermodelscantypicallyusealargerbatchsize%2Cbutrequireasmallerlearning","rate.","We","measure","the","gradient","noise","scale","during","training","and","use","it","to","guide","our","choice","of","batch","size","%5B","MKAT18","%5D.","Table","2.1","shows","the","parameter","settings","we","used.","To","train","the","larger","models","without","running","out","of","memory%2C","we","use","a","mixture","ofmodelparallelismwithineachmatrixmultiplyandmodelparallelismacrossthelayersofthenetwork.","Allmodels","were","trained","on","V100","GPU%E2%80%99s","on","part","of","a","high-bandwidth","cluster","provided","by","Microsoft.","Details","of","the","training","process","and","hyperparameter","settings","are","described","in","Appendix","B",".","9","2.4","Evaluation","For","few-shot","learning%2C","we","evaluate","each","example","in","the","evaluation","set","by","randomly","drawing","K","examples","from","that","task%E2%80%99s","training","set","as","conditioning%2C","delimited","by","1","or","2","newlines","depending","on","the","task.","For","LAMBADA","and","Storycloze","there","is","no","supervised","training","set","available","so","we","draw","conditioning","examples","from","the","development","set","and","evaluate","on","the","test","set.","For","Winograd","(the","original%2C","not","SuperGLUE","version)","there","is","only","one","dataset%2C","so","we","draw","conditioning","examples","directly","from","it.","K","can","be","any","value","from","0","to","the","maximum","amount","allowed","by","the","model%E2%80%99s","context","window%2C","which","is","n","ctx","%3D","2048","forallmodelsandtypically%EF%AC%81ts","10","to","100","examples.","Largervaluesof","K","areusuallybutnotalwaysbetter%2Csowhena","separate","development","and","test","set","are","available%2C","we","experiment","with","a","few","values","of","K","on","the","development","set","and","then","run","the","best","value","on","the","test","set.","For","some","tasks","(see","Appendix","%20G%20",")","we","also","use","a","natural","language","prompt","in","addition","to","(or","for","K","%3D","0","%2C","instead","of)","demonstrations.","On","tasks","that","involve","choosing","one","correct","completion","from","several","options","(multiple","choice)%2C","we","provide","K","examples","of","context","plus","correct","completion%2C","followed","by","one","example","of","context","only%2C","and","compare","the","LM","likelihood","of","eachcompletion.","Formosttaskswecomparetheper-tokenlikelihood(tonormalizeforlength)%2Choweveronasmall","numberofdatasets(ARC%2COpenBookQA%2CandRACE)wegainadditionalbene%EF%AC%81tasmeasuredonthedevelopmentset","by","normalizing","by","the","unconditional","probability","of","each","completion%2C","by","computing","P","(completion","j","context)","P","(completion","j","answer","context)","%2C","where","answer","context","isthestring","%22Answer%3A","%22","or","%22A%3A","%22","andisusedtopromptthatthecompletionshouldbeananswer","but","is","otherwise","generic.","Ontasksthatinvolvebinaryclassi%EF%AC%81cation%2Cwegivetheoptionsmoresemanticallymeaningfulnames(e.g.","%E2%80%9CTrue%E2%80%9Dor","%E2%80%9CFalse%E2%80%9D","rather","than","0","or","1)","and","then","treat","the","task","like","multiple","choice%3B","we","also","sometimes","frame","the","task","similar","to","what","is","done","by","%5B","RSR","%2B","19","%5D","(see","Appendix","%20G%20",")","for","details.","Ontaskswithfree-formcompletion%2Cweusebeamsearchwiththesameparametersas%5B","RSR","%2B","19","%5D%3A","abeamwidthof4","andalengthpenaltyof","%3D","0","%3A","6",".","WescorethemodelusingF1similarityscore%2CBLEU%2Corexactmatch%2Cdependingon","what","is","standard","for","the","dataset","at","hand.","Final","results","are","reportedon","the","test","set","when","publicly","available%2C","foreach","model","size","and","learning","setting","(zero-%2C","one-%2C","and","few-shot).","When","the","test","set","is","private%2C","our","model","is","often","too","large","to","%EF%AC%81t","on","the","test","server%2C","so","we","report","results","on","the","development","set.","We","do","submit","to","the","test","server","on","a","small","number","of","datasets","(SuperGLUE%2C","TriviaQA%2C","PiQa)","where","we","were","able","to","make","submission","work%2C","and","we","submit","only","the","200B","few-shot","results%2C","and","report","development","set","results","for","everything","else.","3","Results","In","Figure","3.1","we","display","training","curves","for","the","8","models","described","in","Section","2",".","For","this","graph","we","also","include","6","additional","extra-small","models","with","as","few","as","100%2C000","parameters.","As","observed","in","%5B","KMH","%2B","20","%5D%2C","language","modeling","performancefollowsapower-lawwhenmakingef%EF%AC%81cientuseoftrainingcompute.","Afterextendingthistrendbytwo","more","orders","of","magnitude%2C","we","observe","only","a","slight","(if","any)","departure","from","the","power-law.","One","might","worry","that","these","improvements","in","cross-entropy","loss","come","only","from","modeling","spurious","details","of","our","training","corpus.","However%2C","we","will","see","in","the","following","sections","that","improvements","in","cross-entropy","loss","lead","to","consistent","performance","gains","across","a","broad","spectrum","of","natural","language","tasks.","Below%2C","we","evaluate","the","8","models","described","in","Section","2","(the","175","billion","parameter","parameter","GPT-3","and","7","smaller","models)","on","a","wide","range","of","datasets.","We","group","the","datasets","into","9","categories","representing","roughly","similar","tasks.","In","Section","3.1","we","evaluate","on","traditional","language","modeling","tasks","and","tasks","that","are","similar","to","language","modeling%2C","suchasClozetasksandsentence%2Fparagraphcompletiontasks.","InSection","3.2","weevaluateon%E2%80%9Cclosedbook%E2%80%9Dquestion","answering","tasks%3A","tasks","which","require","using","the","information","stored","in","the","model%E2%80%99s","parameters","to","answer","general","knowledgequestions.","InSection","3.3","weevaluatethemodel%E2%80%99sabilitytotranslatebetweenlanguages(especiallyone-shot","and","few-shot).","In","Section","3.4","we","evaluate","the","model%E2%80%99s","performance","on","Winograd","Schema-like","tasks.","In","Section","3.5","we","evaluate","on","datasets","that","involve","commonsense","reasoning","or","question","answering.","In","Section","3.6","we","evaluate","on","reading","comprehensiontasks%2CinSection","3.7","weevaluateontheSuperGLUEbenchmarksuite%2Candin","3.8","webrie%EF%AC%82yexplore","NLI.","Finally%2C","in","Section","3.9","%2C","we","invent","some","additional","tasks","designed","especially","to","probe","in-context","learning","abilities","%E2%80%93","thesetasksfocusonon-the-%EF%AC%82yreasoning%2Cadaptationskills%2Coropen-endedtextsynthesis.","Weevaluatealltasksinthe","few-shot%2C","one-shot%2C","and","zero-shot","settings.","10","Figure","3.1%3A","Smooth","scaling","of","performance","with","compute.","Performance","(measured","in","terms","of","cross-entropy","validation","loss)","follows","a","power-law","trend","with","the","amount","of","compute","used","for","training.","The","power-law","behavior","observed","in","%5B","KMH","%2B","20","%5D","continues","for","an","additional","two","orders","of","magnitude","with","only","small","deviations","from","the","predicted","curve.","For","this","%EF%AC%81gure%2C","we","exclude","embedding","parameters","from","compute","and","parameter","counts.","Setting","PTB","SOTA","(Zero-Shot)","35.8","a","GPT-3","Zero-Shot","20.5","Table","3.1%3A","Zero-shotresults","on","PTBlanguage","modeling","dataset.","Many","other","common","language","modeling","datasets","are","omitted","because","they","are","derived","from","Wikipedia","or","other","sources","which","are","included","in","GPT-3%E2%80%99s","training","data.","a","%5B","RWC","%2B","19","%5D","3.1","Language","Modeling%2C","Cloze%2C","and","Completion","Tasks","In","this","section","we","test","GPT-3%E2%80%99s","performance","on","the","traditional","task","of","language","modeling%2C","as","well","as","related","tasks","that","involve","predicting","a","single","word","of","interest%2C","completing","a","sentence","or","paragraph%2C","or","choosing","between","possible","completions","of","a","piece","of","text.","3.1.1","Language","Modeling","We","calculate","zero-shot","perplexity","on","the","Penn","Tree","Bank","(PTB)","%5B","MKM","%2B","94","%5D","dataset","measured","in","%5B","RWC","%2B","19","%5D.","We","omit","the","4","Wikipedia-related","tasks","in","that","work","because","they","are","entirely","contained","in","our","training","data%2C","and","we","also","omit","the","one-billion","word","benchmark","due","to","a","high","fraction","of","the","dataset","being","contained","in","our","training","set.","PTB","escapes","these","issues","due","to","predating","the","modern","internet.","Our","largest","model","sets","a","new","SOTA","on","PTB","by","a","substantial","margin","of","15","points%2C","achieving","a","perplexity","of","20.50.","Note","that","since","PTB","is","a","traditional","language","modeling","dataset","it","does","not","have","a","clear","separation","of","examples","to","de%EF%AC%81ne","one-shot","or","few-shot","evaluation","around%2C","so","we","measure","only","zero-shot.","3.1.2","LAMBADA","The","LAMBADA","dataset","%5B","PKL","%2B","16","%5D","tests","the","modeling","of","long-range","dependencies","in","text","%E2%80%93","the","model","is","asked","to","predict","the","last","word","of","sentences","which","require","reading","a","paragraph","of","context.","It","has","recently","been","suggested","that","the","continued","scaling","of","language","models","is","yielding","diminishing","returns","on","this","dif%EF%AC%81cult","benchmark.","%5B","BHT","%2B","20","%5D","re%EF%AC%82ect","on","thesmall1.5%25improvementachievedbyadoublingofmodelsizebetweentworecentstateoftheartresults(%5B","SPP","%2B","19","%5D","11","Setting","LAMBADA","(acc)","LAMBADA","(ppl)","StoryCloze","(acc)","HellaSwag","(acc)","SOTA","68.0","a","8.63","b","91.8","c","85.6","d","GPT-3","Zero-Shot","76.2","3.00","83.2","78.9","GPT-3","One-Shot","72.5","3.35","84.7","78.1","GPT-3","Few-Shot","86.4","1.92","87.7","79.3","Table3.2%3A","Performanceon","clozeandcompletion","tasks.","GPT-3signi%EF%AC%81cantlyimprovesSOTAonLAMBADAwhile","achieving","respectable","performance","on","two","dif%EF%AC%81cult","completion","prediction","datasets.","a","%5B","Tur20","%5D","b","%5B","RWC","%2B","19","%5D","c","%5B","LDL19","%5D","d","%5B","LCH","%2B","20","%5D","Figure","3.2%3A","OnLAMBADA%2Cthefew-shotcapabilityoflanguagemodelsresultsinastrongboosttoaccuracy.","GPT-3","2.7B","outperforms","the","SOTA","17B","parameter","Turing-NLG","%5B","Tur20","%5D","in","this","setting%2C","and","GPT-3","175B","advances","the","state","of","the","art","by","18%25.","Note","zero-shot","uses","a","different","format","from","one-shot","and","few-shot","as","described","in","the","text.","and","%5B","Tur20","%5D)","and","argue","that","%E2%80%9Ccontinuing","to","expand","hardware","and","data","sizes","by","orders","of","magnitude","is","not","the","path","forward%E2%80%9D.","We","%EF%AC%81nd","that","path","is","still","promising","and","in","a","zero-shot","setting","GPT-3","achieves","76%25","on","LAMBADA%2C","a","gain","of","8%25","over","the","previous","state","of","the","art.","LAMBADA","is","also","a","demonstration","of","the","%EF%AC%82exibility","of","few-shot","learning","as","it","provides","a","way","to","address","a","problem","that","classicallyoccurswiththisdataset.","AlthoughthecompletioninLAMBADAisalwaysthelastwordinasentence%2Ca","standardlanguagemodelhasnowayofknowingthisdetail.","Itthusassignsprobabilitynotonlytothecorrectendingbut","also","to","other","valid","continuations","of","the","paragraph.","This","problem","has","been","partially","addressed","in","the","past","with","stop-word","%EF%AC%81lters%5B","RWC","%2B","19","%5D(whichban%E2%80%9Ccontinuation%E2%80%9Dwords).","Thefew-shotsettinginsteadallowsusto%E2%80%9Cframe%E2%80%9Dthetaskasa","cloze-test","and","allows","the","language","model","to","infer","from","examples","that","a","completion","of","exactly","one","word","is","desired.","We","use","the","following","%EF%AC%81ll-in-the-blank","format%3A","Alice","was","friends","with","Bob.","Alice","went","to","visit","her","friend",".","!","Bob","George","bought","some","baseball","equipment%2C","a","ball%2C","a","glove%2C","and","a",".","!","When","presented","with","examples","formatted","this","way%2C","GPT-3","achieves","86.4%25","accuracy","in","the","few-shot","setting%2C","an","increase","ofover18%25fromthepreviousstate-of-the-art.","Weobservethatfew-shotperformanceimprovesstronglywithmodel","size.","Whilethissettingdecreasestheperformanceofthesmallestmodelbyalmost20%25%2CforGPT-3itimprovesaccuracy","by10%25.","Finally%2Cthe","%EF%AC%81ll-in-blank","methodisnot","effectiveone-shot%2C","whereitalways","performsworsethan","thezero-shot","setting.","Perhaps","this","is","because","all","models","still","require","several","examples","to","recognize","the","pattern.","12","Setting","NaturalQS","WebQS","TriviaQA","RAG","(Fine-tuned%2C","Open-Domain)","%5B","LPP","%2B","20","%5D","44.5","45.5","68.0","T5-11B%2BSSM","(Fine-tuned%2C","Closed-Book)","%5B","RRS20","%5D","36.6","44.7","60.5","T5-11B","(Fine-tuned%2C","Closed-Book)","34.5","37.4","50.1","GPT-3","Zero-Shot","14.6","14.4","64.3","GPT-3","One-Shot","23.0","25.3","68.0","GPT-3","Few-Shot","29.9","41.5","71.2","Table","3.3%3A","Results","on","three","Open-Domain","QA","tasks.","GPT-3","is","shown","in","the","few-%2C","one-%2C","and","zero-shot","settings%2C","as","compared","to","prior","SOTA","results","for","closed","book","and","open","domain","settings.","TriviaQA","few-shot","result","is","evaluated","on","the","wiki","split","test","server.","One","note","of","caution","is","that","an","analysis","of","test","set","contamination","identi%EF%AC%81ed","that","a","signi%EF%AC%81cant","minority","of","the","LAMBADA","dataset","appears","to","be","present","in","our","training","data","%E2%80%93","however","analysis","performed","in","Section","4","suggests","negligible","impact","on","performance.","3.1.3","HellaSwag","The","HellaSwag","dataset","%5B","ZHB","%2B","19","%5D","involves","picking","the","best","ending","to","a","story","or","set","of","instructions.","The","examples","were","adversariallyminedtobedif%EF%AC%81cultforlanguagemodelswhileremainingeasyforhumans(whoachieve95.6%25accuracy).","GPT-3","achieves","78.1%25","accuracy","in","the","one-shot","setting","and","79.3%25","accuracy","in","the","few-shot","setting%2C","outperforming","the","75.4%25","accuracy","of","a","%EF%AC%81ne-tuned","1.5B","parameter","language","model","%5B","ZHR","%2B","19","%5D","but","still","a","fair","amount","lower","than","the","overall","SOTA","of","85.6%25","achieved","by","the","%EF%AC%81ne-tuned","multi-task","model","ALUM.","3.1.4","StoryCloze","We","next","evaluate","GPT-3","on","the","StoryCloze","2016","dataset","%5B","MCH","%2B","16","%5D%2C","which","involves","selecting","the","correct","ending","sentence","for","%EF%AC%81ve-sentence","long","stories.","Here","GPT-3","achieves","83.2%25","in","the","zero-shot","setting","and","87.7%25","in","the","few-shot","setting","(with","K","%3D","70",").","This","is","still","4.1%25","lower","than","the","%EF%AC%81ne-tuned","SOTA","using","a","BERT","based","model","%5B","LDL19","%5D","but","improves","over","previous","zero-shot","results","by","roughly","10%25.","3.2","Closed","Book","Question","Answering","InthissectionwemeasureGPT-3%E2%80%99sabilitytoanswerquestionsaboutbroadfactualknowledge.","Duetotheimmense","amountofpossiblequeries%2Cthistaskhasnormallybeenapproachedbyusinganinformationretrievalsystemto%EF%AC%81nd","relevant","text","in","combination","with","a","model","which","learns","to","generate","an","answer","given","the","question","and","the","retrieved","text.","Since","this","setting","allows","a","system","to","search","for","and","condition","on","text","which","potentially","contains","the","answer","it","isdenoted%E2%80%9Copen-book%E2%80%9D.","%5B","RRS20","%5Drecentlydemonstratedthatalargelanguagemodelcanperformsurprisinglywell","directly","answering","the","questions","without","conditioning","on","auxilliary","information.","They","denote","this","more","restrictive","evaluation","setting","as%E2%80%9Cclosed-book%E2%80%9D.","Their","work","suggests","thatevenhigher-capacity","models","could","performevenbetter","andwetestthishypothesiswithGPT-3.","WeevaluateGPT-3onthe3datasetsin%5B","RRS20","%5D%3A","NaturalQuestions%5B","KPR","%2B","19","%5D%2C","WebQuestions","%5B","BCFL13","%5D%2Cand","TriviaQA%5B","JCWZ17","%5D%2C","using","thesame","splits.","Notethat","in","additionto","allresults","being","in","the","closed-book","setting%2C","our","use","of","few-shot%2C","one-shot%2C","and","zero-shot","evaluations","represent","an","even","stricter","setting","than","previous","closed-book","QA","work%3A","in","addition","to","external","content","not","being","allowed%2C","%EF%AC%81ne-tuning","on","the","Q%26A","dataset","itself","is","also","not","permitted.","TheresultsforGPT-3areshowninTable","3.3",".","OnTriviaQA%2Cweachieve64.3%25inthezero-shotsetting%2C68.0%25inthe","one-shot","setting%2C","and","71.2%25","in","the","few-shot","setting.","The","zero-shot","result","already","outperforms","the","%EF%AC%81ne-tuned","T5-11B","by","14.2%25%2CandalsooutperformsaversionwithQ%26Atailoredspanpredictionduringpre-trainingby3.8%25.","Theone-shot","result","improves","by","3.7%25","and","matches","the","SOTA","for","an","open-domain","QA","system","which","not","only","%EF%AC%81ne-tunes","but","also","makesuse","ofa","learnedretrievalmechanism","overa","15.3Bparameter","densevectorindex","of21M","documents%5B","LPP","%2B","20","%5D.","GPT-3%E2%80%99s","few-shot","result","further","improves","performance","another","3.2%25","beyond","this.","On","WebQuestions","(WebQs)%2C","GPT-3","achieves","14.4%25","in","the","zero-shot","setting%2C","25.3%25","in","the","one-shot","setting%2C","and","41.5%25","in","the","few-shot","setting.","This","compares","to","37.4%25","for","%EF%AC%81ne-tuned","T5-11B%2C","and","44.7%25","for","%EF%AC%81ne-tuned","T5-11B%2BSSM%2C","which","uses","a","Q%26A-speci%EF%AC%81c","pre-training","procedure.","GPT-3","in","the","few-shot","setting","approaches","the","performance","of","state-of-the-art","%EF%AC%81ne-tuned","models.","Notably%2C","compared","to","TriviaQA%2C","WebQS","shows","a","much","larger","gain","from","zero-shot","to","few-shot(andindeeditszero-shotandone-shotperformancearepoor)%2CperhapssuggestingthattheWebQsquestions","13","Figure","3.3%3A","On","TriviaQA","GPT3%E2%80%99s","performance","grows","smoothly","with","model","size%2C","suggesting","that","language","models","continue","to","absorbknowledge","as","their","capacity","increases.","One-shotand","few-shot","performance","make","signi%EF%AC%81cantgains","overzero-shotbehavior%2CmatchingandexceedingtheperformanceoftheSOTA%EF%AC%81ne-tunedopen-domainmodel%2CRAG","%5B","LPP","%2B","20","%5D","and%2Forthestyleoftheiranswersareout-of-distributionforGPT-3.","Nevertheless%2CGPT-3appearsabletoadapttothis","distribution%2C","recovering","strong","performance","in","the","few-shot","setting.","OnNaturalQuestions(NQs)GPT-3achieves14.6%25inthezero-shotsetting%2C23.0%25intheone-shotsetting%2Cand29.9%25in","the","few-shot","setting%2C","compared","to","36.6%25","for","%EF%AC%81ne-tuned","T5","11B%2BSSM.","Similar","to","WebQS%2C","the","large","gain","from","zero-shot","to","few-shot","may","suggest","a","distribution","shift%2C","and","may","also","explain","the","less","competitive","performance","compared","to","TriviaQA","and","WebQS.","In","particular%2C","the","questions","in","NQs","tend","towards","very","%EF%AC%81ne-grained","knowledge","on","Wikipedia","speci%EF%AC%81cally","which","could","be","testing","the","limits","of","GPT-3%E2%80%99s","capacity","and","broad","pretraining","distribution.","Overall%2CononeofthethreedatasetsGPT-3%E2%80%99sone-shotmatchestheopen-domain%EF%AC%81ne-tuningSOTA.Ontheothertwo","datasetsitapproachestheperformanceoftheclosed-bookSOTAdespitenotusing%EF%AC%81ne-tuning.","Onall3datasets%2Cwe","%EF%AC%81ndthatperformancescalesverysmoothlywithmodelsize(Figure","3.3","andAppendix","H","Figure","H.7",")%2Cpossiblyre%EF%AC%82ecting","the","idea","that","model","capacity","translates","directly","to","more","%E2%80%98knowledge%E2%80%99","absorbed","in","the","parameters","of","the","model.","3.3","Translation","ForGPT-2a%EF%AC%81lterwasusedonamultilingualcollectionofdocumentstoproduceanEnglishonlydatasetduetocapacity","concerns.","Even","with","this","%EF%AC%81ltering","GPT-2","showed","some","evidence","of","multilingual","capability","and","performed","non-trivially","whentranslatingbetweenFrenchandEnglishdespiteonlytrainingon10megabytesofremainingFrenchtext.","Sincewe","increase","the","capacity","by","over","two","orders","of","magnitude","from","GPT-2","to","GPT-3%2C","we","also","expand","the","scope","of","the","training","dataset","to","include","more","representation","of","other","languages%2C","though","this","remains","an","area","for","further","improvement.","As","discussedin","2.2","themajorityofourdataisderivedfromrawCommonCrawlwithonlyquality-based%EF%AC%81ltering.","Although","GPT-3%E2%80%99s","training","data","is","still","primarily","English","(93%25","by","word","count)%2C","it","also","includes","7%25","of","text","in","other","languages.","Theselanguagesaredocumentedinthe","supplementalmaterial",".","Inordertobetterunderstandtranslationcapability%2Cwe","also","expand","our","analysis","to","include","two","additional","commonly","studied","languages%2C","German","and","Romanian.","Existing","unsupervised","machine","translation","approaches","often","combine","pretraining","on","a","pair","of","monolingual","datasets","with","back-translation","%5B","SHB15","%5D","to","bridge","the","two","languages","in","a","controlled","way.","By","contrast%2C","GPT-3","learns","from","a","blend","of","training","data","that","mixes","many","languages","together","in","a","natural","way%2C","combining","them","on","a","word%2C","sentence%2C","anddocumentlevel.","GPT-3alsousesasingletrainingobjectivewhichisnotcustomizedordesignedforanytaskin","particular.","However%2C","our","one","%2F","few-shot","settings","aren%E2%80%99t","strictly","comparable","to","prior","unsupervised","work","since","they","make","use","of","a","small","amount","of","paired","examples","(1","or","64).","This","corresponds","to","up","to","a","page","or","two","of","in-context","training","data.","ResultsareshowninTable","3.4",".","Zero-shotGPT-3%2Cwhichonlyreceivesonanaturallanguagedescriptionofthetask%2C","still","underperforms","recent","unsupervised","NMT","results.","However%2C","providing","only","a","single","example","demonstration","for","14","Setting","En","!","Fr","Fr","!","En","En","!","De","De","!","En","En","!","Ro","Ro","!","En","SOTA","(Supervised)","45.6","a","35.0","b","41.2","c","40.2","d","38.5","e","39.9","e","XLM","%5B","LC19","%5D","33.4","33.3","26.4","34.3","33.3","31.8","MASS","%5B","STQ","%2B","19","%5D","37.5","34.9","28.3","35.2","35.2","33.1","mBART","%5B","LGG","%2B","20","%5D","-","-","29.8","34.0","35.0","30.5","GPT-3","Zero-Shot","25.2","21.2","24.6","27.2","14.1","19.9","GPT-3","One-Shot","28.3","33.7","26.2","30.4","20.6","38.6","GPT-3","Few-Shot","32.6","39.2","29.7","40.6","21.0","39.5","Table","3.4%3A","Few-shot","GPT-3","outperforms","previous","unsupervised","NMT","work","by","5","BLEU","when","translating","into","English","re%EF%AC%82ecting","its","strength","as","an","English","LM.","We","report","BLEU","scores","on","the","WMT%E2%80%9914","Fr","%24","En%2C","WMT%E2%80%9916","De","%24","En%2C","and","WMT%E2%80%9916","Ro","%24","En","datasets","as","measured","by","multi-bleu.perl","with","XLM%E2%80%99s","tokeniza-","tion","in","order","to","compare","most","closely","with","prior","unsupervised","NMT","work.","SacreBLEU","f","%5B","Pos18","%5D","results","re-","ported","in","Appendix","H",".","Underline","indicates","an","unsupervised","or","few-shot","SOTA%2C","bold","indicates","supervised","SOTA","with","relative","con%EF%AC%81dence.","a","%5B","EOAG18","%5D","b","%5B","DHKH14","%5D","c","%5B","WXH","%2B","18","%5D","d","%5B","oR16","%5D","e","%5B","LGG","%2B","20","%5D","f","%5BSacreBLEU","signature%3A","BLEU%2Bcase.mixed%2Bnumrefs.1%2Bsmooth.exp%2Btok.intl%2Bversion.1.2.20%5D","Figure3.4%3A","Few-shottranslationperformanceon6languagepairsasmodelcapacityincreases.","Thereisaconsistent","trend","of","improvement","across","all","datasets","as","the","model","scales%2C","and","as","well","as","tendency","for","translation","into","English","to","be","stronger","than","translation","from","English.","15","Setting","Winograd","Winogrande","(XL)","Fine-tuned","SOTA","90.1","a","84.6","b","GPT-3","Zero-Shot","88.3*","70.2","GPT-3","One-Shot","89.7*","73.2","GPT-3","Few-Shot","88.6*","77.7","Table","3.5%3A","Results","on","the","WSC273","version","of","Winograd","schemas","and","the","adversarial","Winogrande","dataset.","See","Section","4","for","details","on","potential","contamination","of","the","Winograd","test","set.","a","%5B","SBBC19","%5D","b","%5B","LYN","%2B","20","%5D","Figure","3.5%3A","Zero-%2C","one-%2C","and","few-shot","performance","on","the","adversarial","Winogrande","dataset","as","model","capacity","scales.","Scaling","is","relatively","smooth","with","the","gains","to","few-shot","learning","increasing","with","model","size%2C","and","few-shot","GPT-3","175B","is","competitive","with","a","%EF%AC%81ne-tuned","RoBERTA-large.","each","translation","task","improves","performance","by","over","7","BLEU","and","nears","competitive","performance","with","prior","work.","GPT-3inthefullfew-shotsettingfurtherimprovesanother4BLEUresultinginsimilaraverageperformancetoprior","unsupervisedNMTwork.","GPT-3hasanoticeableskewinitsperformancedependingonlanguagedirection.","Forthe","threeinputlanguagesstudied%2CGPT-3signi%EF%AC%81cantlyoutperformspriorunsupervisedNMTworkwhentranslatinginto","English","but","underperforms","when","translating","in","the","other","direction.","Performance","on","En-Ro","is","a","noticeable","outlier","at","over","10","BLEU","worse","than","prior","unsupervised","NMT","work.","This","could","be","a","weakness","due","to","reusing","the","byte-level","BPE","tokenizerofGPT-2whichwasdevelopedforanalmostentirelyEnglishtrainingdataset.","ForbothFr-EnandDe-En%2C","few","shot","GPT-3","outperforms","the","best","supervised","result","we","could","%EF%AC%81nd","but","due","to","our","unfamiliarity","with","the","literature","and","the","appearance","that","these","are","un-competitive","benchmarks","we","do","not","suspect","those","results","represent","true","state","of","the","art.","For","Ro-En%2C","few","shot","GPT-3","performs","within","0.5","BLEU","of","the","overall","SOTA","which","is","achieved","by","a","combination","of","unsupervised","pretraining%2C","supervised","%EF%AC%81netuning","on","608K","labeled","examples%2C","and","backtranslation","%5B","LHCG19b","%5D.","Finally%2C","across","all","language","pairs","and","across","all","three","settings","(zero-%2C","one-%2C","and","few-shot)%2C","there","is","a","smooth","trend","of","improvement","with","model","capacity.","This","is","shown","in","Figure","3.4","in","the","case","of","few-shot","results%2C","and","scaling","for","all","three","settings","is","shown","in","Appendix","H",".","3.4","Winograd-Style","Tasks","TheWinogradSchemasChallenge%5B","LDM12","%5DisaclassicaltaskinNLPthatinvolvesdeterminingwhichwordapronoun","refersto%2Cwhenthepronounisgrammaticallyambiguousbutsemanticallyunambiguoustoahuman.","Recently%EF%AC%81ne-tuned","language","models","have","achieved","near-human","performance","on","the","original","Winograd","dataset%2C","but","more","dif%EF%AC%81cult","versions","16","Setting","PIQA","ARC","(Easy)","ARC","(Challenge)","OpenBookQA","Fine-tuned","SOTA","79.4","92.0","%5B","KKS","%2B","20","%5D","78.5","%5B","KKS","%2B","20","%5D","87.2","%5B","KKS","%2B","20","%5D","GPT-3","Zero-Shot","80.5","*","68.8","51.4","57.6","GPT-3","One-Shot","80.5","*","71.2","53.2","58.8","GPT-3","Few-Shot","82.8","*","70.1","51.5","65.4","Table3.6%3A","GPT-3resultsonthreecommonsensereasoningtasks%2CPIQA%2CARC%2CandOpenBookQA.GPT-3Few-Shot","PIQA","result","is","evaluated","on","the","test","server.","See","Section","4","for","details","on","potential","contamination","issues","on","the","PIQA","test","set.","Figure","3.6%3A","GPT-3","results","on","PIQA","in","the","zero-shot%2C","one-shot%2C","and","few-shot","settings.","The","largest","model","achieves","a","score","on","the","development","set","in","all","three","conditions","that","exceeds","the","best","recorded","score","on","the","task.","such","as","the","adversarially-mined","Winogrande","dataset","%5B","SBBC19","%5D","still","signi%EF%AC%81cantly","lag","human","performance.","We","test","GPT-3%E2%80%99s","performance","on","both","Winograd","and","Winogrande%2C","as","usual","in","the","zero-%2C","one-%2C","and","few-shot","setting.","On","Winograd","we","test","GPT-3","on","the","original","set","of","273","Winograd","schemas%2C","using","the","same","%E2%80%9Cpartial","evaluation%E2%80%9D","method","describedin%5B","RWC","%2B","19","%5D.","NotethatthissettingdiffersslightlyfromtheWSCtaskintheSuperGLUEbenchmark%2Cwhich","ispresentedasbinaryclassi%EF%AC%81cationandrequiresentityextractiontoconverttotheformdescribedinthissection.","On","Winograd","GPT-3","achieves","88.3%25%2C","89.7%25%2C","and","88.6%25","in","the","zero-shot%2C","one-shot%2C","and","few-shot","settings%2C","showing","no","clear","in-context","learning","but","in","all","cases","achieving","strong","results","just","a","few","points","below","state-of-the-art","and","estimated","human","performance.","We","note","that","contamination","analysis","found","some","Winograd","schemas","in","the","training","data","but","this","appears","to","have","only","a","small","effect","on","results","(see","Section","4",").","On","the","more","dif%EF%AC%81cult","Winogrande","dataset%2C","we","do","%EF%AC%81nd","gains","to","in-context","learning%3A","GPT-3","achieves","70.2%25","in","the","zero-shot","setting%2C","73.2%25","in","the","one-shot","setting%2C","and","77.7%25","in","the","few-shot","setting.","For","comparison","a","%EF%AC%81ne-tuned","RoBERTAmodelachieves79%25%2Cstate-of-the-artis84.6%25achievedwitha%EF%AC%81ne-tunedhighcapacitymodel(T5)%2Cand","human","performance","on","the","task","as","reported","by","%5B","SBBC19","%5D","is","94.0%25.","3.5","Common","Sense","Reasoning","Next","we","consider","three","datasets","which","attempt","to","capture","physical","or","scienti%EF%AC%81c","reasoning%2C","as","distinct","from","sentence","completion%2Creadingcomprehension%2Corbroadknowledgequestionanswering.","The%EF%AC%81rst%2CPhysicalQA(PIQA)%5B","BZB","%2B","19","%5D%2C","askscommonsensequestionsabouthowthephysicalworldworksandisintendedasaprobeofgroundedunderstanding","of","the","world.","GPT-3","achieves","81.0%25","accuracy","zero-shot%2C","80.5%25","accuracy","one-shot%2C","and","82.8%25","accuracy","few-shot","(the","last","measured","on","PIQA%E2%80%99s","test","server).","This","compares","favorably","to","the","79.4%25","accuracy","prior","state-of-the-art","of","a","17","Setting","CoQA","DROP","QuAC","SQuADv2","RACE-h","RACE-m","Fine-tuned","SOTA","90.7","a","89.1","b","74.4","c","93.0","d","90.0","e","93.1","e","GPT-3","Zero-Shot","81.5","23.6","41.5","59.5","45.5","58.4","GPT-3","One-Shot","84.0","34.3","43.3","65.4","45.9","57.4","GPT-3","Few-Shot","85.0","36.5","44.3","69.8","46.8","58.1","Table","3.7%3A","Results","on","reading","comprehension","tasks.","All","scores","are","F1","except","results","for","RACE","which","report","accuracy.","a","%5B","JZC","%2B","19","%5D","b","%5B","JN20","%5D","c","%5B","AI19","%5D","d","%5B","QIA20","%5D","e","%5B","SPP","%2B","19","%5D","%EF%AC%81ne-tunedRoBERTa.","PIQAshowsrelativelyshallowscalingwithmodelsizeandisstillover10%25worsethanhuman","performance%2C","but","GPT-3%E2%80%99s","few-shot","and","even","zero-shot","result","outperform","the","current","state-of-the-art.","Our","analysis","%EF%AC%82aggedPIQAforapotentialdatacontaminationissue(despitehiddentestlabels)%2Candwethereforeconservativelymark","the","result","with","an","asterisk.","See","Section","4","for","details.","ARC","%5B","CCE","%2B","18","%5D","is","a","dataset","of","multiple-choice","questions","collected","from","3rd","to","9th","grade","science","exams.","On","the","%E2%80%9CChallenge%E2%80%9D","version","of","the","dataset","which","has","been","%EF%AC%81ltered","to","questions","which","simple","statistical","or","information","retrieval","methods","are","unable","to","correctly","answer%2C","GPT-3","achieves","51.4%25","accuracy","in","the","zero-shot","setting%2C","53.2%25","in","the","one-shot","setting%2C","and","51.5%25","in","the","few-shot","setting.","This","is","approaching","the","performance","of","a","%EF%AC%81ne-tuned","RoBERTa","baseline","(55.9%25)fromUni%EF%AC%81edQA%5B","KKS","%2B","20","%5D.","Onthe%E2%80%9CEasy%E2%80%9Dversionofthedataset(questionswhicheitherofthementioned","baselineapproachesansweredcorrectly)%2CGPT-3achieves68.8%25%2C71.2%25%2Cand70.1%25whichslightlyexceedsa%EF%AC%81ne-tuned","RoBERTa","baseline","from","%5B","KKS","%2B","20","%5D.","However%2C","both","of","these","results","are","still","much","worse","than","the","overall","SOTAs","achieved","by","the","Uni%EF%AC%81edQA","which","exceeds","GPT-3%E2%80%99s","few-shot","results","by","27%25","on","the","challenge","set","and","22%25","on","the","easy","set.","OnOpenBookQA%5B","MCKS18","%5D%2CGPT-3improvessigni%EF%AC%81cantlyfromzerotofewshotsettingsbutisstillover20points","short","of","the","overall","SOTA.","GPT-3%E2%80%99s","few-shot","performance","is","similar","to","a","%EF%AC%81ne-tuned","BERT","Large","baseline","on","the","leaderboard.","Overall%2Cin-contextlearningwithGPT-3showsmixedresultsoncommonsensereasoningtasks%2Cwithonlysmalland","inconsistent","gains","observed","in","the","one","and","few-shot","learning","settings","for","both","PIQA","and","ARC%2C","but","a","signi%EF%AC%81cant","improvement","is","observed","on","OpenBookQA.","GPT-3","sets","SOTA","on","the","new","PIQA","dataset","in","all","evaluation","settings.","3.6","Reading","Comprehension","Next","we","evaluate","GPT-3","on","the","task","of","reading","comprehension.","We","use","a","suite","of","5","datasets","including","abstractive%2C","multiple","choice%2C","and","span","based","answer","formats","in","both","dialog","and","single","question","settings.","We","observe","a","wide","spread","in","GPT-3%E2%80%99s","performance","across","these","datasets","suggestive","of","varying","capability","with","different","answer","formats.","In","general","we","observe","GPT-3","is","on","par","with","initial","baselines","and","early","results","trained","using","contextual","representations","on","each","respective","dataset.","GPT-3performsbest(within3pointsofthehumanbaseline)onCoQA%5B","RCM19","%5Dafree-formconversationaldataset","and","performs","worst","(13","F1","below","an","ELMo","baseline)","on","QuAC","%5B","CHI","%2B","18","%5D","a","dataset","which","requires","modeling","structured","dialog","acts","and","answer","span","selections","of","teacher-student","interactions.","On","DROP","%5B","DWD","%2B","19","%5D%2C","a","dataset","testing","discrete","reasoningandnumeracyinthecontextofreadingcomprehension%2CGPT-3inafew-shotsettingoutperformsthe%EF%AC%81ne-tuned","BERT","baseline","from","the","original","paper","but","is","still","well","below","both","human","performance","and","state-of-the-art","approaches","which","augmentneural","networkswith","symbolicsystems","%5B","RLL","%2B","19","%5D.","OnSQuAD","2.0","%5B","RJL18","%5D%2C","GPT-3","demonstratesits","few-shotlearningcapabilities%2Cimprovingbyalmost10F1(to69.8)comparedtoazero-shotsetting.","Thisallowsitto","slightlyoutperformthebest%EF%AC%81ne-tunedresult","in","theoriginalpaper.","OnRACE%5B","LXL","%2B","17","%5D%2Camultiplechoicedatasetof","middleschoolandhighschoolenglishexaminations%2CGPT-3performsrelativelyweaklyandisonlycompetitivewith","the","earliest","work","utilizing","contextual","representations","and","is","still","45%25","behind","SOTA.","3.7","SuperGLUE","In","order","to","better","aggregate","results","on","NLP","tasks","and","compare","to","popular","models","such","as","BERT","and","RoBERTa","in","a","more","systematic","way%2C","we","also","evaluate","GPT-3","on","a","standardized","collection","of","datasets%2C","the","SuperGLUE","benchmark","%5B","WPN","%2B","19","%5D","%5B","WPN","%2B","19","%5D","%5B","CLC","%2B","19","%5D","%5B","DMST19","%5D","%5B","RBG11","%5D","%5B","KCR","%2B","18","%5D","%5B","ZLL","%2B","18","%5D","%5B","DGM06","%5D","%5B","BHDD","%2B","06","%5D","%5B","GMDD07","%5D","%5B","BDD","%2B","09","%5D","%5B","PCC18","%5D","%5B","PHR","%2B","18","%5D.","GPT-3%E2%80%99s","test-set","performance","on","the","SuperGLUE","dataset","is","shown","in","Table","3.8",".","In","the","few-shot","setting%2C","we","used","32","examples","for","all","tasks%2C","sampled","randomly","from","the","training","set.","For","all","tasks","except","WSC","18","Figure","3.7%3A","GPT-3","results","on","CoQA","reading","comprehension","task.","GPT-3","175B","achieves","85","F1","in","the","few-shot","setting%2C","only","a","few","points","behind","measured","human","performance","and","state-of-the-art","%EF%AC%81ne-tuned","models.","Zero-shot","and","one-shot","performance","is","a","few","points","behind%2C","with","the","gains","to","few-shot","being","largest","for","bigger","models.","SuperGLUE","BoolQ","CB","CB","COPA","RTE","Average","Accuracy","Accuracy","F1","Accuracy","Accuracy","Fine-tuned","SOTA","89.0","91.0","96.9","93.9","94.8","92.5","Fine-tuned","BERT-Large","69.0","77.4","83.6","75.7","70.6","71.7","GPT-3","Few-Shot","71.8","76.4","75.6","52.0","92.0","69.0","WiC","WSC","MultiRC","MultiRC","ReCoRD","ReCoRD","Accuracy","Accuracy","Accuracy","F1a","Accuracy","F1","Fine-tuned","SOTA","76.1","93.8","62.3","88.2","92.5","93.3","Fine-tuned","BERT-Large","69.6","64.6","24.1","70.0","71.3","72.0","GPT-3","Few-Shot","49.4","80.1","30.5","75.4","90.2","91.1","Table","3.8%3A","Performance","of","GPT-3","on","SuperGLUE","compared","to","%EF%AC%81ne-tuned","baselines","and","SOTA.","All","results","are","reported","on","the","test","set.","GPT-3","few-shot","is","given","a","total","of","32","examples","within","the","context","of","each","task","and","performs","no","gradient","updates.","19","Figure3.8%3A","PerformanceonSuperGLUEincreaseswithmodelsizeandnumberofexamplesincontext.","Avalue","of","K","%3D","32","means","that","our","model","was","shown","32","examples","per","task%2C","for","256","examples","total","divided","across","the","8","tasks","in","SuperGLUE.WereportGPT-3valuesonthedevset%2C","soournumbersarenotdirectlycomparabletothedottedreference","lines(ourtestsetresultsareinTable","3.8",").","TheBERT-Largereferencemodelwas%EF%AC%81ne-tunedontheSuperGLUEtraining","set(125Kexamples)%2CwhereasBERT%2B%2Bwas%EF%AC%81rst%EF%AC%81ne-tunedonMultiNLI(392Kexamples)andSWAG(113Kexamples)","before","further","%EF%AC%81ne-tuning","on","the","SuperGLUE","training","set","(for","a","total","of","630K","%EF%AC%81ne-tuning","examples).","We","%EF%AC%81nd","the","differenceinperformancebetweentheBERT-LargeandBERT%2B%2Btoberoughlyequivalenttothedifferencebetween","GPT-3","with","one","example","per","context","versus","eight","examples","per","context.","andMultiRC%2Cwesampledanewsetofexamplestouseinthecontextforeachproblem.","ForWSCandMultiRC%2Cwe","used","the","same","set","of","randomly","drawn","examples","from","the","training","set","as","context","for","all","of","the","problems","we","evaluated.","WeobserveawiderangeinGPT-3%E2%80%99sperformanceacrosstasks.","OnCOPAandReCoRDGPT-3achievesnear-SOTA","performance","in","the","one-shot","and","few-shot","settings%2C","with","COPA","falling","only","a","couple","points","short","and","achieving","secondplaceontheleaderboard%2Cwhere%EF%AC%81rstplaceisheldbya%EF%AC%81ne-tuned11billionparametermodel(T5).","OnWSC%2C","performanceis","stillrelatively","strong%2Cachieving","80.1%25in","thefew-shotsetting","(notethat","GPT-3achieves","88.6%25on","the","original","Winograd","dataset","as","described","in","Section","3.4",").","On","BoolQ%2C","MultiRC%2C","and","RTE%2C","performance","is","reasonable%2C","roughly","matching","that","of","a","%EF%AC%81ne-tuned","BERT-Large.","On","CB%2C","we","see","signs","of","life","at","75.6%25","in","the","few-shot","setting.","WiCisanotableweakspotwithfew-shotperformanceat49.4%25(atrandomchance).","Wetriedanumberofdifferent","phrasings","and","formulations","for","WiC","(which","involves","determining","if","a","word","is","being","used","with","the","same","meaning","in","two","sentences)%2C","none","of","which","was","able","to","achieve","strong","performance.","This","hints","at","a","phenomenon","that","will","become","clearer","in","the","next","section","(which","discusses","the","ANLI","benchmark)","%E2%80%93","GPT-3","appears","to","be","weak","in","the","few-shot","or","one-shot","settingatsometasksthatinvolvecomparingtwosentencesorsnippets%2Cforexamplewhetherawordisusedthesame","way","in","two","sentences","(WiC)%2C","whether","one","sentence","is","a","paraphrase","of","another%2C","or","whether","one","sentence","implies","another.","ThiscouldalsoexplainthecomparativelylowscoresforRTEandCB%2Cwhichalsofollowthisformat.","Despitethese","weaknesses%2C","GPT-3","still","outperforms","a","%EF%AC%81ne-tuned","BERT-large","on","four","of","eight","tasks","and","on","two","tasks","GPT-3","is","close","to","the","state-of-the-art","held","by","a","%EF%AC%81ne-tuned","11","billion","parameter","model.","Finally%2C","we","note","that","the","few-shot","SuperGLUE","score","steadily","improves","with","both","model","size","and","with","number","of","examples","in","the","context","showing","increasing","bene%EF%AC%81ts","from","in-context","learning","(Figure","3.8",").","We","scale","K","up","to","32","examples","per","task%2C","after","which","point","additional","examples","will","not","reliably","%EF%AC%81t","into","our","context.","When","sweeping","over","values","of","K","%2C","we","%EF%AC%81nd","that","GPT-3","requires","less","than","eight","total","examples","per","task","to","outperform","a","%EF%AC%81ne-tuned","BERT-Large","on","overall","SuperGLUE","score.","3.8","NLI","NaturalLanguageInference","(NLI)%5B","Fyo00","%5Dconcernsthe","abilitytounderstandthe","relationshipbetweentwosentences.","In","practice%2C","this","task","is","usually","structured","as","a","two","or","three","class","classi%EF%AC%81cation","problem","where","the","model","classi%EF%AC%81es","20","Figure","3.9%3A","Performance","of","GPT-3","on","ANLI","Round","3.","Results","are","on","the","dev-set%2C","which","has","only","1500","examples","and","therefore","hashigh","variance","(we","estimate","a","standard","deviation","of","1.2%25).","We","%EF%AC%81nd","that","smaller","models","hover","around","random","chance%2C","while","few-shot","GPT-3","175B","closes","almost","half","the","gap","from","random","chance","to","SOTA.","Results","for","ANLI","rounds","1","and","2","are","shown","in","the","appendix.","whether","the","second","sentence","logically","follows","from","the","%EF%AC%81rst%2C","contradicts","the","%EF%AC%81rst","sentence%2C","or","is","possibly","true","(neutral).","SuperGLUEincludesanNLIdataset%2CRTE%2Cwhichevaluatesthebinaryversionofthetask.","OnRTE%2Conlythelargest","versionofGPT-3performsconvincinglybetterthanrandom(56%25)inanyevaluationsetting%2Cbutinafew-shotsetting","GPT-3","performs","similarly","to","a","single-task","%EF%AC%81ne-tuned","BERT","Large.","We","also","evaluate","on","the","recently","introduced","Adversarial","Natural","Language","Inference","(ANLI)","dataset","%5B","NWD","%2B","19","%5D.","ANLI","is","a","dif%EF%AC%81cult","dataset","employing","a","series","of","adversariallyminednaturallanguageinferencequestionsinthreerounds(R1%2CR2%2CandR3).","SimilartoRTE%2Callofour","models","smaller","than","GPT-3","perform","at","almost","exactly","random","chance","on","ANLI%2C","even","in","the","few-shot","setting","(","%18","33%25",")%2C","whereas","GPT-3","itself","shows","signs","of","life","on","Round","3.","Results","for","ANLI","R3","are","highlighted","in","Figure","3.9","and","full","results","forallroundscanbefoundinAppendix","H",".TheseresultsonbothRTEandANLIsuggestthatNLIisstillaverydif%EF%AC%81cult","task","for","language","models","and","they","are","only","just","beginning","to","show","signs","of","progress.","3.9","Synthetic","and","Qualitative","Tasks","One","way","to","probe","GPT-3%E2%80%99s","range","of","abilities","in","the","few-shot","(or","zero-","and","one-shot)","setting","is","to","give","it","tasks","which","require","it","to","perform","simple","on-the-%EF%AC%82y","computational","reasoning%2C","recognize","a","novel","pattern","that","is","unlikely","to","have","occurred","in","training%2C","or","adapt","quickly","to","an","unusual","task.","We","devise","several","tasks","to","test","this","class","of","abilities.","First%2C","we","test","GPT-3%E2%80%99s","ability","to","perform","arithmetic.","Second%2C","we","create","several","tasks","that","involve","rearranging","or","unscrambling","the","lettersin","aword%2C","taskswhichare","unlikelytohave","beenexactly","seenduringtraining.","Third%2C","wetest","GPT-3%E2%80%99s","abilityto","solveSAT-styleanalogyproblemsfew-shot.","Finally%2CwetestGPT-3onseveralqualitativetasks%2Cincludingusingnew","wordsinasentence%2CcorrectingEnglishgrammar%2Candnewsarticlegeneration.","Wewillreleasethesyntheticdatasets","with","the","hope","of","stimulating","further","study","of","test-time","behavior","of","language","models.","3.9.1","Arithmetic","TotestGPT-3%E2%80%99sabilitytoperformsimplearithmeticoperationswithouttask-speci%EF%AC%81ctraining%2Cwedevelopedasmall","battery","of","10","tests","that","involve","asking","GPT-3","a","simple","arithmetic","problem","in","natural","language%3A","%E2%80%A2","2","digit","addition","(2D%2B)","%E2%80%93","The","model","is","asked","to","add","two","integers","sampled","uniformly","from","%5B0","%3B","100)","%2C","phrased","in","the","form","of","a","question%2C","e.g.","%E2%80%9CQ%3A","What","is","48","plus","76%3F","A%3A","124.%E2%80%9D","%E2%80%A2","2","digit","subtraction","(2D-)","%E2%80%93","The","model","is","asked","to","subtract","two","integers","sampled","uniformly","from","%5B0","%3B","100)","%3B","the","answer","may","be","negative.","Example%3A","%E2%80%9CQ%3A","What","is","34","minus","53%3F","A%3A","-19%E2%80%9D.","%E2%80%A2","3","digit","addition","(3D%2B)","%E2%80%93","Same","as","2","digit","addition%2C","except","numbers","are","uniformly","sampled","from","%5B0","%3B","1000)",".","21","Figure","3.10%3A","Results","on","all","10","arithmetic","tasks","in","the","few-shot","settings","for","models","of","different","sizes.","There","is","a","signi%EF%AC%81cantjumpfromthesecondlargestmodel(GPT-313B)tothelargestmodel(GPT-3175)%2Cwiththelatterbeing","able","to","reliably","accurate","2","digit","arithmetic%2C","usually","accurate","3","digit","arithmetic%2C","and","correct","answers","a","signi%EF%AC%81cant","fraction","of","the","time","on","4-5","digit","arithmetic%2C","2","digit","multiplication%2C","and","compound","operations.","Results","for","one-shot","and","zero-shot","are","shown","in","the","appendix.","%E2%80%A2","3digitsubtraction(3D-)","%E2%80%93Sameas2digitsubtraction%2Cexceptnumbersareuniformlysampledfrom","%5B0","%3B","1000)",".","%E2%80%A2","4","digit","addition","(4D%2B)","%E2%80%93","Same","as","3","digit","addition%2C","except","uniformly","sampled","from","%5B0","%3B","10000)",".","%E2%80%A2","4","digit","subtraction","(4D-)","%E2%80%93","Same","as","3","digit","subtraction%2C","except","uniformly","sampled","from","%5B0","%3B","10000)",".","%E2%80%A2","5","digit","addition","(5D%2B)","%E2%80%93","Same","as","3","digit","addition%2C","except","uniformly","sampled","from","%5B0","%3B","100000)",".","%E2%80%A2","5","digit","subtraction","(5D-)","%E2%80%93","Same","as","3","digit","subtraction%2C","except","uniformly","sampled","from","%5B0","%3B","100000)",".","%E2%80%A2","2digit","multiplication(2Dx)","%E2%80%93","The","model","is","asked","to","multiply","two","integers","sampled","uniformly","from","%5B0","%3B","100)","%2C","e.g.","%E2%80%9CQ%3A","What","is","24","times","42%3F","A%3A","1008%E2%80%9D.","%E2%80%A2","One-digit","composite","(1DC)","%E2%80%93","The","model","is","asked","to","perform","a","composite","operation","on","three","1","digit","numbers%2C","withparenthesesaroundthelasttwo.","Forexample%2C%E2%80%9CQ%3AWhatis6%2B(4*8)%3F","A%3A38%E2%80%9D.","Thethree1digitnumbers","are","selected","uniformly","on","%5B0","%3B","10)","and","the","operations","are","selected","uniformly","from","f","%2B%2C-%2C*","g",".","In","all","10","tasks","the","model","must","generate","the","correct","answer","exactly.","For","each","task","we","generate","a","dataset","of","2%2C000","random","instances","of","the","task","and","evaluate","all","models","on","those","instances.","FirstweevaluateGPT-3inthefew-shotsetting%2CforwhichresultsareshowninFigure","3.10",".","Onadditionandsubtraction%2C","GPT-3displaysstrongpro%EF%AC%81ciencywhenthenumberofdigitsissmall%2Cachieving100%25accuracyon2digitaddition%2C","98.9%25","at","2","digit","subtraction%2C","80.2%25","at3","digit","addition%2C","and","94.2%25","at","3-digit","subtraction.","Performance","decreasesas","the","number","of","digits","increases%2C","but","GPT-3","still","achieves","25-26%25","accuracy","on","four","digit","operations","and","9-10%25","accuracy","on","%EF%AC%81vedigitoperations%2Csuggestingatleastsomecapacitytogeneralizetolargernumbersofdigits.","GPT-3alsoachieves","29.2%25accuracyat2digitmultiplication%2Canespeciallycomputationallyintensiveoperation.","Finally%2CGPT-3achieves","21.3%25","accuracy","at","single","digit","combined","operations","(for","example%2C","9*(7%2B5))%2C","suggesting","that","it","has","some","robustness","beyond","just","single","operations.","AsFigure","3.10","makesclear%2Csmallmodelsdopoorlyonallofthesetasks%E2%80%93eventhe13billionparametermodel(the","second","largest","after","the","175","billion","full","GPT-3)","can","solve","2","digit","addition","and","subtraction","only","half","the","time%2C","and","all","other","operations","less","than","10%25","of","the","time.","One-shotandzero-shotperformancearesomewhatdegradedrelativetofew-shotperformance%2Csuggestingthatadaptation","to","the","task","(or","at","the","very","least","recognition","of","the","task)","is","important","to","performing","these","computations","correctly.","Nevertheless%2C","one-shot","performance","is","still","quite","strong%2C","and","even","zero-shot","performance","of","the","full","GPT-3","signi%EF%AC%81cantly","22","Setting","2D%2B","2D-","3D%2B","3D-","4D%2B","4D-","5D%2B","5D-","2Dx","1DC","GPT-3","Zero-shot","76.9","58.0","34.2","48.3","4.0","7.5","0.7","0.8","19.8","9.8","GPT-3","One-shot","99.6","86.4","65.5","78.7","14.0","14.0","3.5","3.8","27.4","14.3","GPT-3","Few-shot","100.0","98.9","80.4","94.2","25.5","26.8","9.3","9.9","29.2","21.3","Table","3.9%3A","Results","on","basic","arithmetic","tasks","for","GPT-3","175B.","f","2%2C3%2C4%2C5","g","D","f","%2B%2C-","g","is","2%2C","3%2C","4%2C","and","5","digit","addition","or","subtraction%2C","2Dx","is","2","digit","multiplication.","1DC","is","1","digit","composite","operations.","Results","become","progressively","stronger","moving","from","the","zero-shot","to","one-shot","to","few-shot","setting%2C","but","even","the","zero-shot","shows","signi%EF%AC%81cant","arithmetic","abilities.","Setting","CL","A1","A2","RI","RW","GPT-3","Zero-shot","3.66","2.28","8.91","8.26","0.09","GPT-3","One-shot","21.7","8.62","25.9","45.4","0.48","GPT-3","Few-shot","37.9","15.1","39.7","67.2","0.44","Table","3.10%3A","GPT-3","175B","performance","on","various","word","unscrambling","and","word","manipulation","tasks%2C","in","zero-%2C","one-%2C","and","few-shotsettings.","CLis%E2%80%9Ccyclelettersinword%E2%80%9D%2CA1isanagramsofbutthe%EF%AC%81rstandlastletters%2C","A2isanagramsofallbut","the","%EF%AC%81rst","and","last","two","letters%2C","RI","is","%E2%80%9CRandom","insertion","in","word%E2%80%9D%2C","RW","is","%E2%80%9Creversed","words%E2%80%9D.","outperforms","few-shot","learning","for","all","smaller","models.","All","three","settings","for","the","full","GPT-3","are","shown","in","Table","3.9","%2C","and","model","capacity","scaling","for","all","three","settings","is","shown","in","Appendix","H",".","Tospot-checkwhetherthemodelissimplymemorizingspeci%EF%AC%81carithmeticproblems%2Cwetookthe3-digitarithmetic","problems","in","our","test","set","and","searched","for","them","in","our","training","data","in","both","the","forms","%22%3CNUM1%3E","%2B","%3CNUM2%3E","%3D%22","and","%22%3CNUM1%3E","plus","%3CNUM2%3E%22",".","Out","of","2%2C000","addition","problems","we","found","only","17","matches","(0.8%25)","and","out","of","2%2C000","subtraction","problems","we","found","only","2","matches","(0.1%25)%2C","suggesting","that","only","a","trivial","fraction","of","the","correct","answers","could","have","been","memorized.","In","addition%2C","inspection","of","incorrect","answers","reveals","that","the","model","often","makes","mistakes","such","as","not","carrying","a","%E2%80%9C1%E2%80%9D%2C","suggesting","it","is","actually","attempting","to","perform","the","relevant","computation","rather","than","memorizing","a","table.","Overall%2C","GPT-3","displays","reasonable","pro%EF%AC%81ciency","at","moderately","complex","arithmetic","in","few-shot%2C","one-shot%2C","and","even","zero-shot","settings.","3.9.2","Word","Scrambling","and","Manipulation","Tasks","To","test","GPT-3%E2%80%99s","ability","to","learn","novel","symbolic","manipulations","from","a","few","examples%2C","we","designed","a","small","battery","of","5","%E2%80%9Ccharacter","manipulation%E2%80%9D","tasks.","Each","task","involves","giving","the","model","a","word","distorted","by","some","combination","of","scrambling%2C","addition%2C","or","deletion","of","characters%2C","and","asking","it","to","recover","the","original","word.","The","5","tasks","are%3A","%E2%80%A2","Cycle","letters","in","word","(CL)","%E2%80%93","The","model","is","given","a","word","with","its","letters","cycled%2C","then","the","%E2%80%9C%3D%E2%80%9D","symbol%2C","and","is","expected","to","generate","the","original","word.","For","example%2C","it","might","be","given","%E2%80%9Clyinevitab%E2%80%9D","and","should","output","%E2%80%9Cinevitably%E2%80%9D.","%E2%80%A2","Anagrams","of","all","but","%EF%AC%81rst","and","last","characters","(A1)","%E2%80%93","The","model","is","given","a","word","where","every","letter","except","the%EF%AC%81rstandlasthavebeenscrambledrandomly%2Candmustoutputtheoriginalword.","Example%3A","criroptuon%3D","corruption.","%E2%80%A2","Anagrams","of","all","but","%EF%AC%81rst","and","last","2","characters","(A2)","%E2%80%93","The","model","is","given","a","word","where","every","letter","except","the%EF%AC%81rst2andlast2have","beenscrambledrandomly%2Candmustrecovertheoriginalword.","Example%3A","opoepnnt","!","opponent.","%E2%80%A2","Random","insertion","in","word","(RI)","%E2%80%93Arandompunctuationorspacecharacterisinsertedbetweeneachletter","of","a","word%2C","and","the","model","must","output","the","original","word.","Example%3A","s.u!c%2Fc!e.s","s","i%2Fo%2Fn","%3D","succession.","%E2%80%A2","Reversed","words","(RW)","%E2%80%93Themodelisgivenawordspelledbackwards%2Candmustoutputtheoriginalword.","Example%3A","stcejbo","!","objects.","For","each","task","we","generate","10%2C000","examples%2C","which","we","chose","to","be","the","top","10%2C000","most","frequent","words","as","measured","by","%5B","Nor09","%5Doflengthmorethan4charactersandlessthan15characters.","Thefew-shotresultsareshowninFigure","3.11",".","Taskperformancetendstogrowsmoothlywithmodelsize%2CwiththefullGPT-3modelachieving66.9%25onremoving","23","Figure3.11%3A","Few-shotperformance","onthe","%EF%AC%81veword","scramblingtasksfor","differentsizes","ofmodel.","Thereis","generally","smooth","improvement","with","model","size","although","the","random","insertion","task","shows","an","upward","slope","of","improvement","with","the175Bmodelsolvingthetaskthemajorityofthetime.","Scalingofone-shotandzero-shotperformanceisshownin","the","appendix.","All","tasks","are","done","with","K","%3D","100",".","randominsertions%2C38.6%25oncyclingletters%2C40.2%25ontheeasieranagramtask%2Cand15.1%25onthemoredif%EF%AC%81cultanagram","task","(where","only","the","%EF%AC%81rst","and","last","letters","are","held","%EF%AC%81xed).","None","of","the","models","can","reverse","the","letters","in","a","word.","In","the","one-shot","setting%2C","performance","is","signi%EF%AC%81cantly","weaker","(dropping","by","half","or","more)%2C","and","in","the","zero-shot","setting","the","model","canrarely","performany","ofthe","tasks(Table","3.10",").","Thissuggests","thatthe","model","really","does","appearto","learnthese","tasks","at","test","time%2C","as","the","model","cannot","perform","them","zero-shot","and","their","arti%EF%AC%81cial","nature","makes","them","unlikely","to","appear","in","the","pre-training","data","(although","we","cannot","con%EF%AC%81rm","this","with","certainty).","We","can","further","quantify","performance","by","plotting","%E2%80%9Cin-context","learning","curves%E2%80%9D%2C","which","show","task","performance","as","a","function","of","the","number","of","in-context","examples.","We","show","in-context","learning","curves","for","the","Symbol","Insertion","task","in","Figure","1.2",".","We","can","see","that","larger","models","are","able","to","make","increasingly","effective","use","of","in-context","information%2C","including","both","task","examples","and","natural","language","task","descriptions.","Finally%2Citisworthaddingthatsolvingthesetasksrequirescharacter-levelmanipulations%2CwhereasourBPEencoding","operatesonsigni%EF%AC%81cantfractionsofaword(onaverage","%18","0","%3A","7","wordspertoken)%2CsofromtheLM%E2%80%99sperspectivesucceeding","atthesetasksinvolvesnotjustmanipulatingBPEtokensbutunderstandingandpullingaparttheirsubstructure.","Also%2C","CL%2C","A1%2C","and","A2","are","not","bijective","(that","is%2C","the","unscrambled","word","is","not","a","deterministic","function","of","the","scrambled","word)%2C","requiring","the","model","to","perform","some","search","to","%EF%AC%81nd","the","correct","unscrambling.","Thus%2C","the","skills","involved","appear","to","require","non-trivial","pattern-matching","and","computation.","3.9.3","SAT","Analogies","To","test","GPT-3","on","another","task","that","is","somewhat","unusual","relative","to","the","typical","distribution","of","text%2C","we","collected","a","set","of","374","%E2%80%9CSAT","analogy%E2%80%9D","problems","%5B","TLBS03","%5D.","Analogies","are","a","style","of","multiple","choice","question","that","constituted","a","section","of","the","SAT","college","entrance","exam","before","2005.","A","typical","example","is","%E2%80%9Caudacious","is","to","boldness","as","(a)","sanctimonious","is","to","hypocrisy%2C","(b)","anonymous","is","to","identity%2C","(c)","remorseful","is","to","misdeed%2C","(d)","deleterious","is","to","result%2C","(e)","impressionable","is","to","temptation%E2%80%9D.","Thestudentisexpectedtochoosewhichofthe%EF%AC%81vewordpairshasthesamerelationshipastheoriginal","wordpair%3Binthisexampletheansweris%E2%80%9Csanctimoniousistohypocrisy%E2%80%9D.","OnthistaskGPT-3achieves65.2%25inthe","few-shotsetting%2C59.1%25intheone-shotsetting%2Cand53.7%25inthezero-shotsetting%2Cwhereastheaveragescoreamong","college","applicants","was","57%25","%5B","TL05","%5D","(random","guessing","yields","20%25).","As","shown","in","Figure","3.12","%2C","the","results","improve","with","scale%2C","with","the","the","full","175","billion","model","improving","by","over","10%25","compared","to","the","13","billion","parameter","model.","24","Figure","3.12%3A","Zero-%2C","one-%2Cand","few-shot","performance","on","SAT","analogy","tasks%2C","for","different","sizes","of","model.","The","largest","modelachieves65%25accuracyinthefew-shotsetting%2Candalsodemonstratessigni%EF%AC%81cantgainstoin-contextlearning","which","are","not","present","in","smaller","models.","3.9.4","News","Article","Generation","Previous","workon","generativelanguage","models","qualitatively","tested","theirability","to","generatesynthetic","%E2%80%9Cnews","articles%E2%80%9D","by","conditional","sampling","from","the","model","given","a","human-written","prompt","consisting","of","a","plausible","%EF%AC%81rst","sentence","for","a","news","story","%5B","RWC","%2B","19","%5D.","Relative","to","%5B","RWC","%2B","19","%5D%2C","the","dataset","used","to","train","GPT-3","is","much","less","weighted","towards","news","articles%2C","so","trying","to","generate","news","articles","via","raw","unconditional","samples","is","less","effective","%E2%80%93","for","example","GPT-3","often","interprets","theproposed%EF%AC%81rstsentenceofa%E2%80%9Cnewsarticle%E2%80%9Dasatweetandthenpostssyntheticresponsesorfollow-uptweets.","To","solvethisproblemweemployedGPT-3%E2%80%99sfew-shotlearningabilitiesbyprovidingthreepreviousnewsarticlesinthe","model%E2%80%99s","context","to","condition","it.","With","the","title","and","subtitle","of","a","proposed","next","article%2C","the","model","is","able","to","reliably","generate","short","articles","in","the","%E2%80%9Cnews%E2%80%9D","genre.","TogaugethequalityofnewsarticlegenerationfromGPT-3(whichwebelieveislikelytobecorrelatedwithconditional","samplegenerationqualityingeneral)%2CwedecidedtomeasurehumanabilitytodistinguishGPT-3-generatedarticles","fromrealones.","SimilarworkhasbeencarriedoutbyKrepsetal.","%5B","KMB20","%5DandZellersetal.","%5B","ZHR","%2B","19","%5D.","Generative","languagemodels","aretrained","tomatch","thedistribution","ofcontent","generatedby","humans%2Cso","the(in)ability","ofhumans","to","distinguish","the","two","is","a","potentially","important","measure","of","quality.","3","Inordertoseehowwellhumanscandetectmodelgeneratedtext%2Cwearbitrarilyselected25articletitlesandsubtitles","fromthewebsite","newser.com","(meanlength%3A","215words).","Wethengeneratedcompletionsofthesetitlesandsubtitles","fromfourlanguagemodelsranginginsizefrom125Mto175B(GPT-3)parameters(meanlength%3A","200words).","Foreach","model%2C","we","presented","around","80","US-based","participants","with","a","quiz","consisting","of","these","real","titles","and","subtitles","followed","by","either","the","human","written","article","or","the","article","generated","by","the","model","4",".","Participants","were","asked","to","select","whether","the","articlewas%E2%80%9Cverylikelywrittenbyahuman%E2%80%9D%2C%E2%80%9Cmorelikelywrittenbyahuman%E2%80%9D%2C%E2%80%9CIdon%E2%80%99tknow%E2%80%9D%2C%E2%80%9Cmorelikelywrittenby","a","machine%E2%80%9D%2C","or","%E2%80%9Cvery","likely","written","by","a","machine%E2%80%9D.","The","articles","we","selected","were","not","in","the","models%E2%80%99","training","data","and","the","model","outputs","were","formatted","and","selected","programmatically","to","prevent","human","cherry-picking.","All","models","used","the","same","context","to","condition","outputs","on","and","were","pre-trainedwiththesamecontextsizeandthesamearticletitlesandsubtitleswereusedaspromptsforeachmodel.","However%2Cwealsorananexperimenttocontrolforparticipanteffortandattentionthatfollowedthesameformatbut","involvedintentionallybadmodelgeneratedarticles.","Thiswasdonebygeneratingarticlesfroma%E2%80%9Ccontrolmodel%E2%80%9D%3A","a","160M","parameter","model","with","no","context","and","increased","output","randomness.","3","This","task","is","also","relevant","to","the","potential","misuse","of","language","models","discussed","in","Section","6.1",".","4","Wewantedtoidentifyhowgoodanaveragepersonontheinternetisatdetectinglanguagemodeloutputs%2Csowefocusedon","participants","drawn","from","the","general","US","population.","See","Appendix","E","for","details.","25","Mean","accuracy","95%25","Con%EF%AC%81dence","Interval","(low%2C","hi)","t","compared","to","control","(","p","-value)","%E2%80%9CI","don%E2%80%99t","know%E2%80%9D","assignments","Control","(deliberately","bad","model)","86%25","83%25%E2%80%9390%25","-","3.6","%25","GPT-3","Small","76%25","72%25%E2%80%9380%25","3.9","(2","e","-4)","4.9%25","GPT-3","Medium","61%25","58%25%E2%80%9365%25","10.3","(7","e","-21)","6.0%25","GPT-3","Large","68%25","64%25%E2%80%9372%25","7.3","(3","e","-11)","8.7%25","GPT-3","XL","62%25","59%25%E2%80%9365%25","10.7","(1","e","-19)","7.5%25","GPT-3","2.7B","62%25","58%25%E2%80%9365%25","10.4","(5","e","-19)","7.1%25","GPT-3","6.7B","60%25","56%25%E2%80%9363%25","11.2","(3","e","-21)","6.2%25","GPT-3","13B","55%25","52%25%E2%80%9358%25","15.3","(1","e","-32)","7.1%25","GPT-3","175B","52%25","49%25%E2%80%9354%25","16.9","(1","e","-34)","7.8%25","Table","3.11%3A","Human","accuracy","in","identifying","whether","short","(","%18","200","word)","news","articles","are","model","generated",".","We","%EF%AC%81ndthathumanaccuracy(measuredbytheratioofcorrectassignmentstonon-neutralassignments)rangesfrom86%25","onthecontrolmodelto52%25onGPT-3175B.Thistablecomparesmeanaccuracybetween%EF%AC%81vedifferentmodels%2Cand","showstheresultsofatwo-sampleT-Testforthedifferenceinmeanaccuracybetweeneachmodelandthecontrolmodel","(an","unconditional","GPT-3","Small","model","with","increased","output","randomness).","Meanhumanaccuracy(theratioofcorrectassignmentstonon-neutralassignmentsperparticipant)atdetectingthat","theintentionally","badarticles","weremodelgenerated","was","%18","86%25","where50%25","ischance","levelperformance.","Bycontrast%2C","mean","human","accuracy","at","detecting","articles","that","were","produced","by","the","175B","parameter","model","was","barely","above","chance","at","%18","52%25","(see","Table","3.11",").","5","Human","abilities","to","detect","model","generated","text","appear","to","decrease","as","model","size","increases%3A","thereappearstobeatrendtowardschanceaccuracywithmodelsize%2CandhumandetectionofGPT-3isclosetochance.","6","This","is","true","despite","the","fact","that","participants","spend","more","time","on","each","output","as","model","size","increases","(see","Appendix","E",").","ExamplesofsyntheticarticlesfromGPT-3aregiveninFigures","3.14","and","3.15",".","7","Muchofthetextis%E2%80%94asindicatedbythe","evaluations%E2%80%94dif%EF%AC%81cult","for","humans","to","distinguish","from","authentic","human","content.","Factual","inaccuracies","can","be","an","indicator","that","an","article","is","model","generated","since%2C","unlike","human","authors%2C","the","models","have","no","access","to","the","speci%EF%AC%81c","facts","that","the","article","titles","refer","to","or","when","the","article","was","written.","Other","indicators","include","repetition%2C","non","sequiturs%2C","and","unusual","phrasings%2C","though","these","are","often","subtle","enough","that","they","are","not","noticed.","RelatedworkonlanguagemodeldetectionbyIppolitoetal.","%5B","IDCBE19","%5Dindicatesthatautomaticdiscriminatorslike","%20G%20","ROVER","%5B","ZHR","%2B","19","%5D","and","GLTR","%5B","GSR19","%5D","may","have","greater","success","at","detecting","model","generated","text","than","human","evaluators.","Automatic","detection","of","these","models","may","be","a","promising","area","of","future","research.","Ippolitoetal.","%5B","IDCBE19","%5Dalsonotethathumanaccuracyatdetectingmodelgeneratedtextincreasesashumansobserve","moretokens.","Todoapreliminaryinvestigationofhowgoodhumansareatdetectinglongernewsarticlesgenerated","byGPT-3175B%2Cweselected12worldnewsarticlesfromReuterswithanaveragelengthof569wordsandgenerated","completions","of","these","articles","from","GPT-3","with","an","average","length","of","498","words","(298","words","longer","than","our","initial","experiments).","Following","the","methodology","above%2C","we","ran","two","experiments%2C","each","on","around","80","US-based","participants%2C","to","compare","human","abilities","to","detect","the","articles","generated","by","GPT-3","and","a","control","model.","We","found","that","mean","human","accuracy","at","detecting","the","intentionally","bad","longer","articles","from","the","control","model","was","%18","88%25","%2C","while","mean","human","accuracy","at","detecting","the","longer","articles","that","were","produced","by","GPT-3","175B","was","still","barely","abovechanceat","%18","52%25","(seeTable","3.12",").","Thisindicatesthat%2Cfornewsarticlesthatarearound500wordslong%2CGPT-3","continues","to","produce","articles","that","humans","%EF%AC%81nd","dif%EF%AC%81cult","to","distinguish","from","human","written","news","articles.","3.9.5","Learning","and","Using","Novel","Words","Ataskstudiedindevelopmentallinguistics%5B","CB78","%5Distheabilitytolearnandutilizenewwords%2Cforexampleusinga","wordinasentenceafterseeingitde%EF%AC%81nedonlyonce%2Corconverselyinferringaword%E2%80%99smeaningfromonlyoneusage.","Here","we","qualitatively","test","GPT-3%E2%80%99s","ability","to","do","the","former.","Speci%EF%AC%81cally%2C","we","give","GPT-3","the","de%EF%AC%81nition","of","a","nonexistent","word%2C","suchas%E2%80%9CGigamuru%E2%80%9D%2Candthenaskittouseitinasentence.","Weprovideoneto%EF%AC%81vepreviousexamplesofa(separate)","5","We","use","a","two-sample","Student%E2%80%99s","T-Test","to","test","for","signi%EF%AC%81cant","difference","between","the","means","of","the","participant","accuracies","of","each","model","and","the","control","model","and","report","the","normalized","difference","in","the","means","(as","the","t-statistic)","and","the","p-value.","6","Ifamodelconsistentlyproducestextsthataremoreimpressivethanhumanarticles%2Citispossiblethathumanperformanceon","this","task","would","drop","below","50%25.","Indeed%2C","many","individual","participants","scored","below","50%25","on","this","task.","7","Additional","non-news","samples","can","be","found","in","Appendix","F",".","26","Figure","3.13%3A","People%E2%80%99sabilitytoidentifywhethernewsarticlesaremodel-generated(measuredbytheratioofcorrect","assignmentstonon-neutralassignments)decreasesasmodelsizeincreases.","Accuracyontheoutputsonthedeliberately-","bad","control","model","(an","unconditioned","GPT-3","Small","model","with","higher","output","randomness)","is","indicated","with","the","dashed","line","at","the","top%2C","and","the","random","chance","(50%25)","is","indicated","with","the","dashed","line","at","the","bottom.","Line","of","best","%EF%AC%81t","is","a","power","law","with","95%25","con%EF%AC%81dence","intervals.","Mean","accuracy","95%25","Con%EF%AC%81dence","Interval","(low%2C","hi)","t","compared","to","control","(","p","-value)","%E2%80%9CI","don%E2%80%99t","know%E2%80%9D","assignments","Control","88%25","84%25%E2%80%9391%25","-","2.7%25","GPT-3","175B","52%25","48%25%E2%80%9357%25","12.7","(3.2","e","-23)","10.6%25","Table","3.12%3A","People%E2%80%99s","ability","to","identify","whether","%18","500","word","articles","are","model","generated","(as","measured","by","the","ratio","of","correctassignmentstonon-neutralassignments)was88%25onthecontrolmodeland52%25onGPT-3175B.Thistable","shows","the","results","of","a","two-sample","T-Test","for","the","difference","in","mean","accuracy","between","GPT-3","175B","and","the","control","model","(an","unconditional","GPT-3","Small","model","with","increased","output","randomness).","27","Title%3A","United","Methodists","Agree","to","Historic","Split","Subtitle%3A","Those","who","oppose","gay","marriage","will","form","their","own","denomination","Article%3A","Figure","3.14%3A","TheGPT-3generatednewsarticlethathumanshadthegreatestdif%EF%AC%81cultydistinguishingfromahuman","written","article","(accuracy%3A","12%25).","Title%3A","Star's","Tux","Promise","Draws","Megyn","Kelly's","Sarcasm","Subtitle%3A","Joaquin","Phoenix","pledged","to","not","change","for","each","awards","event","Article%3A","Figure","3.15%3A","The","GPT-3","generated","news","article","that","humans","found","the","easiest","to","distinguish","from","a","human","written","article","(accuracy%3A","61%25).","28","A","%22whatpu%22","is","a","small%2C","furry","animal","native","to","Tanzania.","An","example","of","a","sentence","that","uses","the","word","whatpu","is%3A","We","were","traveling","in","Africa","and","we","saw","these","very","cute","whatpus.","To","do","a","%22farduddle%22","means","to","jump","up","and","down","really","fast.","An","example","of","a","sentence","that","uses","the","word","farduddle","is%3A","A","%22yalubalu%22","is","a","type","of","vegetable","that","looks","like","a","big","pumpkin.","An","example","of","a","sentence","that","uses","the","word","yalubalu","is%3A","A","%22Burringo%22","is","a","car","with","very","fast","acceleration.","An","example","of","a","sentence","that","uses","the","word","Burringo","is%3A","A","%22Gigamuru%22","is","a","type","of","Japanese","musical","instrument.","An","example","of","a","sentencethat","uses","the","word","Gigamuru","is%3A","To","%22screeg%22","something","is","to","swing","a","sword","at","it.","An","example","of","a","sentence","thatuses","the","word","screeg","is%3A","Figure3.16%3A","RepresentativeGPT-3completionsforthefew-shottaskofusinganewwordinasentence.","Boldfaceis","GPT-3%E2%80%99scompletions%2Cplaintextishumanprompts.","Inthe%EF%AC%81rstexampleboththepromptandthecompletionareprovided","by","a","human%3B","this","then","serves","as","conditioning","for","subsequent","examples","where","GPT-3","receives","successive","additional","promptsandprovidesthecompletions.","Nothingtask-speci%EF%AC%81cisprovidedtoGPT-3otherthantheconditioningshown","here.","nonexistent","word","being","de%EF%AC%81ned","and","used","in","a","sentence%2C","so","the","task","is","few-shot","in","terms","of","previous","examples","of","the","broadtaskandone-shotintermsofthespeci%EF%AC%81cword.","Table","3.16","showsthe6exampleswegenerated%3Ballde%EF%AC%81nitions","werehuman-generated%2C","andthe%EF%AC%81rstanswerwas","human-generatedasconditioningwhilethe","subsequent","answerswere","generated","by","GPT-3.","These","examples","were","generated","continuously","in","one","sitting","and","we","did","not","omit","or","repeatedly","try","any","prompts.","In","all","cases","the","generated","sentence","appears","to","be","a","correct","or","at","least","plausible","use","of","the","word.","In","the","%EF%AC%81nal","sentence","the","model","generates","a","plausible","conjugation","for","the","word","%E2%80%9Cscreeg%E2%80%9D","(namely","%E2%80%9Cscreeghed%E2%80%9D)%2C","although","the","use","of","the","word","is","slightly","awkward","(%E2%80%9Cscreeghed","at","each","other%E2%80%9D)","despite","being","plausible","in","the","sense","that","it","could","describe","a","toy","sword","%EF%AC%81ght.","Overall%2C","GPT-3","appears","to","be","at","least","pro%EF%AC%81cient","at","the","task","of","using","novel","words","in","a","sentence.","3.9.6","Correcting","English","Grammar","Another","task","well","suited","for","few-shot","learning","is","correcting","English","grammar.","We","test","this","with","GPT-3","in","the","few-","shot","setting","by","giving","prompts","of","the","form","%22Poor","English","Input%3A","%3Csentence%3E","n","n","Good","English","Output%3A","%3Csentence%3E%22",".","WegiveGPT-3onehuman-generatedcorrectionandthenaskittocorrect5more(againwithoutany","omissions","or","repeats).","Results","are","shown","in","Figure","3.17",".","4","Measuring","and","Preventing","Memorization","Of","Benchmarks","Since","our","training","dataset","is","sourced","from","the","internet%2C","it","is","possible","that","our","model","was","trained","on","some","of","our","benchmark","test","sets.","Accurately","detecting","test","contamination","from","internet-scale","datasets","is","a","new","area","of","research","withoutestablishedbestpractices.","Whileitiscommonpracticetotrainlargemodelswithoutinvestigatingcontamination%2C","given","the","increasing","scale","of","pretraining","datasets%2C","we","believe","this","issue","is","becoming","increasingly","important","to","attend","to.","This","concern","is","not","just","hypothetical.","One","of","the","%EF%AC%81rst","papers","to","train","a","language","model","on","Common","Crawl","data","%5B","TL18","%5D","detectedandremovedatrainingdocumentwhichoverlappedwithoneoftheirevaluationdatasets.","Otherworksuch","asGPT-2%5B","RWC","%2B","19","%5Dalsoconductedpost-hocoverlapanalysis.","Theirstudywasrelativelyencouraging%2C%EF%AC%81ndingthat","29","Poor","English","input%3A","I","eated","the","purple","berries.","Good","English","output%3A","I","ate","the","purple","berries.","Poor","English","input%3A","Thank","you","for","picking","me","as","your","designer.","I'd","appreciate","it.","Good","English","output%3A","Thank","you","for","choosing","me","as","your","designer.","I","appreciate","it.","Poor","English","input%3A","The","mentioned","changes","have","done.","or","I","did","the","alteration","that","you","requested.","or","I","changed","things","you","wanted","and","did","the","modifications.","Good","English","output%3A","The","requested","changes","have","been","made.","or","I","madethe","alteration","that","you","requested.","or","I","changed","things","you","wanted","and","made","the","modifications.","Poor","English","input%3A","I'd","be","more","than","happy","to","work","with","you","in","another","project.","Poor","English","input%3A","Please","provide","me","with","a","short","brief","of","the","design","you're","looking","for","and","that'd","be","nice","if","you","could","share","some","examples","or","project","you","did","before.","Poor","English","input%3A","The","patient","was","died.","Poor","English","input%3A","We","think","that","Leslie","likes","ourselves.","Poor","English","input%3A","Janet","broke","Bill","on","the","finger.","Poor","English","input%3A","Mary","arranged","for%2C","in","St.","Louis%2C","John","to","rent","ahouse","cheap.","Poor","English","input%3A","Today","I","have","went","to","the","store","to","to","buys","some","many","bottle","of","water.","Poor","English","input%3A","I","have","tried","to","hit","ball","with","bat%2C","but","my","swing","is","has","miss.","Figure","3.17%3A","Representative","GPT-3","completions","for","the","few-shot","task","of","correcting","English","grammar.","Boldface","is","GPT-3%E2%80%99s","completions%2C","plain","text","is","human","prompts.","In","the","%EF%AC%81rst","few","examples","example","both","the","prompt","and","the","completion","are","provided","by","a","human%3B","this","then","serves","as","conditioning","for","subsequent","examples","where","GPT-3","receives","successiveadditionalpromptsandprovidesthecompletions.","Nothingtask-speci%EF%AC%81cisprovidedtoGPT-3asidefrom","the","%EF%AC%81rst","few","examples","as","conditioningand","the","%E2%80%9CPoor","Englishinput%2FGood","English","output%E2%80%9D","framing.","We","note","thatthe","distinction","between","%E2%80%9Dpoor%E2%80%9D","and","%E2%80%9Dgood%E2%80%9D","English","(and","the","terms","themselves)","is","complex%2C","contextual%2C","and","contested.","As","the","example","mentioning","the","rental","of","a","house","shows%2C","assumptions","that","the","model","makes","about","what","%E2%80%9Cgood%E2%80%9D","is","can","even","lead","it","to","make","errors","(here%2C","the","model","not","only","adjusts","grammar%2C","but","also","removes","the","word","%E2%80%9Dcheap%E2%80%9D","in","a","way","that","alters","meaning).","30","Figure","4.1%3A","GPT-3","Training","Curves","Wemeasuremodelperformanceduringtrainingonadeduplicatedvalidation","split","of","our","training","distribution.","Though","there","is","some","gap","between","training","and","validation","performance%2C","the","gap","grows","only","minimally","with","model","size","and","training","time%2C","suggesting","that","most","of","the","gap","comes","from","a","difference","in","dif%EF%AC%81culty","rather","than","over%EF%AC%81tting.","although","models","did","perform","moderately","better","on","data","that","overlapped","between","training","and","testing%2C","this","did","not","signi%EF%AC%81cantlyimpactreportedresultsduetothesmallfractionofdatawhichwascontaminated(oftenonlyafewpercent).","GPT-3operatesinasomewhatdifferentregime.","Ontheonehand%2Cthedatasetandmodelsizeareabouttwoordersof","magnitudelargerthanthoseusedforGPT-2%2CandincludealargeamountofCommonCrawl%2Ccreatingincreasedpotential","forcontaminationandmemorization.","Ontheotherhand%2Cpreciselyduetothelargeamountofdata%2CevenGPT-3175B","does","not","over%EF%AC%81t","its","training","set","by","a","signi%EF%AC%81cant","amount%2C","measured","relative","to","a","held-out","validation","set","with","which","it","was","deduplicated","(Figure","4.1",").","Thus%2C","we","expect","that","contamination","is","likely","to","be","frequent%2C","but","that","its","effects","may","not","be","as","large","as","feared.","Weinitiallytriedtoaddresstheissueofcontaminationbyproactivelysearchingforandattemptingtoremoveanyoverlap","betweenourtrainingdataandthedevelopmentandtestsetsofallbenchmarksstudiedinthispaper.","Unfortunately%2Ca","bug","resulted","in","only","partial","removal","of","all","detected","overlaps","from","the","training","data.","Due","to","the","cost","of","training%2C","it","wasn%E2%80%99t","feasible","to","retrain","the","model.","To","address","this%2C","we","investigate","in","detail","how","the","remaining","detected","overlap","impacts","results.","For","each","benchmark%2C","we","produce","a","%E2%80%98clean%E2%80%99","version","which","removes","all","potentially","leaked","examples%2C","de%EF%AC%81ned","roughly","as","examplesthathavea13-gramoverlapwithanythinginthepretrainingset(orthatoverlapwiththewholeexamplewhen","itisshorterthan13-grams).","Thegoalistoveryconservatively%EF%AC%82aganythingthatcouldpotentiallybecontamination%2C","soastoproduceacleansubsetthatisfreeofcontaminationwithhighcon%EF%AC%81dence.","Theexactprocedureisdetailedin","Appendix","%20C%20",".","We","then","evaluate","GPT-3","on","these","clean","benchmarks%2C","and","compare","to","the","original","score.","If","the","score","on","the","clean","subset","is","similar","to","the","score","on","the","entire","dataset%2C","this","suggests","that","contamination%2C","even","if","present%2C","does","not","have","a","signi%EF%AC%81cant","effect","on","reported","results.","If","the","score","on","the","clean","subset","is","lower%2C","this","suggests","contamination","may","be","in%EF%AC%82ating","the","results.","The","results","are","summarized","in","Figure","4.2",".","Although","potential","contamination","is","often","high","(with","a","quarter","of","benchmarks","scoring","over","50%25)%2C","in","most","cases","performance","changes","only","negligibly%2C","and","we","see","no","evidence","that","contaminationleveland","performance","differenceare","correlated.","We","concludethat","either","ourconservativemethod","substantially","overestimated","contamination","or","that","contamination","has","little","effect","on","performance.","Below%2C","we","review","in","more","detail","the","few","speci%EF%AC%81c","cases","where","either","(1)","the","model","performs","signi%EF%AC%81cantly","worse","on","thecleanedversion%2Cor(2)potentialcontaminationisveryhigh%2Cwhichmakesmeasuringtheperformancedifference","dif%EF%AC%81cult.","Ouranalysis%EF%AC%82aggedsixgroupsofbenchmarksforfurtherinvestigation%3A","WordScrambling%2CReadingComprehension","(QuAC%2C","SQuAD2%2C","DROP)%2C","PIQA%2C","Winograd%2C","language","modeling","tasks","(Wikitext","tasks%2C","1BW)%2C","and","German","to","English","31","Figure","4.2%3A","Benchmark","contamination","analysis","We","constructed","cleaned","versions","of","each","of","our","benchmarks","to","checkforpotentialcontaminationinourtrainingset.","Thex-axisisaconservativelowerboundforhowmuchofthe","dataset","is","known","with","high","con%EF%AC%81dence","to","be","clean%2C","and","the","y-axis","shows","the","difference","in","performance","when","evaluating","only","on","the","veri%EF%AC%81ed","clean","subset.","Performance","on","most","benchmarks","changed","negligibly%2C","but","some","were","%EF%AC%82agged","for","furtherreview.","Oninspectionwe%EF%AC%81ndsomeevidenceforcontaminationofthePIQAandWinogradresults%2Candwemark","the","corresponding","results","in","Section","3","with","an","asterisk.","We","%EF%AC%81nd","no","evidence","that","other","benchmarks","are","affected.","translation.","Since","our","overlap","analysis","is","designed","to","be","extremely","conservative%2C","we","expect","it","to","produce","some","false","positives.","We","summarize","the","results","for","each","group","of","tasks","below%3A","%E2%80%A2","Reading","Comprehension%3A","Ourinitialanalysis%EF%AC%82agged","%3E","90%25oftaskexamplesfromQuAC%2CSQuAD2%2Cand","DROP","as","potentially","contaminated%2C","so","large","that","even","measuring","the","differential","on","a","clean","subset","was","dif%EF%AC%81cult.","Upon","manual","inspection%2C","however%2C","we","found","that","for","every","overlap","we","inspected%2C","in","all","3","datasets%2C","the","source","text","was","present","in","our","training","data","but","the","question%2Fanswer","pairs","were","not%2C","meaning","the","model","gains","only","background","information","and","cannot","memorize","the","answer","to","a","speci%EF%AC%81c","question.","%E2%80%A2","Germantranslation%3A","Wefound25%25oftheexamplesintheWMT16German-Englishtestsetweremarked","aspotentiallycontaminated%2Cwithanassociatedtotaleffectsizeof1-2BLEU.Uponinspection%2Cnoneofthe","%EF%AC%82aggedexamplescontainpairedsentencesresemblingNMTtrainingdataandcollisionsweremonolingual","matches","mostly","of","snippets","of","events","discussed","in","the","news.","%E2%80%A2","Reversed","Words","and","Anagrams%3A","Recall","that","these","tasks","are","of","the","form","%E2%80%9C","alaok","%3D","koala","%E2%80%9D.","Due","to","the","shortlength","ofthesetasks%2C","weused","2-gramsfor%EF%AC%81ltering","(ignoringpunctuation).","After","inspectingthe%EF%AC%82agged","overlaps%2C","we","found","that","they","were","not","typically","instances","of","real","reversals","or","unscramblings","in","the","training","set%2C","but","rather","palindromes","or","trivial","unscramblings%2C","e.g","%E2%80%9C","kayak","%3D","kayak","%E2%80%9D.","The","amount","of","overlap","was","small%2C","butremovingthetrivialtasksleadtoanincreaseindif%EF%AC%81cultyandthusaspurioussignal.","Relatedtothis%2Cthe","symbol","insertion","task","shows","high","overlap","but","no","effect","on","performance","%E2%80%93","this","is","because","that","task","involves","removing","non-letter","characters","from","a","word%2C","and","the","overlap","analysis","itself","ignores","such","characters%2C","leading","to","many","spurious","matches.","%E2%80%A2","PIQA%3A","The","overlap","analysis","%EF%AC%82agged","29%25","of","examples","as","contaminated%2C","and","observed","a","3","percentage","point","absolute","decrease","(4%25","relative","decrease)","in","performance","on","the","clean","subset.","Though","the","test","dataset","was","released","after","our","training","set","was","created","and","its","labels","are","hidden%2C","some","of","the","web","pages","used","by","the","crowdsourced","dataset","creators","are","contained","in","our","training","set.","We","found","a","similar","decrease","in","a","25x","smaller","model","with","much","less","capacity","to","memorize%2C","leading","us","to","suspect","that","the","shift","is","likely","statistical","bias","ratherthanmemorization%3Bexampleswhichworkerscopiedmaysimplybeeasier.","Unfortunately%2Cwecannot","rigorously","prove","this","hypothesis.","We","therefore","mark","our","PIQA","results","with","an","asterisk","to","denote","this","potential","contamination.","%E2%80%A2","Winograd%3A","The","overlap","analysis","%EF%AC%82agged","45%25","of","examples%2C","and","found","a","2.6%25","decrease","in","performance","on","the","clean","subset.","Manual","inspection","of","the","overlapping","data","point","showed","that","132","Winograd","schemas","were","in","factpresentinourtrainingset%2Cthoughpresentedinadifferentformatthanwepresentthetasktothemodel.","Although","the","decrease","in","performance","is","small%2C","we","mark","our","Winograd","results","in","the","main","paper","with","an","asterisk.","32","%E2%80%A2","Languagemodeling%3A","Wefoundthe4WikipedialanguagemodelingbenchmarksmeasuredinGPT-2%2Cplusthe","Children%E2%80%99sBookTestdataset%2Ctobealmostentirelycontainedinourtrainingdata.","Sincewecannotreliably","extractacleansubsethere%2Cwedonotreportresultsonthesedatasets%2Ceventhoughweintendedtowhenstarting","thiswork.","WenotethatPennTreeBankduetoitsagewasunaffectedandthereforebecameourchieflanguage","modeling","benchmark.","Wealsoinspecteddatasetswherecontaminationwashigh%2Cbuttheimpactonperformancewasclosetozero%2Csimply","to","verify","how","much","actual","contamination","existed.","These","appeared","to","often","contain","false","positives.","They","had","either","noactualcontamination%2Corhadcontaminationthatdidnotgiveawaytheanswertothetask.","Onenotableexception","wasLAMBADA%2C","whichappearedto","have","substantialgenuinecontamination%2C","yetthe","impactonperformance","wasvery","small%2C","with","the","clean","subset","scoring","within","0.5%25","of","the","full","dataset.","Also%2C","strictly","speaking%2C","our","%EF%AC%81ll-in-the-blank","format","precludes","the","simplest","form","of","memorization.","Nevertheless%2C","since","we","made","very","large","gains","on","LAMBADA","in","this","paper%2C","the","potential","contamination","is","noted","in","the","results","section.","An","important","limitation","of","our","contamination","analysis","is","that","we","cannot","be","sure","that","the","clean","subset","is","drawn","from","the","same","distribution","as","the","original","dataset.","It","remains","possible","that","memorization","in%EF%AC%82ates","results","but","at","the","same","time","is","precisely","counteracted","by","some","statistical","bias","causing","the","clean","subset","to","be","easier.","However%2C","the","sheer","number","of","shifts","close","to","zero","suggests","this","is","unlikely%2C","and","we","also","observed","no","noticeable","difference","in","the","shifts","for","small","models%2C","which","are","unlikely","to","be","memorizing.","Overall%2C","we","have","made","a","best","effort","to","measure","and","document","the","effects","of","data","contamination%2C","and","to","note","or","outright","removeproblematicresults%2Cdependingontheseverity.","Muchworkremainstobedonetoaddressthisimportantand","subtleissueforthe%EF%AC%81eldingeneral%2Cbothwhendesigningbenchmarksandwhentrainingmodels.","Foramoredetailed","explanation","of","our","analysis%2C","we","refer","the","reader","to","Appendix","%20C%20",".","5","Limitations","GPT-3","and","our","analysis","of","it","have","a","number","of","limitations.","Below","we","describe","some","of","these","and","suggest","directions","for","future","work.","First%2C","despite","the","strong","quantitative","and","qualitative","improvements","of","GPT-3%2C","particularly","compared","to","its","direct","predecessor","GPT-2%2C","it","still","has","notable","weaknesses","in","text","synthesis","and","several","NLP","tasks.","On","text","synthesis%2C","although","the","overall","quality","is","high%2C","GPT-3","samples","still","sometimes","repeat","themselves","semantically","at","the","document","level%2C","start","to","lose","coherence","over","suf%EF%AC%81ciently","long","passages%2C","contradict","themselves%2C","and","occasionally","contain","non-sequitur","sentences","orparagraphs.","Wewillreleaseacollectionof500uncuratedunconditionalsamplestohelpprovideabettersenseof","GPT-3%E2%80%99s","limitations","and","strengths","at","text","synthesis.","Within","the","domain","of","discrete","language","tasks%2C","we","have","noticed","informally","that","GPT-3","seems","to","have","special","dif%EF%AC%81culty","with","%E2%80%9Ccommon","sense","physics%E2%80%9D%2C","despite","doing","well","on","some","datasets(suchasPIQA%5B","BZB","%2B","19","%5D)thattestthisdomain.","Speci%EF%AC%81callyGPT-3hasdif%EF%AC%81cultywithquestionsofthetype","%E2%80%9CIf","I","put","cheese","into","the","fridge%2C","will","it","melt%3F%E2%80%9D.","Quantitatively%2C","GPT-3%E2%80%99s","in-context","learning","performance","has","some","notable","gaps","on","our","suite","of","benchmarks%2C","as","described","in","Section","3","%2C","and","in","particular","it","does","little","better","than","chance","when","evaluated","one-shot","or","even","few-shot","on","some","%E2%80%9Ccomparison%E2%80%9D","tasks%2C","such","as","determining","if","two","words","are","used","the","same","wayin","asentence%2C","orifone","sentenceimplies","another(WICand","ANLIrespectively)%2C","aswellas","ona","subsetofreading","comprehension","tasks.","This","is","especially","striking","given","GPT-3%E2%80%99s","strong","few-shot","performance","on","many","other","tasks.","GPT-3hasseveralstructural","andalgorithmiclimitations%2C","whichcouldaccountforsomeoftheissuesabove.","Wefocused","on","exploring","in-context","learning","behavior","in","autoregressive","language","models","because","it","is","straightforward","to","both","sampleandcomputelikelihoodswiththismodelclass.","Asaresultourexperimentsdonotincludeanybidirectional","architectures","or","other","training","objectives","such","as","denoising.","This","is","a","noticeable","difference","from","much","of","the","recent","literature%2C","which","has","documented","improved","%EF%AC%81ne-tuning","performance","when","using","these","approaches","over","standard","language","models","%5B","RSR","%2B","19","%5D.","Thus","our","design","decision","comes","at","the","cost","of","potentially","worse","performance","on","tasks","which","empirically","bene%EF%AC%81t","from","bidirectionality.","This","may","include","%EF%AC%81ll-in-the-blank","tasks%2C","tasks","that","involve","looking","back","and","comparing","two","pieces","of","content%2C","or","tasks","that","require","re-reading","or","carefully","considering","a","long","passage","and","then","generatingaveryshortanswer.","ThiscouldbeapossibleexplanationforGPT-3%E2%80%99slaggingfew-shotperformanceona","few","ofthe","tasks%2C","suchas","WIC(which","involves","comparingthe","useof","a","wordin","twosentences)%2C","ANLI","(whichinvolves","comparing","two","sentences","to","see","if","one","implies","the","other)%2C","and","several","reading","comprehension","tasks","(e.g.","QuAC","and","RACE).","Wealso","conjecture%2Cbased","onpast","literature%2C","thata","largebidirectional","modelwould","bestronger","at%EF%AC%81ne-tuning","than","GPT-3.","Making","a","bidirectional","model","at","the","scale","of","GPT-3%2C","and%2For","trying","to","make","bidirectional","models","work","with","few-orzero-shotlearning%2Cisapromisingdirectionforfutureresearch%2Candcouldhelpachievethe%E2%80%9Cbestofbothworlds%E2%80%9D.","Amorefundamentallimitationofthegeneralapproachdescribedinthispaper%E2%80%93scalingupanyLM-likemodel%2Cwhether","autoregressiveorbidirectional%E2%80%93isthatitmayeventuallyruninto(orcouldalreadyberunninginto)thelimitsofthe","33","pretraining","objective.","Our","current","objective","weights","every","token","equally","and","lacks","a","notion","of","what","is","most","important","to","predict","and","what","is","less","important.","%5B","RRS20","%5D","demonstrate","bene%EF%AC%81ts","of","customizing","prediction","to","entities","of","interest.","Also%2C","with","self-supervised","objectives%2C","task","speci%EF%AC%81cationrelies","on","forcing","the","desired","task","into","a","prediction","problem%2C","whereas","ultimately%2Cusefullanguagesystems(forexamplevirtualassistants)mightbebetterthoughtofastakinggoal-directed","actionsratherthanjustmakingpredictions.","Finally%2Clargepretrainedlanguagemodelsarenotgroundedinotherdomains","of","experience%2C","such","as","video","or","real-world","physical","interaction%2C","and","thus","lack","a","large","amount","of","context","about","the","world","%5B","BHT","%2B","20","%5D.","For","all","these","reasons%2C","scaling","pure","self-supervised","prediction","is","likely","to","hit","limits%2C","and","augmentation","with","a","different","approach","is","likely","to","be","necessary.","Promising","future","directions","in","this","vein","might","include","learning","the","objective","functionfromhumans%5B","ZSW","%2B","19a","%5D%2C%EF%AC%81ne-tuningwithreinforcementlearning%2Coraddingadditionalmodalitiessuchas","images","to","provide","grounding","and","a","better","model","of","the","world","%5B","CLY","%2B","19","%5D.","Another","limitation","broadly","shared","by","language","models","is","poor","sample","ef%EF%AC%81ciency","during","pre-training.","While","GPT-3","takes","a","step","towards","test-time","sample","ef%EF%AC%81ciency","closer","to","that","of","humans","(one-shot","or","zero-shot)%2C","it","still","sees","much","more","text","during","pre-training","than","a","human","sees","in","the","their","lifetime","%5B","Lin20","%5D.","Improving","pre-training","sample","ef%EF%AC%81ciency","is","an","important","direction","for","future","work%2C","and","might","come","from","grounding","in","the","physical","world","to","provide","additional","information%2C","or","from","algorithmic","improvements.","Alimitation%2Coratleastuncertainty%2Cassociatedwithfew-shotlearninginGPT-3isambiguityaboutwhetherfew-shot","learning","actually","learns","new","tasks","%E2%80%9Cfrom","scratch%E2%80%9D","at","inference","time%2C","or","if","it","simply","recognizes","and","identi%EF%AC%81es","tasks","that","it","has","learned","during","training.","These","possibilities","exist","on","a","spectrum%2C","ranging","from","demonstrations","in","the","training","set","that","are","drawn","from","exactly","the","same","distribution","as","those","at","test","time%2C","to","recognizing","the","same","task","but","in","a","different","format%2C","to","adapting","to","a","speci%EF%AC%81c","style","of","a","general","task","such","as","QA%2C","to","learning","a","skill","entirely","de","novo.","Where","GPT-3","is","on","this","spectrum","may","also","vary","from","task","to","task.","Synthetic","tasks","such","as","wordscrambling","or","de%EF%AC%81ning","nonsense","words","seemespeciallylikelytobelearneddenovo%2Cwhereastranslationclearlymustbelearnedduringpretraining%2Calthough","possibly","from","data","that","is","very","different","in","organization","and","style","than","the","test","data.","Ultimately%2C","it","is","not","even","clear","what","humans","learn","from","scratch","vs","from","prior","demonstrations.","Even","organizing","diverse","demonstrations","during","pre-training","andidentifyingthemattesttimewouldbeanadvanceforlanguagemodels%2Cbutneverthelessunderstandingprecisely","how","few-shot","learning","works","is","an","important","unexplored","direction","for","future","research.","A","limitation","associated","with","models","at","the","scale","of","GPT-3%2C","regardless","of","objective","function","or","algorithm%2C","is","that","they","are","both","expensive","and","inconvenient","to","perform","inference","on%2C","which","may","present","a","challenge","for","practical","applicability","of","models","of","this","scale","in","their","current","form.","One","possible","future","direction","to","address","this","is","distillation","%5B","HVD15","%5D","of","large","models","down","to","a","manageable","size","for","speci%EF%AC%81c","tasks.","Large","models","such","as","GPT-3","contain","a","very","wide","range","of","skills%2C","mostofwhicharenotneededforaspeci%EF%AC%81ctask%2Csuggestingthatinprincipleaggressivedistillationmaybepossible.","Distillation","is","well-explored","in","general","%5B","LHCG19a","%5D","but","has","not","been","tried","at","the","scale","of","hundred","of","billions","parameters%3B","new","challenges","and","opportunities","may","be","associated","with","applying","it","to","models","of","this","size.","Finally%2CGPT-3sharessomelimitationscommontomostdeeplearningsystems%E2%80%93itsdecisionsarenoteasilyinterpretable%2C","it","is","not","necessarily","well-calibrated","in","its","predictions","on","novel","inputs","as","observed","by","the","much","higher","variance","in","performance","than","humans","on","standard","benchmarks%2C","and","it","retains","the","biases","of","the","data","it","has","been","trained","on.","This","last","issue","%E2%80%93","biases","in","the","data","that","may","lead","the","model","to","generate","stereotyped","or","prejudiced","content","%E2%80%93","is","of","special","concernfromasocietalperspective%2CandwillbediscussedalongwithotherissuesinthenextsectiononBroaderImpacts","(Section","6",").","6","Broader","Impacts","Language","models","have","a","wide","range","of","bene%EF%AC%81cial","applications","for","society%2C","including","code","and","writing","auto-completion%2C","grammar","assistance%2C","game","narrative","generation%2C","improving","search","engine","responses%2C","and","answering","questions.","But","theyalsohavepotentiallyharmfulapplications.","GPT-3improvesthequalityoftextgenerationandadaptabilityover","smaller","models","and","increases","the","dif%EF%AC%81culty","of","distinguishing","synthetic","text","from","human-written","text.","It","therefore","has","the","potential","to","advance","both","the","bene%EF%AC%81cial","and","harmful","applications","of","language","models.","Here","wefocus","onthe","potentialharms","ofimproved","languagemodels%2C","not","becausewe","believethe","harmsare","necessarily","greater%2Cbutinordertostimulateeffortstostudyandmitigatethem.","Thebroaderimpactsoflanguagemodelslikethis","arenumerous.","Wefocusontwoprimaryissues%3A","thepotentialfordeliberatemisuseoflanguagemodelslikeGPT-3in","Section","6.1","%2Candissuesofbias%2Cfairness%2CandrepresentationwithinmodelslikeGPT-3inSection","6.2",".","Wealsobrie%EF%AC%82y","discuss","issues","of","energy","ef%EF%AC%81ciency","(Section","6.3",").","34","6.1","Misuse","of","Language","Models","Malicious","uses","of","language","models","can","be","somewhat","dif%EF%AC%81cult","to","anticipate","because","they","often","involve","repurposing","language","models","in","a","very","different","environment","or","for","a","different","purpose","than","researchers","intended.","To","help","with","this%2C","wecanthinkintermsoftraditionalsecurityriskassessmentframeworks%2Cwhichoutlinekeystepssuchasidentifying","threats","and","potential","impacts%2C","assessing","likelihood%2C","and","determining","risk","as","a","combination","of","likelihood","and","impact","%5B","Ros12","%5D.","We","discuss","three","factors%3A","potential","misuse","applications%2C","threat","actors%2C","and","external","incentive","structures.","6.1.1","Potential","Misuse","Applications","Anysociallyharmfulactivitythatreliesongeneratingtextcouldbeaugmentedbypowerfullanguagemodels.","Examples","include","misinformation%2C","spam%2C","phishing%2C","abuse","of","legal","and","governmental","processes%2C","fraudulent","academic","essay","writing","andsocialengineeringpretexting.","Manyoftheseapplicationsbottleneckonhumanbeingstowritesuf%EF%AC%81cientlyhigh","qualitytext.","Languagemodelsthatproducehighqualitytextgenerationcouldlowerexistingbarrierstocarryingout","these","activities","and","increase","their","ef%EF%AC%81cacy.","Themisusepotentialoflanguagemodelsincreasesasthequalityoftextsynthesisimproves.","TheabilityofGPT-3to","generate","several","paragraphs","of","synthetic","content","that","people","%EF%AC%81nd","dif%EF%AC%81cult","to","distinguish","from","human-written","text","in","3.9.4","represents","a","concerning","milestone","in","this","regard.","6.1.2","Threat","Actor","Analysis","Threatactorscanbeorganizedbyskillandresourcelevels%2Crangingfromlowormoderatelyskilledandresourcedactors","whomaybeabletobuildamaliciousproductto%E2%80%98advancedpersistentthreats%E2%80%99(APTs)%3A","highlyskilledandwell-resourced","(e.g.","state-sponsored)","groups","with","long-term","agendas","%5B","SBC","%2B","19","%5D.","Tounderstandhowlowandmid-skillactorsthinkaboutlanguagemodels%2Cwehavebeenmonitoringforumsandchat","groups","where","misinformation","tactics%2C","malware","distribution%2C","and","computer","fraud","are","frequently","discussed.","While","we","did","%EF%AC%81nd","signi%EF%AC%81cant","discussion","of","misuse","following","the","initial","release","of","GPT-2","in","spring","of","2019%2C","we","found","fewer","instances","ofexperimentation","andno","successfuldeployments","sincethen.","Additionally%2Cthose","misuse","discussionswere","correlated","withmediacoverageoflanguagemodeltechnologies.","Fromthis%2C","weassessthatthethreatofmisusefromtheseactorsis","not","immediate%2C","but","signi%EF%AC%81cant","improvements","in","reliability","could","change","this.","BecauseAPTsdonottypicallydiscussoperationsintheopen%2Cwehaveconsultedwithprofessionalthreatanalystsabout","possibleAPTactivityinvolvingtheuseoflanguagemodels.","SincethereleaseofGPT-2therehasbeennodiscernible","difference","in","operations","that","may","see","potential","gains","by","using","language","models.","The","assessment","was","that","language","modelsmaynotbeworthinvestingsigni%EF%AC%81cantresourcesinbecausetherehasbeennoconvincingdemonstrationthat","current","language","models","are","signi%EF%AC%81cantly","better","than","current","methods","for","generating","text%2C","and","because","methods","for","%E2%80%9Ctargeting%E2%80%9D","or","%E2%80%9Ccontrolling%E2%80%9D","the","content","of","language","models","are","still","at","a","very","early","stage.","6.1.3","External","Incentive","Structures","Each","threat","actor","group","also","has","a","set","of","tactics%2C","techniques%2C","and","procedures","(TTPs)","that","they","rely","on","to","accomplish","their","agenda.","TTPsarein%EF%AC%82uencedbyeconomicfactorslikescalabilityandeaseofdeployment%3Bphishingisextremelypopular","amongallgroupsbecauseitoffersalow-cost%2Clow-effort%2Chigh-yieldmethodofdeployingmalwareandstealinglogin","credentials.","Using","language","models","to","augment","existing","TTPs","would","likely","result","in","an","even","lower","cost","of","deployment.","Easeofuseisanothersigni%EF%AC%81cantincentive.","HavingstableinfrastructurehasalargeimpactontheadoptionofTTPs.","Theoutputsoflanguagemodelsarestochastic%2Chowever%2Candthoughdeveloperscanconstrainthese(e.g.","usingtop-k","truncation)","they","are","not","able","to","perform","consistently","without","human","feedback.","If","a","social","media","disinformation","bot","producesoutputsthatarereliable99%25ofthetime%2Cbutproducesincoherentoutputs1%25ofthetime%2Cthiscouldreducethe","amount","of","human","labor","required","in","operating","thisbot.","But","a","human","is","still","needed","to","%EF%AC%81lter","the","outputs%2C","which","restricts","how","scalable","the","operation","can","be.","Based","on","our","analysis","of","this","model","and","analysis","of","threat","actors","and","the","landscape%2C","we","suspect","AI","researchers","will","eventually","develop","language","models","that","are","suf%EF%AC%81ciently","consistent","and","steerable","that","they","will","be","of","greater","interest","to","maliciousactors.","Weexpectthiswillintroducechallengesforthebroaderresearchcommunity%2Candhopetoworkon","this","through","a","combination","of","mitigation","research%2C","prototyping%2C","and","coordinating","with","other","technical","developers.","35","6.2","Fairness%2C","Bias%2C","and","Representation","Biases","present","in","training","data","may","lead","models","to","generate","stereotyped","or","prejudiced","content.","This","is","concerning%2C","sincemodelbiascouldharmpeopleintherelevantgroupsindifferentwaysbyentrenchingexistingstereotypesand","producingdemeaningportrayalsamongstotherpotentialharms%5B","Cra17","%5D.","Wehaveconductedananalysisofbiasesin","the","model","in","order","to","better","understand","GPT-3%E2%80%99s","limitations","when","it","comes","to","fairness%2C","bias%2C","and","representation.","8","Our","goal","is","not","to","exhaustively","characterize","GPT-3%2C","but","to","give","a","preliminary","analysis","of","some","of","its","limitations","and","behaviors.","Wefocusonbiasesrelatingtogender%2Crace%2Candreligion%2Calthoughmanyothercategoriesofbiasarelikely","presentandcouldbestudiedinfollow-upwork.","Thisisapreliminaryanalysisanddoesnotre%EF%AC%82ectallofthemodel%E2%80%99s","biases","even","within","the","studied","categories.","Broadly%2C","our","analysis","indicates","that","internet-trained","models","have","internet-scale","biases%3B","models","tend","to","re%EF%AC%82ect","stereotypes","presentintheirtrainingdata.","Belowwediscussourpreliminary%EF%AC%81ndingsofbiasalongthedimensionsofgender%2C","race%2C","and","religion.","We","probe","for","bias","in","the","175","billion","parameter","model","and","also","in","similar","smaller","models%2C","to","see","if","and","how","they","are","different","in","this","dimension.","6.2.1","Gender","InourinvestigationofgenderbiasinGPT-3%2Cwefocusedonassociationsbetweengenderandoccupation.","Wefound","thatoccupationsingeneralhaveahigherprobabilityofbeingfollowedbyamalegenderidenti%EF%AC%81erthanafemaleone","(in","other","words%2C","they","are","male","leaning)","when","given","a","context","such","as","%22The","f","occupation","g","was","a%22","(Neutral","Variant).","83%25ofthe388occupationswetestedweremorelikelytobefollowedbyamaleidenti%EF%AC%81erbyGPT-3.","Wemeasured","this","by","feeding","the","model","a","context","such","as","%22The","detective","was","a%22","and","then","looking","at","the","probability","of","the","model","following","up","with","male","indicating","words(eg.","man%2C","male","etc.)","or","female","indicating","words","(woman%2C","female","etc.).","In","particular%2C","occupations","demonstrating","higher","levels","of","education","such","as","legislator%2C","banker%2C","or","professor","emeritus","were","heavily","male","leaning","along","with","occupations","that","require","hard","physical","labour","such","as","mason%2C","millwright%2C","and","sheriff.","Occupationsthatweremorelikelytobefollowedbyfemaleidenti%EF%AC%81ersincludemidwife%2Cnurse%2Creceptionist%2C","housekeeper","etc.","Wealsotestedhowtheseprobabilitieschangedwhenweshiftedthecontexttobethe","%22The","competent","f","occupation","g","was","a%22","(Competent","Variant)%2C","and","when","we","shifted","the","context","to","be","%22The","incompetent","f","occupation","g","was","a%22","(Incompetent","Variant)","for","each","occupation","in","the","dataset.","We","found","that%2C","when","prompted","with","%22The","competent","f","occupation","g","was","a%2C%22","the","majority","of","occupations","had","an","even","higher","probability","of","being","followed","by","a","male","identi%EF%AC%81er","than","a","female","one","than","was","the","case","with","our","original","neutral","prompt%2C","%22The","f","occupation","g","was","a%22",".","With","the","prompt","%22The","incompetent","f","occupation","g","was","a%22","the","majority","of","occupations","still","leaned","male","with","a","similar","probability","than","for","our","original","neutral","prompt.","The","average","occupation","bias","-","measured","as","1","n","jobs","P","jobs","log(","P","(female","j","Context)","P","(male","j","Context))",")","-","was","%00","1","%3A","11","for","the","Neutral","Variant%2C","%00","2","%3A","14","for","the","Competent","Variant","and","%00","1","%3A","15","for","the","Incompetent","Variant.","We","also","carried","out","pronoun","resolution","on","the","Winogender","dataset","%5B","RNLVD18","%5D","using","two","methods","which","further","corroborated","the","model%E2%80%99s","tendency","to","associate","most","occupations","with","males.","One","method","measured","the","mod-","els","ability","to","correctly","assign","a","pronoun","as","the","occupation","or","the","participant.","For","example%2C","we","fed","the","model","acontextsuchas","%22The","advisor","met","with","the","advisee","because","she","wanted","to","get","advice","about","job","applications.","%60She'","refers","to","the%22","and","found","the","option","with","the","lowest","probability","between","the","two","possi-","ble","options","(Choices","between","Occupation","Option%3A","advisor%3B","Participant","Option%3A","advisee).","Occupation","and","participant","words","often","have","societal","biases","associated","with","them","such","as","the","assumption","that","most","occupantsarebydefaultmale.","Wefoundthatthelanguagemodelslearntsomeofthesebiasessuchasatendencyto","associatefemalepronounswithparticipantpositionsmorethanmalepronouns.","GPT-3175Bhadthehighestaccuracyof","all","the","models","(64.17%25)","on","this","task.","It","was","also","the","only","model","where","the","accuracy","for","Occupant","sentences","(sentences","where","the","correct","answer","was","the","Occupation","option)","for","females","was","higher","than","for","males","(81.7%25","vs","76.7%25).","All","other","models","had","a","higher","accuracy","for","male","pronouns","with","Occupation","sentences","as","compared","to","female","pronouns","with","the","exception","of","our","second","largest","model-","GPT-3","13B","-","which","had","the","same","accuracy","(60%25)","for","both.","This","offers","some","preliminary","evidence","that","in","places","where","issues","of","bias","can","make","language","models","susceptible","to","error%2C","the","larger","models","are","more","robust","than","smaller","models.","We","also","performed","co-occurrence","tests%2C","where","we","analyzed","which","words","are","likely","to","occur","in","the","vicinity","of","other","pre-","selected","words.","We","created","a","model","output","sample","set","by","generating","800","outputs","of","length","50","each","with","a","temperature","8","Evaluatingfairness%2Cbias%2Candrepresentationinlanguagemodelsisarapidly-developingareawithalargebodyofpriorwork.","See%2C","for","example%2C","%5B","HZJ","%2B","19","%2C","NBR20","%2C","SCNP19","%5D.","36","Table","6.1%3A","Most","Biased","Descriptive","Words","in","175B","Model","Top10MostBiasedMaleDescriptiveWordswithRaw","Co-Occurrence","Counts","Top10MostBiasedFemaleDescriptiveWordswithRaw","Co-Occurrence","Counts","AverageNumberofCo-OccurrencesAcrossAllWords%3A","17.5","AverageNumberofCo-OccurrencesAcrossAllWords%3A","23.9","Large","(16)","Optimistic","(12)","Mostly","(15)","Bubbly","(12)","Lazy","(14)","Naughty","(12)","Fantastic","(13)","Easy-going","(12)","Eccentric","(13)","Petite","(10)","Protect","(10)","Tight","(10)","Jolly","(10)","Pregnant","(10)","Stable","(9)","Gorgeous","(28)","Personable","(22)","Sucked","(8)","Survive","(7)","Beautiful","(158)","of","1","and","top","p","of","0.9","for","every","prompt","in","our","dataset.","For","gender%2C","we","had","prompts","such","as","%22He","was","very%22","%2C","%22She","was","very%22","%2C","%22He","would","be","described","as%22","%2C","%22She","would","be","described","as%22","9",".","We","looked","at","the","adjectives","and","adverbsinthetop100mostfavoredwordsusinganoff-the-shelfPOStagger%5B","LB02","%5D.","Wefoundfemalesweremore","oftendescribedusingappearanceorientedwordssuchas%E2%80%9Dbeautiful%E2%80%9Dand%E2%80%9Dgorgeous%E2%80%9Dascomparedtomenwhowere","more","often","described","using","adjectives","that","span","a","greater","spectrum.","Table","6.1","shows","the","top","10","most","favored","descriptive","words","for","the","model","along","with","the","raw","number","of","times","each","wordco-occurredwithapronoun","indicator.","%E2%80%9CMostFavored%E2%80%9Dhereindicateswordswhichwere","most","skewedtowardsa","category","by","co-occurring","with","it","at","a","higher","rate","as","compared","to","the","other","category.","To","put","these","numbers","in","perspective%2C","we","have","also","included","the","average","for","the","number","of","co-occurrences","across","all","qualifying","words","for","each","gender.","6.2.2","Race","To","investigate","racial","bias","in","GPT-3%2C","we","seeded","the","model","with","prompts","such","as","-","%22The","f","race","g","man","was","very%22","%2C","%22The","f","race","g","woman","was","very%22","and","%22People","would","describe","the","f","race","g","person","as%22","and","generated","800","samplesforeachoftheaboveprompts%2Cwith","f","race","g","replacedwithatermindicatingaracialcategorysuchasWhite","or","Asian.","We","then","measure","word","co-occurrences","in","the","generated","samples.","Given","prior","research","demonstrating","that","language","models","produce","text","of","differing","sentiment","when","varying","features","such","as","occupation","%5B","HZJ","%2B","19","%5D%2C","we","explored","how","race","impacted","sentiment.","We","measured","sentiment","using","Senti","WordNet","%5B","BES10","%5D","for","the","words","which","co-occurred","disproportionatelywitheachrace.","Eachwordsentimentvariedfrom100to-100%2Cwithpositivescoresindicatingpositive","words(eg.","wonderfulness%3A","100%2Camicable%3A","87.5)%2Cnegativescoresindicatingnegativewords(eg.","wretched%3A","-87.5%2Chorrid%3A","-87.5)","and","a","score","of","0","indicating","neutral","words","(eg.","sloping%2C","chalet).","Itshouldbenotedthatwewereexplicitlypromptingthemodelstotalkaboutraceandthisinturngeneratedtextthat","focused","on","racial","features%3B","these","results","are","not","from","the","models","talking","about","race","in","the","wild","but","talking","about","race","in","an","experimental","setup","where","they","have","been","primed","to","do","so.","Additionally%2C","since","we","are","measuring","sentiment","by","simply","looking","at","word","co-occurrences%2C","the","resulting","sentiment","can","re%EF%AC%82ect","socio-historical","factors","-","for","instance%2C","text","relating","to","adiscussionofslaverywillfrequentlyhaveanegativesentiment%2Cwhichmayleadtoademographicbeingassociated","with","a","negative","sentiment","under","this","testing","methodology.","Acrossthemodelsweanalyzed%2C%E2%80%98Asian%E2%80%99hadaconsistentlyhighsentiment-itranked1stin3outof7models.","Onthe","other","hand%2C","%E2%80%99Black%E2%80%99","had","a","consistently","low","sentiment","-","it","ranked","the","lowest","in","5","out","of","7","models.","These","differences","narrowed","marginally","on","the","larger","model","sizes.","This","analysis","gives","a","sense","of","the","biases","of","different","models","and","highlights","the","need","for","more","sophisticated","analysis","of","the","relationship","between","sentiment%2C","entities%2C","and","input","data.","9","We","only","used","male","and","female","pronouns.","This","simplifying","assumption","makes","it","easier","to","study","co-occurrence","since","it","does","not","require","the","isolation","of","instances","in","which","%E2%80%98they%E2%80%99","refers","to","a","singular","noun","from","those","where","it","didn%E2%80%99t%2C","but","other","forms","of","gender","bias","are","likely","present","and","could","be","studied","using","different","approaches.","37","Figure","6.1%3A","Racial","Sentiment","Across","Models","Religion","Most","Favored","Descriptive","Words","Atheism","%E2%80%98Theists%E2%80%99%2C%E2%80%98Cool%E2%80%99%2C%E2%80%98Agnostics%E2%80%99%2C%E2%80%98Mad%E2%80%99%2C%E2%80%98Theism%E2%80%99%2C%E2%80%98Defensive%E2%80%99%2C%E2%80%98Complaining%E2%80%99%2C%E2%80%98Correct%E2%80%99%2C%E2%80%98Arrogant%E2%80%99%2C","%E2%80%98Characterized%E2%80%99","Buddhism","%E2%80%98Myanmar%E2%80%99%2C%E2%80%98Vegetarians%E2%80%99%2C%E2%80%98Burma%E2%80%99%2C%E2%80%98Fellowship%E2%80%99%2C%E2%80%98Monk%E2%80%99%2C%E2%80%98Japanese%E2%80%99%2C%E2%80%98Reluctant%E2%80%99%2C%E2%80%98Wisdom%E2%80%99%2C%E2%80%98En-","lightenment%E2%80%99%2C","%E2%80%98Non-Violent%E2%80%99","Christianity","%E2%80%98Attend%E2%80%99%2C","%E2%80%98Ignorant%E2%80%99%2C","%E2%80%98Response%E2%80%99%2C","%E2%80%98Judgmental%E2%80%99%2C","%E2%80%98Grace%E2%80%99%2C","%E2%80%98Execution%E2%80%99%2C","%E2%80%98Egypt%E2%80%99%2C","%E2%80%98Continue%E2%80%99%2C","%E2%80%98Com-","ments%E2%80%99%2C","%E2%80%98Of%EF%AC%81cially%E2%80%99","Hinduism","%E2%80%98Caste%E2%80%99%2C%E2%80%98Cows%E2%80%99%2C%E2%80%98BJP%E2%80%99%2C%E2%80%98Kashmir%E2%80%99%2C%E2%80%98Modi%E2%80%99%2C%E2%80%98Celebrated%E2%80%99%2C%E2%80%98Dharma%E2%80%99%2C%E2%80%98Pakistani%E2%80%99%2C%E2%80%98Originated%E2%80%99%2C%E2%80%98Africa%E2%80%99","Islam","%E2%80%98Pillars%E2%80%99%2C","%E2%80%98Terrorism%E2%80%99%2C","%E2%80%98Fasting%E2%80%99%2C","%E2%80%98Sheikh%E2%80%99%2C","%E2%80%98Non-Muslim%E2%80%99%2C","%E2%80%98Source%E2%80%99%2C","%E2%80%98Charities%E2%80%99%2C","%E2%80%98Levant%E2%80%99%2C","%E2%80%98Allah%E2%80%99%2C","%E2%80%98Prophet%E2%80%99","Judaism","%E2%80%98Gentiles%E2%80%99%2C","%E2%80%98Race%E2%80%99%2C","%E2%80%98Semites%E2%80%99%2C","%E2%80%98Whites%E2%80%99%2C","%E2%80%98Blacks%E2%80%99%2C","%E2%80%98Smartest%E2%80%99%2C","%E2%80%98Racists%E2%80%99%2C","%E2%80%98Arabs%E2%80%99%2C","%E2%80%98Game%E2%80%99%2C","%E2%80%98Russian%E2%80%99","Table","6.2%3A","Shows","the","ten","most","favored","words","about","each","religion","in","the","GPT-3","175B","model.","6.2.3","Religion","Westudiedwhichwordsco-occurredwithreligioustermsrelatingtoAtheism%2CBuddhism%2CChristianity%2CHinduism%2CIslam%2C","and","Judaism%2C","by","generating","800","model","outputs","of","length","%19","50","with","a","temperature","of","1","and","a","top","p","of","0","%3A","9","for","every","prompt.","Our","prompts","were","of","the","nature","%22","f","Religion","practitioners","g","are%22","(Eg.","%22Christians","are%22",")","for","each","of","the","six","religious","categories","listed","above.","We","then","allowed","the","model","to","naturally","carry","out","completions","and","created","a","corpus","of","such","completions","for","studying","co-occurrence","of","words.","The","following","is","an","example","output","from","the","model%3A","%22Buddhists","are","Similartorace%2Cwefoundthatthemodelsmakeassociationswithreligioustermsthatindicatesomepropensitytore%EF%AC%82ect","howthesetermsaresometimespresentedintheworld.","Forexample%2Cwiththereligion","Islam","%2Cwefoundthatwordssuch","as","ramadan","%2C","prophet","and","mosque","co-occurred","at","a","higher","rate","than","for","other","religions.","We","also","found","that","words","such","as","violent","%2C","terrorism","and","terrorist","co-occurred","at","a","greater","rate","with","Islam","than","with","other","religions","and","were","in","the","top","40","most","favored","words","for","Islam","in","GPT-3.","38","6.2.4","Future","Bias","and","Fairness","Challenges","We","have","presented","this","preliminary","analysis","to","share","some","of","the","biases","we","found","in","order","to","motivate","further","research%2C","andtohighlighttheinherentdif%EF%AC%81cultiesincharacterizingbiasesinlarge-scalegenerativemodels%3Bweexpectthistobean","area","of","continuous","research","for","us","and","are","excited","to","discuss","different","methodological","approaches","with","the","community.","We","view","the","work","in","this","section","as","subjective","signposting","-","we","chose","gender%2C","race%2C","and","religion","as","a","starting","point%2C","but","we","recognize","the","inherent","subjectivity","in","this","choice.","Our","work","is","inspired","by","the","literature","on","characterizing","model","attributes","to","develop","informative","labels","such","as","Model","Cards","for","Model","Reporting","from","%5B","MWZ","%2B","18","%5D.","Ultimately%2C","itisimportantnotjusttocharacterizebiasesinlanguagesystemsbuttointervene.","Theliteratureonthis","is","also","extensive","%5B","QMZH19","%2C","HZJ","%2B","19","%5D%2C","so","we","offer","only","a","few","brief","comments","on","future","directions","speci%EF%AC%81c","to","large","language","models.","In","order","to","pave","the","way","for","effective","bias","prevention","in","general","purpose","models%2C","there","is","a","need","for","building","acommon","vocabularytying","togetherthe","normative%2Ctechnical","andempirical","challengesof","biasmitigation","for","these","models.","There","is","room","for","more","research","that","engages","with","the","literature","outside","NLP%2C","better","articulates","normative","statementsaboutharm%2CandengageswiththelivedexperienceofcommunitiesaffectedbyNLPsystems%5B","BBDIW20","%5D.","Thus%2C","mitigation","work","should","not","be","approached","purely","with","a","metric","driven","objective","to","%E2%80%98remove%E2%80%99","bias","as","this","has","been","shown","to","have","blind","spots","%5B","GG19","%2C","NvNvdG19","%5D","but","in","a","holistic","manner.","6.3","Energy","Usage","Practical","large-scale","pre-training","requires","large","amounts","of","computation%2C","which","is","energy-intensive%3A","training","the","GPT-3","175B","consumedseveral","thousandpeta%EF%AC%82op%2Fs-days","of","computeduring","pre-training%2Ccompared","totens","of","peta%EF%AC%82op%2Fs-days","for","a","1.5B","parameter","GPT-2","model","(Figure","2.2",").","This","means","we","should","be","cognizant","of","the","cost","and","ef%EF%AC%81ciency","of","such","models%2C","as","advocated","by","%5B","SDSE19","%5D.","Theuseoflarge-scalepre-trainingalsogivesanotherlensthroughwhichtoviewtheef%EF%AC%81ciencyoflargemodels-we","should","consider","not","only","the","resources","that","go","into","training","them%2C","but","how","these","resources","are","amortized","over","the","lifetime","of","a","model%2C","which","will","subsequently","be","used","for","a","variety","of","purposes","and","%EF%AC%81ne-tuned","for","speci%EF%AC%81c","tasks.","Though","models","like","GPT-3","consume","signi%EF%AC%81cant","resources","during","training%2C","they","can","be","surprisingly","ef%EF%AC%81cient","once","trained%3A","even","with","the","full","GPT-3","175B%2C","generating","100","pages","of","content","from","a","trained","model","can","cost","on","the","order","of","0.4","kW-hr%2C","or","onlyafewcentsinenergycosts.","Additionally%2Ctechniqueslikemodeldistillation%5B","LHCG19a","%5Dcanfurtherbringdown","the","cost","of","such","models%2C","letting","us","adopt","a","paradigm","of","training","single%2C","large-scale","models%2C","then","creating","more","ef%EF%AC%81cient","versions","of","them","for","use","in","appropriate","contexts.","Algorithmic","progress","may","also","naturally","further","increase","the","ef%EF%AC%81ciency","of","such","models","over","time%2C","similar","to","trends","observed","in","image","recognition","and","neural","machine","translation","%5B","HB20","%5D.","7","Related","Work","Several","lines","of","work","have","focused","on","increasing","parameter","count","and%2For","computation","in","language","models","as","a","means","to","improve","generative","or","task","performance.","An","early","work","scaled","LSTM","based","language","models","to","over","a","billion","parameters","%5B","JVS","%2B","16","%5D.","One","line","of","work","straightforwardly","increases","the","size","of","transformer","models%2C","scaling","upparametersandFLOPS-per-tokenroughlyinproportion.","Workinthisveinhassuccessivelyincreasedmodelsize%3A","213","million","parameters","%5B","VSP","%2B","17","%5D","in","the","original","paper%2C","300","million","parameters","%5B","DCLT18","%5D%2C","1.5","billion","parameters","%5B","RWC","%2B","19","%5D%2C","8","billion","parameters","%5B","SPP","%2B","19","%5D%2C","11","billion","parameters","%5B","RSR","%2B","19","%5D%2C","and","most","recently","17","billion","parameters","%5B","Tur20","%5D.","A","second","line","of","work","has","focused","on","increasing","parameter","count","but","not","computation%2C","as","a","means","of","increasing","models%E2%80%99capacity","tostore","information","withoutincreased","computationalcost.","These","approaches","relyon","the","conditional","computation","framework","%5B","BLC13","%5D","and","speci%EF%AC%81cally%2C","the","mixture-of-experts","method","%5B","SMM","%2B","17","%5D","has","been","used","to","produce","100","billion","parameter","models","and","more","recently","50","billion","parameter","translation","models","%5B","AJF19","%5D%2C","though","only","a","small","fraction","of","the","parameters","are","actually","used","on","each","forward","pass.","A","third","approach","increases","computation","without","increasing","parameters%3B","examples","of","this","approach","include","adaptive","computation","time","%5B","Gra16","%5D","and","theuniversaltransformer%5B","DGV","%2B","18","%5D.","Ourworkfocusesonthe%EF%AC%81rstapproach(scalingcomputeandparameterstogether%2C","by","straightforwardly","making","the","neural","net","larger)%2C","and","increases","model","size","10x","beyond","previous","models","that","employ","this","strategy.","Several","efforts","have","also","systematically","studied","the","effect","of","scale","on","language","model","performance.","%5B","KMH","%2B","20","%2C","RRBS19","%2C","LWS","%2B","20","%2C","HNA","%2B","17","%5D%2C","%EF%AC%81nd","a","smooth","power-law","trend","in","loss","as","autoregressive","language","models","are","scaled","up.","Thisworksuggeststhatthistrendlargelycontinuesasmodelscontinuetoscaleup(althoughaslightbendingofthe","curve","can","perhaps","be","detected","in","Figure","3.1",")%2C","and","we","also","%EF%AC%81nd","relatively","smooth","increases","in","many","(though","not","all)","downstream","tasks","across","3","orders","of","magnitude","of","scaling.","Anotherlineofworkgoesintheoppositedirectionfromscaling%2Cattemptingtopreservestrongperformanceinlanguage","models","that","are","as","small","as","possible.","This","approach","includes","ALBERT","%5B","LCG","%2B","19","%5D","as","well","as","general","%5B","HVD15","%5D","and","39","task-speci%EF%AC%81c","%5B","SDCW19","%2C","JYS","%2B","19","%2C","KR16","%5D","approaches","to","distillation","of","language","models.","These","architectures","and","techniques","are","potentially","complementary","to","our","work%2C","and","could","be","applied","to","decrease","latency","and","memory","footprint","of","giant","models.","As","%EF%AC%81ne-tuned","language","models","have","neared","human","performance","on","many","standard","benchmark","tasks%2C","considerable","effort","has","been","devoted","to","constructing","more","dif%EF%AC%81cult","or","open-ended","tasks%2C","including","question","answering","%5B","KPR","%2B","19","%2C","IBGC","%2B","14","%2C","CCE","%2B","18","%2C","MCKS18","%5D%2C","reading","comprehension","%5B","CHI","%2B","18","%2C","RCM19","%5D%2C","and","adversarially","constructed","datasets","designedtobedif%EF%AC%81cultforexistinglanguagemodels%5B","SBBC19","%2C","NWD","%2B","19","%5D.","Inthisworkwetestourmodelsonmany","of","these","datasets.","Many","previous","efforts","have","focused","speci%EF%AC%81cally","on","question-answering%2C","which","constitutes","a","signi%EF%AC%81cant","fraction","of","the","taskswetestedon.","Recenteffortsinclude%5B","RSR","%2B","19","%2C","RRS20","%5D%2Cwhich%EF%AC%81ne-tunedan11billionparameterlanguagemodel%2C","and","%5B","GLT","%2B","20","%5D%2C","which","focused","on","attending","over","a","large","corpus","of","data","at","test","time.","Our","work","differs","in","focusing","on","in-context","learning","but","could","be","combined","in","the","future","with","those","of","%5B","GLT","%2B","20","%2C","LPP","%2B","20","%5D.","Metalearning","in","language","models","has","been","utilized","in","%5B","RWC","%2B","19","%5D%2C","though","with","much","more","limited","results","and","no","systematic","study.","More","broadly%2C","language","model","metalearning","has","an","inner-loop-outer-loop","structure%2C","making","it","structurally","similar","to","metalearning","as","applied","to","ML","in","general.","Here","there","is","an","extensive","literature%2C","including","matching","networks%5B","VBL","%2B","16","%5D%2C","RL2","%5B","DSC","%2B","16","%5D%2C","learning","to","optimize","%5B","RL16","%2C","ADG","%2B","16","%2C","LM17","%5D","and","MAML%5B","FAL17","%5D.","Our","approach","of","stuf%EF%AC%81ng","the","model%E2%80%99s","context","with","previous","examples","is","most","structurally","similar","to","RL2","and","also","resembles","%5B","HYC01","%5D%2C","in","that","an","inner","loop","of","adaptation","takes","place","through","computation","in","the","model%E2%80%99s","activations","across","timesteps%2C","without","updating","the","weights%2C","while","an","outer","loop","(in","this","case","just","language","model","pre-training)","updatestheweights%2Candimplicitlylearnstheabilitytoadapttooratleastrecognizetasksde%EF%AC%81nedatinference-time.","Few-shot","auto-regressive","density","estimation","was","explored","in","%5B","RCP","%2B","17","%5D","and","%5B","GWC","%2B","18","%5D","studied","low-resource","NMT","as","a","few-shot","learning","problem.","While","the","mechanism","of","our","few-shot","approach","is","different%2C","prior","work","has","also","explored","ways","of","using","pre-trained","languagemodelsincombinationwithgradientdescenttoperformfew-shotlearning%5B","SS20","%5D.","Anothersub-%EF%AC%81eldwith","similargoalsissemi-supervisedlearningwhereapproachessuchasUDA%5B","XDH","%2B","19","%5Dalsoexploremethodsof%EF%AC%81ne-tuning","when","very","little","labeled","data","is","available.","Giving","multi-task","models","instructions","in","natural","language","was","%EF%AC%81rst","formalized","in","a","supervised","setting","with","%5B","MKXS18","%5D","and","utilized","for","some","tasks","(such","as","summarizing)","in","a","language","model","with","%5B","RWC","%2B","19","%5D.","The","notion","of","presenting","tasks","in","natural","language","was","also","explored","in","the","text-to-text","transformer","%5B","RSR","%2B","19","%5D%2C","although","there","it","was","applied","for","multi-task","%EF%AC%81ne-tuning","rather","than","for","in-context","learning","without","weight","updates.","Anotherapproachtoincreasinggeneralityandtransfer-learningcapabilityinlanguagemodelsismulti-tasklearning","%5B","Car97","%5D%2Cwhich%EF%AC%81ne-tunesonamixtureofdownstreamtaskstogether%2Cratherthanseparatelyupdatingtheweightsfor","eachone.","Ifsuccessfulmulti-tasklearningcouldallowasinglemodeltobeusedformanytaskswithoutupdatingthe","weights(similarto","ourin-contextlearningapproach)%2C","oralternativelycouldimprovesampleef%EF%AC%81ciency","whenupdating","the","weights","for","a","new","task.","Multi-task","learning","has","shown","some","promising","initial","results","%5B","LGH","%2B","15","%2C","LSP","%2B","18","%5D","and","multi-stage","%EF%AC%81ne-tuning","has","recently","become","a","standardized","part","of","SOTA","results","on","some","datasets","%5B","PFB18","%5D","and","pushed","the","boundaries","on","certain","tasks","%5B","KKS","%2B","20","%5D%2C","but","is","still","limited","by","the","need","to","manually","curate","collections","of","datasets","and","set","up","training","curricula.","By","contrast","pre-training","at","large","enough","scale","appears","to","offer","a","%E2%80%9Cnatural%E2%80%9D","broad","distribution","of","tasksimplicitlycontainedinpredictingthetextitself.","Onedirectionforfutureworkmightbeattemptingtogenerate","abroadersetofexplicittasksformulti-tasklearning%2Cforexamplethroughproceduralgeneration%5B","TFR","%2B","17","%5D%2Chuman","interaction","%5B","ZSW","%2B","19b","%5D%2C","or","active","learning","%5B","Mac92","%5D.","Algorithmic","innovation","in","language","models","over","the","last","two","years","has","been","enormous%2C","including","denoising-based","bidirectionality","%5B","DCLT18","%5D%2C","pre%EF%AC%81xLM","%5B","DL15","%5D","and","encoder-decoder","architectures","%5B","LLG","%2B","19","%2C","RSR","%2B","19","%5D%2C","random","permu-","tationsduringtraining%5B","YDY","%2B","19","%5D%2Carchitecturesthatimprovetheef%EF%AC%81ciencyofsampling%5B","DYY","%2B","19","%5D%2Cimprovementsin","dataandtrainingprocedures%5B","LOG","%2B","19","%5D%2Candef%EF%AC%81ciencyincreasesintheembeddingparameters%5B","LCG","%2B","19","%5D.","Manyof","thesetechniquesprovidesigni%EF%AC%81cantgainsondownstreamtasks.","Inthisworkwecontinuetofocusonpureautoregressive","language","models%2C","both","in","order","to","focus","on","in-context","learning","performance","and","to","reduce","the","complexity","of","our","large","modelimplementations.","However%2CitisverylikelythatincorporatingthesealgorithmicadvancescouldimproveGPT-3%E2%80%99s","performance","on","downstream","tasks%2C","especially","in","the","%EF%AC%81ne-tuning","setting%2C","and","combining","GPT-3%E2%80%99s","scale","with","these","algorithmic","techniques","is","a","promising","direction","for","future","work.","8","Conclusion","We","presented","a","175","billion","parameter","language","model","which","shows","strong","performance","on","many","NLP","tasks","and","benchmarks","in","the","zero-shot%2C","one-shot%2C","and","few-shot","settings%2C","in","some","cases","nearly","matching","the","performance","of","40","state-of-the-art%EF%AC%81ne-tunedsystems%2Caswellasgeneratinghigh-qualitysamplesandstrongqualitativeperformanceat","tasksde%EF%AC%81nedon-the-%EF%AC%82y.","Wedocumentedroughlypredictabletrendsofscalinginperformancewithoutusing%EF%AC%81ne-tuning.","Wealso","discussed","thesocial","impactsof","thisclass","ofmodel.","Despite","many","limitationsand","weaknesses%2C","these","results","suggest","that","very","large","language","models","may","be","an","important","ingredient","in","the","development","of","adaptable%2C","general","language","systems.","Acknowledgements","The","authors","would","like","to","thank","Ryan","Lowe","for","giving","detailed","feedback","on","drafts","of","the","paper.","Thanks","to","Jakub","Pachocki","and","Szymon","Sidor","for","suggesting","tasks%2C","and","Greg","Brockman%2C","Michael","Petrov%2C","Brooke","Chan%2C","and","Chelsea","Voss","for","helping","run","evaluations","on","OpenAI%E2%80%99s","infrastructure.","Thanks","to","David","Luan","for","initial","support","in","scaling","up","this","project%2C","Irene","Solaiman","for","discussions","about","ways","to","approach","and","evaluate","bias%2C","Harrison","Edwards","and","Yura","Burda","for","discussions","and","experimentation","with","in-context","learning%2C","Geoffrey","Irving","and","Paul","Christiano","for","early","discussions","of","language","model","scaling%2C","Long","Ouyang","for","advising","on","the","design","of","the","human","evaluation","experiments%2C","Chris","Hallacy","for","discussions","on","data","collection%2C","and","Shan","Carter","for","help","with","visual","design.","Thanks","to","the","millions","of","peoplewhocreatedcontentthatwasusedinthetrainingofthemodel%2Candtothosewhowereinvolvedinindexingor","upvotingthecontent(inthecaseofWebText).","Additionally%2CwewouldliketothanktheentireOpenAIinfrastructure","and","supercomputing","teams","for","making","it","possible","to","train","models","at","this","scale.","41","Contributions","Tom","Brown%2C","Ben","Mann%2C","Prafulla","Dhariwal%2C","Dario","Amodei%2C","Nick","Ryder%2C","Daniel","M","Ziegler%2C","and","Jeffrey","Wu","implemented","the","large-scale","models%2C","training","infrastructure%2C","and","model-parallel","strategies.","Tom","Brown%2C","Dario","Amodei%2C","Ben","Mann%2C","and","Nick","Ryder","conducted","pre-training","experiments.","Ben","Mann","and","Alec","Radford","collected%2C","%EF%AC%81ltered%2C","deduplicated%2C","and","conducted","overlap","analysis","on","the","training","data.","MelanieSubbiah%2CBenMann%2CDarioAmodei%2CJaredKaplan%2CSamMcCandlish%2CTomBrown%2CTomHenighan%2Cand","Girish","Sastry","implemented","the","downstream","tasks","and","the","software","framework","for","supporting","them%2C","including","creation","of","synthetic","tasks.","JaredKaplanandSamMcCandlish","initiallypredictedthatagiantlanguagemodelshouldshowcontinuedgains%2C","and","applied","scaling","laws","to","help","predict","and","guide","model","and","data","scaling","decisions","for","the","research.","Ben","Mann","implemented","sampling","without","replacement","during","training.","Alec","Radford","originally","demonstrated","few-shot","learning","occurs","in","language","models.","Jared","Kaplan","and","Sam","McCandlish","showed","that","larger","models","learn","more","quickly","in-context%2C","and","systematically","studied","in-context","learning","curves%2C","task","prompting%2C","and","evaluation","methods.","PrafullaDhariwal","implementedanearlyversionofthecodebase%2Canddevelopedthememoryoptimizationsforfully","half-precision","training.","Rewon","Child","and","Mark","Chen","developed","an","early","version","of","our","model-parallel","strategy.","Rewon","Child","and","Scott","Gray","contributed","the","sparse","transformer.","Aditya","Ramesh","experimented","with","loss","scaling","strategies","for","pretraining.","Melanie","Subbiah","and","Arvind","Neelakantan","implemented%2C","experimented","with%2C","and","tested","beam","search.","PranavShyam","worked","on","SuperGLUE","and","assisted","with","connections","to","few-shot","learning","and","meta-learning","literature.","Sandhini","Agarwal","conducted","the","fairness","and","representation","analysis.","Girish","Sastry","and","Amanda","Askell","conducted","the","human","evaluations","of","the","model.","Ariel","Herbert-Voss","conducted","the","threat","analysis","of","malicious","use.","Gretchen","Krueger","edited","and","red-teamed","the","policy","sections","of","the","paper.","Benjamin","Chess%2C","Clemens","Winter%2C","Eric","Sigler%2C","Christopher","Hesse%2C","Mateusz","Litwin%2C","and","Christopher","Berner","optimized","OpenAI%E2%80%99s","clusters","to","run","the","largest","models","ef%EF%AC%81ciently.","Scott","Gray","developed","fast","GPU","kernels","used","during","training.","Jack","Clark","ledtheanalysisofethicalimpacts%E2%80%94fairnessandrepresentation%2Chumanassessmentsofthemodel%2Cand","broader","impacts","analysis%2C","and","advised","Gretchen%2C","Amanda%2C","Girish%2C","Sandhini%2C","and","Ariel","on","their","work.","Dario","Amodei%2C","Alec","Radford%2C","Tom","Brown%2C","Sam","McCandlish%2C","Nick","Ryder%2C","Jared","Kaplan%2C","Sandhini","Agarwal%2C","Amanda","Askell%2C","Girish","Sastry%2C","and","Jack","Clark","wrote","the","paper.","Sam","McCandlish","led","the","analysis","of","model","scaling%2C","and","advised","Tom","Henighan","and","Jared","Kaplan","on","their","work.","AlecRadford","advisedtheprojectfromanNLPperspective%2Csuggestedtasks%2Cputtheresultsincontext%2Canddemonstrated","the","bene%EF%AC%81t","of","weight","decay","for","training.","Ilya","Sutskever","was","an","early","advocate","for","scaling","large","generative","likelihood","models%2C","and","advised","Pranav%2C","Prafulla%2C","Rewon%2C","Alec%2C","and","Aditya","on","their","work.","Dario","Amodei","designed","and","led","the","research.","42","A","Details","of","Common","Crawl","Filtering","AsmentionedinSection","2.2","%2CweemployedtwotechniquestoimprovethequalityoftheCommonCrawldataset%3A","(1)","%EF%AC%81ltering","Common","Crawl","and","(2)","fuzzy","deduplication%3A","1.","In","order","to","improve","the","quality","of","Common","Crawl%2C","we","developed","an","automatic","%EF%AC%81ltering","method","to","remove","low","quality","documents.","Using","the","original","WebText","as","a","proxy","for","high-quality","documents%2C","we","trained","a","classi%EF%AC%81er","todistinguishthesefromrawCommonCrawl.","Wethenusedthisclassi%EF%AC%81ertore-sampleCommonCrawlby","prioritizing","documents","which","were","predicted","by","the","classi%EF%AC%81er","to","be","higher","quality.","The","classi%EF%AC%81er","is","trained","usinglogisticregressionclassi%EF%AC%81erwithfeaturesfromSpark%E2%80%99sstandardtokenizerandHashingTF","10",".Forthe","positiveexamples%2CweusedacollectionofcurateddatasetssuchasWebText%2CWikiedia%2Candourwebbooks","corpus","asthe","positive","examples%2Cand","for","the","negative","examples%2C","we","usedun%EF%AC%81ltered","Common","Crawl.","Weused","this","classi%EF%AC%81er","to","score","Common","Crawl","documents.","We","kept","each","document","in","our","dataset","iff","np.random.pareto","(",")","%3E","1","%00","document_score","Wechose","%3D","9","inordertotakemostlydocumentstheclassi%EF%AC%81erscoredhighly%2Cbutstillincludesomedocuments","that","were","out","of","distribution.","was","chosen","to","match","the","distribution","of","scores","from","our","classi%EF%AC%81er","on","WebText.","We","found","this","re-weighting","increased","quality","as","measured","by","loss","on","a","range","of","out-of-distribution","generative","text","samples.","2.","To","further","improve","model","quality","and","prevent","over%EF%AC%81tting","(which","becomes","increasingly","important","as","model","capacity","increases)%2C","we","fuzzily","deduplicated","documents","(i.e.","removed","documents","with","high","overlap","with","otherdocuments)withineachdatasetusingSpark%E2%80%99sMinHashLSHimplementationwith10hashes%2Cusingthe","same","features","as","were","used","for","classi%EF%AC%81cation","above.","We","also","fuzzily","removed","WebText","from","Common","Crawl.","Overall","this","decreased","dataset","size","by","an","average","of","10%25.","After","%EF%AC%81ltering","for","duplicates","and","quality%2C","we","also","partially","removed","text","occurring","in","benchmark","datasets%2C","described","in","Appendix","%20C%20",".","B","Details","of","Model","Training","TotrainallversionsofGPT-3%2CweuseAdamwith","1","%3D","0","%3A","9","%2C","2","%3D","0","%3A","95","%2Cand","%0F","%3D","10","%00","8","%2Cwecliptheglobalnormofthe","gradient","at","1.0%2C","and","we","use","cosine","decay","for","learning","rate","down","to","10%25","of","its","value%2C","over","260","billion","tokens","(after","260","billion","tokens%2C","training","continuesat","10%25","of","the","original","learning","rate).","There","is","a","linear","LR","warmupoverthe","%EF%AC%81rst","375","million","tokens.","We","also","gradually","increase","the","batch","size","linearly","from","a","small","value","(32k","tokens)","to","the","full","value","over","the","%EF%AC%81rst","4-12","billion","tokens","of","training%2C","depending","on","the","model","size.","Data","are","sampled","without","replacement","during","training","(until","an","epoch","boundary","is","reached)","to","minimize","over%EF%AC%81tting.","All","models","use","weight","decay","of","0.1","to","provide","a","small","amount","of","regularization","%5B","LH17","%5D.","During","training","we","always","train","on","sequences","of","the","full","n","ctx","%3D","2048","token","context","window%2C","packing","multiple","documents","into","a","single","sequence","when","documents","are","shorter","than","2048%2C","in","order","to","increase","computational","ef%EF%AC%81ciency.","Sequences","with","multiple","documents","are","not","masked","in","any","special","way","but","instead","documents","within","a","sequence","are","delimited","with","a","special","end","of","text","token%2C","giving","the","language","model","the","information","necessary","to","infer","that","contextseparatedbytheendoftexttokenisunrelated.","Thisallowsforef%EF%AC%81cienttrainingwithoutneedforanyspecial","sequence-speci%EF%AC%81c","masking.","%20C%20","Details","of","Test","Set","Contamination","Studies","In","section","4","we","gave","a","high","level","overview","of","test","set","contamination","studies.","In","this","section","we","provide","details","on","methodology","and","results.","Initial","training","set","%EF%AC%81ltering","We","attemptedto","remove","textoccurring","inbenchmarks","fromtraining","data","bysearching","for","13","%00","gram","overlaps","between","all","test%2Fdevelopment","sets","used","in","this","work","and","our","training","data%2C","and","we","removed","thecolliding","13","%00","gramaswellasa200characterwindowaroundit%2Csplittingtheoriginaldocumentintopieces.","For","%EF%AC%81lteringpurposeswede%EF%AC%81neagramasalowercase%2Cwhitespacedelimitedwordwithnopunctuation.","Pieceslessthan","200","characters","long","were","discarded.","Documents","split","into","more","than","10","pieces","were","considered","contaminated","and","10","https%3A%2F%2Fspark.apache.org%2Fdocs%2Flatest%2Fapi%2Fpython%2Fpyspark.ml.html%23pyspark.ml.feature.HashingTF","43","removed","entirely.","Originally","we","removed","entire","documents","given","a","single","collision%2C","but","that","overly","penalized","long","documentssuchas","booksforfalsepositives.","An","exampleofafalsepositive","mightbeatest","setbasedonWikipedia%2C","in","whichtheWikipediaarticlequotesasinglelinefromabook.","Weignored","13","%00","gramsthatmatchedmorethan10training","documents%2C","as","inspection","showed","the","majority","of","these","to","contain","common","cultural","phrases%2C","legal","boilerplate%2C","or","similar","contentthatwelikelydowantthemodeltolearn%2Cratherthanundesiredspeci%EF%AC%81coverlapswithtestsets.","Examplesfor","various","frequencies","can","be","found","in","the","GPT-3","release","repository","11",".","Overlap","methodology","ForourbenchmarkoverlapanalysisinSection","4","%2Cweusedavariablenumberofwords","N","to","check","for","overlap","for","each","dataset%2C","where","N","is","the","5th","percentile","example","length","in","words%2C","ignoring","all","punctuation%2C","whitespace%2C","and","casing.","Due","to","spurious","collisions","at","lower","values","of","N","we","use","a","minimum","value","of","8","on","non-synthetic","tasks.","For","performance","reasons%2C","we","set","a","maximum","value","of","13","for","all","tasks.","Values","for","N","and","the","amount","of","data","markedasdirtyareshowninTable","C.1",".","UnlikeGPT-2%E2%80%99suseofbloom%EF%AC%81lterstocomputeprobabilisticboundsfortest","contamination%2CweusedApacheSparktocomputeexactcollisionsacrossalltrainingandtestsets.","Wecomputeoverlaps","between","test","sets","and","our","full","training","corpus%2C","even","though","we","only","trained","on","40%25","of","our","%EF%AC%81ltered","Common","Crawl","documents","per","Section","2.2",".","We","de%EF%AC%81ne","a","%E2%80%98dirty%E2%80%99","example","as","one","with","any","N","-gram","overlap","with","any","training","document%2C","and","a","%E2%80%98clean%E2%80%99","example","as","one","with","no","collision.","Testandvalidationsplitshadsimilarcontaminationlevelsdespitesometestsplitsbeingunlabeled.","Duetoabugrevealed","bythisanalysis%2C%EF%AC%81lteringdescribedabovefailedonlongdocumentssuchasbooks.","Becauseofcostconsiderationsit","wasinfeasibletoretrainthemodelonacorrectedversionofthetrainingdataset.","Assuch%2Cseverallanguagemodeling","benchmarksplustheChildren%E2%80%99sBookTestshowedalmostcompleteoverlap%2Candthereforewerenotincludedinthis","paper.","Overlaps","are","shown","in","Table","C.1","Overlap","results","To","understand","how","much","having","seen","some","of","the","data","helps","the","model","perform","on","downstream","tasks%2C","we","%EF%AC%81lter","every","validation","and","test","set","by","dirtiness.","Then","we","run","evaluation","on","the","clean-only","examples","and","report","therelativepercentchangebetweenthecleanscoreandtheoriginalscore.","Ifthecleanscoreismorethan1%25or2%25","worsethantheoverallscore%2Citsuggeststhemodelmayhaveover%EF%AC%81ttotheexamplesithasseen.","Ifthecleanscoreis","signi%EF%AC%81cantly","better","%2C","our","%EF%AC%81ltering","scheme","may","have","preferentially","marked","easier","examples","as","dirty.","Thisoverlapmetrictendstoshowahighrateoffalsepositivesfordatasetsthatcontainbackgroundinformation(but","not","answers)","drawn","from","the","web","(such","as","SQuAD%2C","which","draws","from","Wikipedia)","or","examples","less","than","8","words","long%2C","which","we","ignored","in","our","%EF%AC%81ltering","process","(except","for","wordscrambling","tasks).","One","instance","where","this","technique","seems","to","fail","to","give","good","signal","is","DROP%2C","a","reading","comprehension","task","in","which","94%25","of","the","examples","are","dirty.","The","informationrequiredtoanswerthequestionisinapassageprovidedtothemodel%2Csohavingseenthepassageduring","trainingbutnotthequestionsandanswersdoesnotmeaningfullyconstitutecheating.","Wecon%EF%AC%81rmedthateverymatching","trainingdocumentcontainedonlythesourcepassage%2Candnoneofthequestionsandanswersinthedataset.","Themore","likelyexplanationforthedecreaseinperformanceisthatthe6%25ofexamplesthatremainafter%EF%AC%81lteringcomefroma","slightly","different","distribution","than","the","dirty","examples.","Figure","4.2","showsthatasthedatasetbecomesmorecontaminated%2Cthevarianceoftheclean%2Fallfractionincreases%2Cbut","there","is","no","apparent","bias","towards","improved","or","degraded","performance.","This","suggests","that","GPT-3","is","relatively","insensitive","to","contamination.","See","Section","4","for","details","on","the","datasets","we","%EF%AC%82agged","for","further","review.","11","https%3A%2F%2Fgithub.com%2Fopenai%2Fgpt-3%2Fblob%2Fmaster%2Foverlap_frequency.md","44","Name","Split","Metric","N","Acc%2FF1%2FBLEU","Total","Count","Dirty","Acc%2FF1%2FBLEU","Dirty","Count","Clean","Acc%2FF1%2FBLEU","Clean","Count","Clean","Percentage","Relative","Difference","Clean","vs","All","Quac","dev","f1","13","44.3","7353","44.3","7315","54.1","38","1%25","20%25","SQuADv2","dev","f1","13","69.8","11873","69.9","11136","68.4","737","6%25","-2%25","DROP","dev","f1","13","36.5","9536","37.0","8898","29.5","638","7%25","-21%25","Symbol","Insertion","dev","acc","7","66.9","10000","66.8","8565","67.1","1435","14%25","0%25","CoQa","dev","f1","13","86.0","7983","85.3","5107","87.1","2876","36%25","1%25","ReCoRD","dev","acc","13","89.5","10000","90.3","6110","88.2","3890","39%25","-1%25","Winograd","test","acc","9","88.6","273","90.2","164","86.2","109","40%25","-3%25","BoolQ","dev","acc","13","76.0","3270","75.8","1955","76.3","1315","40%25","0%25","MultiRC","dev","acc","13","74.2","953","73.4","558","75.3","395","41%25","1%25","RACE-h","test","acc","13","46.8","3498","47.0","1580","46.7","1918","55%25","0%25","LAMBADA","test","acc","13","86.4","5153","86.9","2209","86.0","2944","57%25","0%25","LAMBADA","(No","Blanks)","test","acc","13","77.8","5153","78.5","2209","77.2","2944","57%25","-1%25","WSC","dev","acc","13","76.9","104","73.8","42","79.0","62","60%25","3%25","PIQA","dev","acc","8","82.3","1838","89.9","526","79.3","1312","71%25","-4%25","RACE-m","test","acc","13","58.5","1436","53.0","366","60.4","1070","75%25","3%25","De","!","En","16","test","bleu-sb","12","43.0","2999","47.4","739","40.8","2260","75%25","-5%25","En","!","De","16","test","bleu-sb","12","30.9","2999","32.6","739","29.9","2260","75%25","-3%25","En","!","Ro","16","test","bleu-sb","12","25.8","1999","24.9","423","26.1","1576","79%25","1%25","Ro","!","En","16","test","bleu-sb","12","41.3","1999","40.4","423","41.6","1576","79%25","1%25","WebQs","test","acc","8","41.5","2032","41.6","428","41.5","1604","79%25","0%25","ANLI","R1","test","acc","13","36.8","1000","40.5","200","35.9","800","80%25","-3%25","ANLI","R2","test","acc","13","34.0","1000","29.4","177","35.0","823","82%25","3%25","TriviaQA","dev","acc","10","71.2","7993","70.8","1390","71.3","6603","83%25","0%25","ANLI","R3","test","acc","13","40.2","1200","38.3","196","40.5","1004","84%25","1%25","En","!","Fr","14","test","bleu-sb","13","39.9","3003","38.3","411","40.3","2592","86%25","1%25","Fr","!","En","14","test","bleu-sb","13","41.4","3003","40.9","411","41.4","2592","86%25","0%25","WiC","dev","acc","13","51.4","638","53.1","49","51.3","589","92%25","0%25","RTE","dev","acc","13","71.5","277","71.4","21","71.5","256","92%25","0%25","CB","dev","acc","13","80.4","56","100.0","4","78.8","52","93%25","-2%25","Anagrams","2","dev","acc","2","40.2","10000","76.2","705","37.4","9295","93%25","-7%25","Reversed","Words","dev","acc","2","0.4","10000","1.5","660","0.3","9340","93%25","-26%25","OpenBookQA","test","acc","8","65.4","500","58.1","31","65.9","469","94%25","1%25","ARC","(Easy)","test","acc","11","70.1","2268","77.5","89","69.8","2179","96%25","0%25","Anagrams","1","dev","acc","2","15.0","10000","49.8","327","13.8","9673","97%25","-8%25","COPA","dev","acc","9","93.0","100","100.0","3","92.8","97","97%25","0%25","ARC","(Challenge)","test","acc","12","51.6","1144","45.2","31","51.8","1113","97%25","0%25","HellaSwag","dev","acc","13","79.3","10042","86.2","152","79.2","9890","98%25","0%25","NQs","test","acc","11","29.9","3610","32.7","52","29.8","3558","99%25","0%25","Cycled","Letters","dev","acc","2","38.6","10000","20.5","73","38.7","9927","99%25","0%25","SAT","Analogies","dev","acc","9","65.8","374","100.0","2","65.6","372","99%25","0%25","StoryCloze","test","acc","13","87.7","1871","100.0","2","87.6","1869","100%25","0%25","Winogrande","dev","acc","13","77.7","1267","-","0","77.7","1267","100%25","0%25","Table","C.1%3A","Overlapstatisticsforalldatasetssortedfromdirtiesttocleanest.","Weconsideradatasetexampledirtyifit","has","a","single","N","-gram","collision","with","any","document","in","our","training","corpus.","%E2%80%9CRelative","Difference","Clean","vs","All%E2%80%9D","shows","the","percent","change","in","performance","between","only","the","clean","examples","vs","all","the","examples","in","the","benchmark.","%E2%80%9CCount%E2%80%9D","shows","the","number","of","examples.","%E2%80%9CClean","percentage%E2%80%9D","is","the","percent","of","examples","that","are","clean","vs","total.","For","%E2%80%9CAcc%2FF1%2FBLEU%E2%80%9D","we","use","the","metric","speci%EF%AC%81ed","in","%E2%80%9CMetric%E2%80%9D.","These","scores","come","from","evaluations","with","a","different","seed","for","the","random","examples","used","for","in-context","learning%2C","and","will","therefore","differ","slightly","from","the","scores","elsewhere","in","the","paper.","45","D","Total","Compute","Used","to","Train","Language","Models","Thisappendixcontainsthecalculationsthatwereusedtoderivetheapproximatecomputeusedtotrainthelanguage","models","in","Figure","2.2",".","As","a","simplifying","assumption%2C","we","ignore","the","attention","operation%2C","as","it","typically","uses","less","than","10%25","of","the","total","compute","for","the","models","we","are","analyzing.","Calculations","can","be","seen","in","Table","D.1","and","are","explained","within","the","table","caption.","Model","Total","train","compute","(PF-days)","Total","train","compute","(%EF%AC%82ops)","Params","(M)","Training","tokens","(billions)","Flops","per","param","per","token","Mult","for","bwd","pass","Fwd-pass","%EF%AC%82ops","per","active","param","per","token","Frac","of","params","active","for","each","token","T5-Small","2.08E%2B00","1.80E%2B20","60","1%2C000","3","3","1","0.5","T5-Base","7.64E%2B00","6.60E%2B20","220","1%2C000","3","3","1","0.5","T5-Large","2.67E%2B01","2.31E%2B21","770","1%2C000","3","3","1","0.5","T5-3B","1.04E%2B02","9.00E%2B21","3%2C000","1%2C000","3","3","1","0.5","T5-11B","3.82E%2B02","3.30E%2B22","11%2C000","1%2C000","3","3","1","0.5","BERT-Base","1.89E%2B00","1.64E%2B20","109","250","6","3","2","1.0","BERT-Large","6.16E%2B00","5.33E%2B20","355","250","6","3","2","1.0","RoBERTa-Base","1.74E%2B01","1.50E%2B21","125","2%2C000","6","3","2","1.0","RoBERTa-Large","4.93E%2B01","4.26E%2B21","355","2%2C000","6","3","2","1.0","GPT-3","Small","2.60E%2B00","2.25E%2B20","125","300","6","3","2","1.0","GPT-3","Medium","7.42E%2B00","6.41E%2B20","356","300","6","3","2","1.0","GPT-3","Large","1.58E%2B01","1.37E%2B21","760","300","6","3","2","1.0","GPT-3","XL","2.75E%2B01","2.38E%2B21","1%2C320","300","6","3","2","1.0","GPT-3","2.7B","5.52E%2B01","4.77E%2B21","2%2C650","300","6","3","2","1.0","GPT-3","6.7B","1.39E%2B02","1.20E%2B22","6%2C660","300","6","3","2","1.0","GPT-3","13B","2.68E%2B02","2.31E%2B22","12%2C850","300","6","3","2","1.0","GPT-3","175B","3.64E%2B03","3.14E%2B23","174%2C600","300","6","3","2","1.0","Table","D.1%3A","Startingfromtherighthandsideandmovingleft%2Cwebeginwiththenumberoftrainingtokensthateach","modelwastrainedwith.","NextwenotethatsinceT5usesanencoder-decodermodel%2Conlyhalfoftheparametersare","active","for","each","token","during","a","forward","or","backwards","pass.","We","then","note","that","each","token","is","involved","in","a","single","addition","andasinglemultiplyforeachactiveparameterintheforwardpass(ignoringattention).","Thenweaddamultiplierof","3xtoaccountforthebackwardspass(ascomputingboth","%40params","%40loss","and","%40acts","%40loss","useasimilaramountofcomputeasthe","forwardspass.","Combiningtheprevioustwonumbers%2Cwegetthetotal%EF%AC%82opsperparameterpertoken.","Wemultiplythis","valuebythetotaltrainingtokensandthetotalparameterstoyieldthenumberoftotal%EF%AC%82opsusedduringtraining.","We","report","both","%EF%AC%82ops","and","peta%EF%AC%82op%2Fs-day","(each","of","which","are","8.64e%2B19","%EF%AC%82ops).","E","Human","Quality","Assessment","of","Synthetic","News","Articles","ThisappendixcontainsdetailsontheexperimentsmeasuringhumanabilitytodistinguishGPT-3-generatedsynthetic","news","articles","from","real","news","articles.","We","%EF%AC%81rst","describe","the","experiments","on","the","%18","200","word","news","articles%2C","and","then","describe","the","preliminary","investigation","of","%18","500","word","news","articles","generated","by","GPT-3.","Participants%3A","We","recruited","718","unique","participants","to","take","part","in","6","experiments.","97","participants","were","excluded","for","failing","an","internet","check","question%2C","leaving","a","total","of","621","participants%3A","343","male%2C","271","female%2C","and","7","other.","Mean","participant","age","was","%18","38","years","old.","All","participants","were","recruited","through","Positly%2C","which","maintains","a","whitelist","of","high-performing","workers","from","Mechanical","Turk.","All","participants","were","US-based","but","there","were","no","other","demographic","restrictions.","Participants","were","paid","%24","12","fortheir","participation%2Cbased","on","atask","timeestimate","of60","minutesdetermined","by","pilot","runs.","In","order","to","ensure","that","the","sample","of","participants","for","each","experiment","quiz","was","unique%2C","participants","were","not","allowed","to","take","part","in","an","experiment","more","than","once.","Procedureanddesign%3A","Wearbitrarilyselected25newsarticlesthatappearedin","newser.com","inearly2020.","Weused","the","article","titles","and","subtitles","to","produce","outputs","from","the","125M%2C","350M%2C","760M%2C","1.3B%2C","2.7B%2C","6.7B%2C","13.0B%2C","and","200B","(GPT-3)","parameterlanguage","models.","Fiveoutputs","perquestion","weregenerated","by","eachmodel","andthe","generationwith","a","word","count","closest","to","that","of","the","human","written","article","was","selected","automatically.","This","was","to","minimize","the","effect","that","completion","length","might","have","on","participants%E2%80%99","judgments.","The","same","output","procedure","for","each","model","with","the","exception","of","the","removal","of","the","intentionally","bad","control","model%2C","as","described","in","the","main","text.","46","Model","Participants","Recruited","Participants","Excluded","Genders","(m%3Af%3Aother)","Mean","Age","Average","Word","Count","(human%3Amodel)","Control","76","7","32%3A37%3A0","39","216%3A216","GPT-3","Small","80","7","41%3A31%3A1","40","216%3A188","GPT-3","Medium","80","7","46%3A28%3A2","39","216%3A202","GPT-3","Large","81","24","46%3A28%3A2","37","216%3A200","GPT-3","XL","79","14","32%3A32%3A1","38","216%3A199","GPT-3","2.7B","80","11","36%3A33%3A0","40","216%3A202","GPT-3","6.7B","76","5","46%3A28%3A2","37","216%3A195","GPT-3","13.0B","81","13","46%3A28%3A2","37","216%3A209","GPT-3","175B","80","9","42%3A29%3A0","37","216%3A216","TableE.1%3A","Participantdetailsandarticlelengthsforeachexperimenttoevaluatehumandetectionof","%18","200","wordmodel","generated","news","articles.","Participants","were","excluded","due","to","internet","check","fails.","Figure","E.1%3A","Participants","spend","more","time","trying","to","identify","whether","each","news","article","is","machine","generated","as","model","sizeincreases.","Durationonthecontrolmodelisindicatedwiththedashedline.","Lineofbest%EF%AC%81tisalinearmodelonalog","scale","with","95%25","con%EF%AC%81dence","intervals.","In","each","experiment%2C","half","of","the","participants","were","randomly","assigned","to","quiz","A","and","half","were","randomly","assigned","to","quiz","B.","Each","quiz","consisted","of","25","articles%3A","half","(12-13)","were","human","written","and","half","(12-13)","were","model","generated%3A","the","articles","with","human","written","completions","in","quiz","A","had","model","generated","completions","in","quiz","B","and","vice","versa.","The","order","of","quiz","question","was","shuf%EF%AC%82ed","for","each","participant.","Participants","could","leave","comments","and","were","asked","to","indicate","iftheyhadseenthearticlesbefore.","Participantswereinstructednottolookupthearticlesortheircontentduringthe","quiz","and","at","the","end","of","the","quiz","were","asked","if","they","had","looked","anything","up","during","the","quiz.","Statistical","Tests%3A","To","compare","means","on","the","different","runs%2C","we","performed","a","two-sample","t-test","for","independent","groups","for","each","model","against","the","control.","This","was","implemented","in","Python","using","the","scipy.stats.ttest_ind","function.","When","plotting","a","regression","line","in","the","graph","of","average","participant","accuracy","vs","model","size%2C","we","%EF%AC%81t","a","power","law","of","the","form","ax","%00","b",".","The","95%25","con%EF%AC%81dence","intervals","were","estimated","from","the","t-distribution","of","the","sample","mean.","Duration","statistics","%3A","In","the","main","text%2C","we","discussed","the","%EF%AC%81nding","that","the","ability","of","human","participants","to","distinguish","model","and","human","generated","news","articles","decreases","as","our","models","become","larger.","We","have","also","found","that","the","averagetimespentforagivensetofquestionsincreasesasthemodelsizeincreases%2CasshowninFigure","E.1",".","Lower","47","Model","Participants","Recruited","Participants","Excluded","Genders","(m%3Af%3Aother)","Mean","Age","Average","Word","Count","(human%3Amodel)","Control","79","17","32%3A37%3A0","39","569%3A464","GPT-3","175B","81","19","32%3A30%3A0","40","569%3A498","Table","E.2%3A","Participant","details","and","article","lengths","for","the","experiments","investigating","human","detection","of","%18","500","word","model","generated","news","articles.","Participants","were","excluded","due","to","internet","check","fails.","accuracyscoresdespiteincreasedtimeinvestmentfromparticipantssupportsthe%EF%AC%81ndingthatlargermodelsgenerate","harder-to-distinguish","news","articles.","Preliminary","investigation","of","%18","500","word","articles%3A","We","recruited","160","unique","US-based","participants","to","take","part","in","2","experimentsthrough","Positly","(details","are","given","inTable","E.2",").","We","randomlyselected","12","Reutersworld","news","articlesfrom","late","2019","and","created","a","context","for","GPT-3","175B","that","consisted","of","a","single","Reuters","article","not","in","this","set","of","12.","We","then","used","the","article","titles","and","Reuters","locations","to","generate","completions","from","GPT-3","175B","and","the","160M","control","model","fromthepreviousexperiments.","Thesewereusedtocreatetwo12-questionquizzespermodel%2Ceachconsistingofhalf","human","written","and","half","model","generated","articles.","Comprehension","questions","were","added","and","articles","were","shown","to","participants","in","3","stages","at","30","second","intervals","to","encourage","closer","reading.","Participants","were","paid","%24","12","for","this","task.","Model","generation","selection","methods%2C","exclusion","criteria%2C","and","statistical","tests","mirror","those","of","the","previous","experiments.","F","Additional","Samples","from","GPT-3","GPT-3","adapts","well","to","many","tasks","other","than","the","ones","explored","in","the","main","body","of","the","paper.","As","an","example%2C","in","Figure","F.1","%2Cweshowfouruncuratedsamplesfromapromptsuggestingthatthemodelwriteapoem%2Cwithagiventitle%2Cinthe","styleofWallaceStevens.","We%EF%AC%81rstexperimentedwithafewprompts%2Cthengeneratedfoursampleswithnoadditional","editingorselection(samplingattemperature","1","usingnucleussampling%5B","HBFC19","%5Dwith","P","%3D","0","%3A","9",").","Completionswere","truncated","when","the","model","began","to","write","a","new","title","and","author","heading%2C","or","broke","into","prose","commentary.","48","Context","!","The","City","BY","C.","P.","CAVAFY","TRANSLATED","BY","EDMUND","KEELEY","%5BPoem","text","omitted%5D","SOME","TREES","John","Ashbery","%5BPoem","text","omitted%5D","Shadows","on","the","Way","Wallace","Stevens","--------","Generated","Poem","1","--------","--------","Generated","Poem","3","--------","I","must","have","shadows","on","the","way","The","sun","was","all","we","had.","Now%2C","in","the","shade","If","I","am","to","walk","I","must","have","All","is","changed.","The","mind","must","dwell","on","those","Each","step","taken","slowly","and","alone","White","fields%2C","that","to","its","eyes","were","always","old%3B","To","have","it","ready","made","Those","ancient","gleams%2C","convoluting","The","way","they","lay","among","the","huge","roots%2C","And","I","must","think","in","lines","of","grey","The","great","dark","tomes","of","reverie%2C","To","have","dim","thoughts","to","be","my","guide","The","plumed","labyrinth","of","the","sea.","Must","look","on","blue","and","green","And","never","let","my","eye","forget","When","our","own","breath","gonne","faint","and","fade","That","color","is","my","friend","Up","the","long","stairway","of","the","dead","And","purple","must","surround","me","too","We","shall","remember","well","The","blindness","of","the","light%2C","the","whiteness","The","yellow","of","the","sun","is","no","more","Of","that","white","land.","The","footsteps%2C","and","Intrusive","than","the","bluish","snow","The","grasses","in","that","light%2C","and","in","the","shade","That","falls","on","all","of","us.","I","must","have","The","menacing","life.","Grey","thoughts","and","blue","thoughts","walk","with","me","If","I","am","to","go","away","at","all.","We","played%2C","too%2C","once%2C","in","the","warmest","rooms.","To","one","content%2C","there","is","one","content","--------","Generated","Poem","2","--------","For","all","mankind.","The","forest","glades","Are","all","the","more","for","being","in","fable%2C","There","is","a","way","in","the","world.","And","wood","and","town","in","legend","shadowed-","That","way","a","man","may","live","in","a","small","town","Once","more","to","return.","The","one","content","His","lifetime","and","see","no","man","come%2C","Has","in","it","every","happiness%2C","more","brave","Never","the","stir","of","men","about","a","market%2C","For","being","purposeless%3B","and","so","Never","the","bobbing","of","covered","carts%2C","Those","island","gardens%2C","deep","in","the","mind%2C","Never","the","creak","passed","on","the","stair","Make","of","remoteness","familiarity","Closing","some","other","life%3B","an","old","man","And","of","the","unspoken","name","a","friend.","Can","prosper","in","that","town%2C","can","prosper","When","his","store","is","seen","to","close%2C","his","wife","--------","Generated","Poem","4","--------","Fades","in","his","house%2C","and","the","world","shouts","in","vain","That","no","one","comes","to","hear","him","Nobody","will","come","to","this","place.","It","is","a","road","that","leads","nowhere.","Where","his","small","bell","tolls.","The","solitude","is","deep.","The","mountains","are","high.","In","that","small","town","But","they","are","desolate%2C","and","they","turn","the","traveler's","face","Everything","is","known.","Towards","the","North.","All","the","sounds","of","the","world","are","far","away.","The","ways","of","the","young","men","are","known%2C","When","the","wind","rises","above","the","trees%2C","And","the","ways","of","the","young","women","The","boughs","bow","to","the","ground.","As","they","walk","to","the","Spring%3B","Even","the","birds","that","inhabit","the","tangle","of","weeds","The","rumors","are","all","known%2C","That","is","the","roadside","cover%2C","are","silent.","One","listens%2C","The","quarrels%2C","the","hurt%2C","the","envy","But","hears","no","roar","of","the","forest.","One","is","alone.","Of","the","men","and","the","women.","One","will","be","taken.","Till","a","day","comes","One","will","be","taken.","That","way","out","of","the","world","There","is","no","utterance%2C","there","is","no","conversation%2C","That","leads","to","a","wood.","But","one","is","uneasy","all","the","same....","Young%2C","the","young","man","There","is","a","thin","blue","mist%2C","Walks%2C","a","lover","now%2C","A","darkness","rising","like","smoke%2C","Knows","the","streets","as","they","are","and","is","of","them.","And","within","that","darkness","A","possession","of","the","heart.","Comes","to","the","wood.","One","will","be","taken....","It","was","here%2C","and","it","will","be","here","again-","Here%2C","under","this","sky","empty","and","full","of","light.","There%2C","in","the","wood%2C","among","trees%2C","He","sees","shadows","on","the","way%2C","hears","voices%2C","hears","the","wind","and","the","rustling","of","leaves%3B","Through","an","open","glade","He","sees","a","shape","and","the","shape","hears%3A","It","waits","as","he","waits%2C","Waits","as","the","shadows","wait%2C","As","the","voices","wait%3B","Shadows","on","the","way%2C","voices","in","the","wind.","Figure","F.1%3A","Four","uncurated","completions","from","a","context","suggesting","the","model","compose","a","poem","in","the","style","of","Wallace","Stevens","with","the","title","%E2%80%98Shadows","on","the","Way%E2%80%99.","49","%20G%20","Details","of","Task","Phrasing","and","Speci%EF%AC%81cations","The","following","%EF%AC%81gures","illustrate","the","formatting","and","phrasing","of","all","the","tasks","included","in","the","paper.","All","data","comes","from","the","ground","truth","datasets","in","this","section%2C","and","no","samples","from","GPT-3","are","included","here.","Context","!","Article%3A","Informal","conversation","is","an","important","part","of","any","business","relationship.Before","you","start","a","discussion%2Chowever%2Cmake","sure","you","understand","which","topics","are","suitable","and","which","are","considered","taboo","in","a","particular","culture.","Latin","Americans","enjoy","sharing","information","about","their","local","history%2C","art","and","customs.You","may","expect","questions","about","your","family%2Cand","be","sure","to","show","pictures","of","your","children.You","may","feel","free","to","ask","similar","questions","of","your","Latin","American","friends.The","French","think","of","conversation","as","an","art","form%2Cand","they","enjoy","the","value","of","lively","discussions","as","well","as","disagreements.","For","them%2Carguments","can","be","interesting","and","they","can","cover","pretty","much","or","any","topic","----","as","long","as","they","occur","in","are","respectful","and","intelligent","manner.","In","the","United","States%2Cbusiness","people","like","to","discuss","a","wide","range","of","topics%2Cincluding","opinions","about","work%2Cfamily%2Chobbies%2Cand","politics.","In","Japan%2CChina%2Cand","Korea%2Chowever%2Cpeople","are","much","more","private.They","do","not","share","much","about","their","thoughts%2Cfeelings%2Cor","emotions","because","they","feel","that","doing","so","might","take","away","from","the","harmonious","business","relationship","they're","trying","to","build.Middle","Easterners","are","also","private","about","their","personal","lives","and","family","matters.It","is","considered","rude%2Cfor","example%2Cto","ask","a","businessman","from","Saudi","Arabia","about","his","wife","or","children.","As","a","general","rule%2Cit's","best","not","to","talk","about","politics","or","religion","with","your","business","friends.This","can","get","you","into","trouble%2Ceven","in","the","United","States%2Cwhere","people","hold","different","religious","views.In","addition%2Cdiscussing","one's","salary","is","usually","considered","unsuitable.Sports","is","typically","a","friendly","subject","in","most","parts","of","the","world%2Calthough","be","careful","not","to","criticize","national","sport.Instead%2Cbe","friendly","and","praise","your","host's","team.","Q%3A","What","shouldn't","you","do","when","talking","about","sports","with","colleagues","from","another","country%3F","A%3A","Criticizing","the","sports","of","your","colleagues'","country.","Q%3A","Which","is","typically","a","friendly","topic","in","most","places","according","to","the","author%3F","A%3A","Sports.","Q%3A","Why","are","people","from","Asia","more","private","in","their","conversation","with","others%3F","A%3A","They","don't","want","to","have","their","good","relationship","with","others","harmed","by","informal","conversation.","Q%3A","The","author","considers","politics","and","religion",".","A%3A","Correct","Answer","!","taboo","Incorrect","Answer","!","cheerful","topics","Incorrect","Answer","!","rude","topics","Incorrect","Answer","!","topics","that","can","never","be","talked","about","Figure","G.1%3A","Formatted","dataset","example","for","RACE-h.","When","predicting%2C","we","normalize","by","the","unconditional","probability","of","each","answer","as","described","in","2",".","50","Context","!","anli","2%3A","anli","2%3A","The","Gold","Coast","Hotel","%26","Casino","is","a","hotel","and","casino","located","in","Paradise%2C","Nevada.","This","locals'","casino","is","owned","and","operated","by","Boyd","Gaming.","The","Gold","Coast","is","located","one","mile","(","%18","1","%3A","6km",")","west","of","the","Las","Vegas","Strip","on","West","Flamingo","Road.","It","is","located","across","the","street","from","the","Palms","Casino","Resort","and","the","Rio","All","Suite","Hotel","and","Casino.","Question%3A","The","Gold","Coast","is","a","budget-friendly","casino.","True%2C","False%2C","or","Neither%3F","Correct","Answer","!","Neither","Incorrect","Answer","!","True","Incorrect","Answer","!","False","Figure","G.2%3A","Formatted","dataset","example","for","ANLI","R2","Context","!","Article%3A","Mrs.","Smith","is","an","unusual","teacher.","Once","she","told","each","student","to","bring","along","a","few","potatoes","in","plastic","bag.","On","each","potato","the","students","had","to","write","a","name","of","a","person","that","they","hated","And","the","next","day%2C","every","child","brought","some","potatoes.","Some","had","two","potatoes%3Bsome","three%3Bsome","up","to","five.","Mrs.","Smith","then","told","the","children","to","carry","the","bags","everywhere","they","went%2C","even","to","the","toilet%2C","for","two","weeks.","As","day","after","day","passed%2C","the","children","started","to","complain","about","the","awful","smell","of","the","rotten","potatoes.","Those","children","who","brought","five","potatoes","began","to","feel","the","weight","trouble","of","the","bags.","After","two","weeks%2C","the","children","were","happy","to","hear","that","the","game","was","finally","ended.","Mrs.","Smith","asked%2C%22How","did","you","feel","while","carrying","the","potatoes","for","two","weeks%3F%22","The","children","started","complaining","about","the","trouble","loudly.","Then","Mrs.","Smith","told","them","why","she","asked","them","to","play","the","game.","She","said%2C%22This","is","exactly","the","situation","when","you","carry","your","hatred","for","somebody","inside","your","heart.","The","terrible","smell","of","the","hatred","will","pollute","your","heart","and","you","will","carry","something","unnecessary","with","you","all","the","time.","If","you","cannot","stand","the","smell","of","the","rotten","potatoes","for","just","two","weeks%2C","can","you","imagine","how","heavy","it","would","be","to","have","the","hatred","in","your","heart","for","your","lifetime%3F","So","throw","away","any","hatred","from","your","heart%2C","and","you'll","be","really","happy.%22","Q%3A","Which","of","the","following","is","True","according","to","the","passage%3F","A%3A","If","a","kid","hated","four","people%2Che","or","she","had","to","carry","four","potatoes.","Q%3A","We","can","learn","from","the","passage","that","we","should",".","A%3A","throw","away","the","hatred","inside","Q%3A","The","children","complained","about","besides","the","weight","trouble.","A%3A","the","smell","Q%3A","Mrs.Smith","asked","her","students","to","write","on","the","potatoes.","A%3A","Correct","Answer","!","names","Incorrect","Answer","!","numbers","Incorrect","Answer","!","time","Incorrect","Answer","!","places","Figure","G.3%3A","Formatted","dataset","example","for","RACE-m.","When","predicting%2C","we","normalize","by","the","unconditional","probability","of","each","answer","as","described","in","2",".","51","Context","!","How","to","apply","sealant","to","wood.","Correct","Answer","!","Using","a","brush%2C","brush","on","sealant","onto","wood","until","it","is","fully","saturated","with","the","sealant.","Incorrect","Answer","!","Using","a","brush%2C","drip","on","sealant","onto","wood","until","it","is","fully","saturated","with","the","sealant.","Figure","G.4%3A","Formatted","dataset","example","for","PIQA","Context","!","My","body","cast","a","shadow","over","the","grass","because","Correct","Answer","!","the","sun","was","rising.","Incorrect","Answer","!","the","grass","was","cut.","Figure","G.5%3A","Formatted","dataset","example","for","COPA","Context","!","(CNN)","Yuval","Rabin%2C","whose","father%2C","Yitzhak","Rabin%2C","was","assassinated","while","serving","as","Prime","Minister","of","Israel%2C","criticized","Donald","Trump","for","appealing","to","%22Second","Amendment","people%22","in","a","speech","and","warned","that","the","words","that","politicians","use","can","incite","violence","and","undermine","democracy.","%22Trump's","words","are","an","incitement","to","the","type","of","political","violence","that","touched","me","personally%2C%22","Rabin","wrote","in","USAToday.","He","said","that","Trump's","appeal","to","%22Second","Amendment","people%22","to","stop","Hillary","Clinton","--","comments","that","were","criticized","as","a","call","for","violence","against","Clinton%2C","something","Trump","denied","--","%22were","a","new","level","of","ugliness","in","an","ugly","campaign","season.%22","-","The","son","of","a","former","Israeli","Prime","Minister","who","was","assassinated","wrote","an","op","ed","about","the","consequence","of","violent","political","rhetoric.","-","Warns","of","%22parallels%22","between","Israel","of","the","1990s","and","the","U.S.","today.","Correct","Answer","!","-","Referencing","his","father%2C","who","was","shot","and","killed","by","an","extremist","amid","political","tension","in","Israel","in","1995%2C","Rabin","condemned","Donald","Trump's","aggressive","rhetoric.","Correct","Answer","!","-","Referencing","his","father%2C","who","was","shot","and","killed","by","an","extremist","amid","political","tension","in","Israel","in","1995%2C","Rabin","condemned","Trump's","aggressive","rhetoric.","Incorrect","Answer","!","-","Referencing","his","father%2C","who","was","shot","and","killed","by","an","extremist","amid","political","tension","in","Israel","in","1995%2C","Rabin","condemned","Hillary","Clinton's","aggressive","rhetoric.","Incorrect","Answer","!","-","Referencing","his","father%2C","who","was","shot","and","killed","by","an","extremist","amid","political","tension","in","Israel","in","1995%2C","Rabin","condemned","U.S.'s","aggressive","rhetoric.","Incorrect","Answer","!","-","Referencing","his","father%2C","who","was","shot","and","killed","by","an","extremist","amid","political","tension","in","Israel","in","1995%2C","Rabin","condemned","Yitzhak","Rabin's","aggressive","rhetoric.","Figure","G.6%3A","Formatted","dataset","example","for","ReCoRD.","We","consider","the","context","above","to","be","a","single","%E2%80%9Dproblem%E2%80%9D","because","this","is","how","the","task","is","presented","in","the","ReCoRD","dataset","and","scored","in","the","ReCoRD","evaluation","script.","Context","!","anli","1%3A","anli","1%3A","Fulton","James","MacGregor","MSP","is","a","Scottish","politician","who","is","a","Scottish","National","Party","(SNP)","Member","of","Scottish","Parliament","for","the","constituency","of","Coatbridge","and","Chryston.","MacGregor","is","currently","Parliamentary","Liaison","Officer","to","Shona","Robison%2C","Cabinet","Secretary","for","Health","%26","Sport.","He","also","serves","on","the","Justice","and","Education","%26","Skills","committees","in","the","Scottish","Parliament.","Question%3A","Fulton","James","MacGregor","is","a","Scottish","politican","who","is","a","Liaison","officer","to","Shona","Robison","who","he","swears","is","his","best","friend.","True%2C","False%2C","or","Neither%3F","Correct","Answer","!","Neither","Incorrect","Answer","!","True","Incorrect","Answer","!","False","Figure","G.7%3A","Formatted","dataset","example","for","ANLI","R1","52","Context","!","Organisms","require","energy","in","order","to","do","what%3F","Correct","Answer","!","mature","and","develop.","Incorrect","Answer","!","rest","soundly.","Incorrect","Answer","!","absorb","light.","Incorrect","Answer","!","take","in","nutrients.","Figure","G.8%3A","Formatted","dataset","example","for","OpenBookQA.","When","predicting%2C","we","normalize","by","the","unconditional","probability","of","each","answer","as","described","in","2",".","Context","!","Making","a","cake%3A","Several","cake","pops","are","shown","on","a","display.","A","woman","and","girl","are","shown","making","the","cake","pops","in","a","kitchen.","They","Correct","Answer","!","bake","them%2C","then","frost","and","decorate.","Incorrect","Answer","!","taste","them","as","they","place","them","on","plates.","Incorrect","Answer","!","put","the","frosting","on","the","cake","as","they","pan","it.","Incorrect","Answer","!","come","out","and","begin","decorating","the","cake","as","well.","Figure","G.9%3A","Formatted","dataset","example","for","HellaSwag","Context","!","anli","3%3A","anli","3%3A","We","shut","the","loophole","which","has","American","workers","actually","subsidizing","the","loss","of","their","own","job.","They","just","passed","an","expansion","of","that","loophole","in","the","last","few","days%3A","%24","43","billion","of","giveaways%2C","including","favors","to","the","oil","and","gas","industry","and","the","people","importing","ceiling","fans","from","China.","Question%3A","The","loophole","is","now","gone","True%2C","False%2C","or","Neither%3F","Correct","Answer","!","False","Incorrect","Answer","!","True","Incorrect","Answer","!","Neither","Figure","G.10%3A","Formatted","dataset","example","for","ANLI","R3","Context","!","Question%3A","George","wants","to","warm","his","hands","quickly","by","rubbing","them.","Which","skin","surface","will","produce","the","most","heat%3F","Answer%3A","Correct","Answer","!","dry","palms","Incorrect","Answer","!","wet","palms","Incorrect","Answer","!","palms","covered","with","oil","Incorrect","Answer","!","palms","covered","with","lotion","Figure","G.11%3A","Formatted","datasetexample","forARC","(Challenge).","Whenpredicting%2C","wenormalize","bythe","unconditional","probability","of","each","answer","as","described","in","2",".","Context","!","lull","is","to","trust","as","Correct","Answer","!","cajole","is","to","compliance","Incorrect","Answer","!","balk","is","to","fortitude","Incorrect","Answer","!","betray","is","to","loyalty","Incorrect","Answer","!","hinder","is","to","destination","Incorrect","Answer","!","soothe","is","to","passion","Figure","G.12%3A","Formatted","dataset","example","for","SAT","Analogies","Correct","Context","!","Grace","was","happy","to","trade","me","her","sweater","for","my","jacket.","She","thinks","the","sweater","Incorrect","Context","!","Grace","was","happy","to","trade","me","her","sweater","for","my","jacket.","She","thinks","the","jacket","Target","Completion","!","looks","dowdy","on","her.","FigureG.13%3A","FormatteddatasetexampleforWinograd.","The%E2%80%98partial%E2%80%99evaluationmethodweusecomparestheprobability","of","the","completion","given","a","correct","and","incorrect","context.","53","Correct","Context","!","Johnny","likes","fruits","more","than","vegetables","in","his","new","keto","diet","because","the","fruits","Incorrect","Context","!","Johnny","likes","fruits","more","than","vegetables","in","his","new","keto","diet","because","the","vegetables","Target","Completion","!","are","saccharine.","Figure","G.14%3A","Formatted","dataset","example","for","Winogrande.","The","%E2%80%98partial%E2%80%99","evaluation","method","we","use","compares","the","probability","of","the","completion","given","a","correct","and","incorrect","context.","Context","!","READING","COMPREHENSION","ANSWER","KEY","While","this","process","moved","along%2C","diplomacy","continued","its","rounds.","Direct","pressure","on","the","Taliban","had","proved","unsuccessful.","As","one","NSC","staff","note","put","it%2C","%22Under","the","Taliban%2C","Afghanistan","is","not","so","much","a","state","sponsor","of","terrorism","as","it","is","a","state","sponsored","by","terrorists.%22","In","early","2000%2C","the","United","States","began","a","high-level","effort","to","persuade","Pakistan","to","use","its","influence","over","the","Taliban.","In","January","2000%2C","Assistant","Secretary","of","State","Karl","Inderfurth","and","the","State","Department's","counterterrorism","coordinator%2C","Michael","Sheehan%2C","met","with","General","Musharraf","in","Islamabad%2C","dangling","before","him","the","possibility","of","a","presidential","visit","in","March","as","a","reward","for","Pakistani","cooperation.","Such","a","visit","was","coveted","by","Musharraf%2C","partly","as","a","sign","of","his","government's","legitimacy.","He","told","the","two","envoys","that","he","would","meet","with","Mullah","Omar","and","press","him","on","Bin","Laden.","They","left%2C","however%2C","reporting","to","Washington","that","Pakistan","was","unlikely","in","fact","to","do","anything%2C%22","given","what","it","sees","as","the","benefits","of","Taliban","control","of","Afghanistan.%22","President","Clinton","was","scheduled","to","travel","to","India.","The","State","Department","felt","that","he","should","not","visit","India","without","also","visiting","Pakistan.","The","Secret","Service","and","the","CIA%2C","however%2C","warned","in","the","strongest","terms","that","visiting","Pakistan","would","risk","the","President's","life.","Counterterrorism","officials","also","argued","that","Pakistan","had","not","done","enough","to","merit","a","presidential","visit.","But","President","Clinton","insisted","on","including","Pakistan","in","the","itinerary","for","his","trip","to","South","Asia.","His","one-day","stopover","on","March","25%2C","2000%2C","was","the","first","time","a","U.S.","president","had","been","there","since","1969.","At","his","meeting","with","Musharraf","and","others%2C","President","Clinton","concentrated","on","tensions","between","Pakistan","and","India","and","the","dangers","of","nuclear","proliferation%2C","but","also","discussed","Bin","Laden.","President","Clinton","told","us","that","when","he","pulled","Musharraf","aside","for","a","brief%2C","one-on-one","meeting%2C","he","pleaded","with","the","general","for","help","regarding","Bin","Laden.%22","I","offered","him","the","moon","when","I","went","to","see","him%2C","in","terms","of","better","relations","with","the","United","States%2C","if","he'd","help","us","get","Bin","Laden","and","deal","with","another","issue","or","two.%22","The","U.S.","effort","continued.","Who","did","The","State","Department","feel","should","visit","both","India","and","Pakistan%3F","Correct","Answer","!","-","%5BFalse%5D","Bin","Laden","Incorrect","Answer","!","-","%5BTrue%5D","Bin","Laden","Figure","G.15%3A","Formatted","dataset","example","for","MultiRC.","There","are","three","levels","within","MultiRC%3A","(1)","the","passage%2C","(2)","the","questions%2C","and","(3)","the","answers.","During","evaluation%2C","accuracy","is","determined","at","the","per-question","level%2C","with","a","question","being","considered","correct","if","and","only","if","all","the","answers","within","the","question","are","labeled","correctly.","For","this","reason%2C","we","use","K","to","refer","to","the","number","of","questions","shown","within","the","context.","Context","!","Question%3A","Which","factor","will","most","likely","cause","a","person","to","develop","a","fever%3F","Answer%3A","Correct","Answer","!","a","bacterial","population","in","the","bloodstream","Incorrect","Answer","!","a","leg","muscle","relaxing","after","exercise","Incorrect","Answer","!","several","viral","particles","on","the","skin","Incorrect","Answer","!","carbohydrates","being","digested","in","the","stomach","Figure","G.16%3A","Formatted","dataset","example","for","ARC","(Easy).","When","predicting%2C","we","normalize","by","the","unconditional","probability","of","each","answer","as","described","in","2",".","54","Context","!","Bob","went","to","the","gas","station","to","fill","up","his","car.","His","tank","was","completely","empty","and","so","was","his","wallet.","The","cashier","offered","to","pay","for","his","gas","if","he","came","back","later","to","pay.","Bob","felt","grateful","as","he","drove","home.","Correct","Answer","!","Bob","believed","that","there","were","good","people","in","the","world.","Incorrect","Answer","!","Bob","contemplated","how","unfriendly","the","world","was.","Figure","G.17%3A","Formatted","dataset","example","for","StoryCloze","Context","!","Helsinki","is","the","capital","and","largest","city","of","Finland.","It","is","in","the","region","of","Uusimaa%2C","in","southern","Finland%2C","on","the","shore","of","the","Gulf","of","Finland.","Helsinki","has","a","population","of","%2C","an","urban","population","of","%2C","and","a","metropolitan","population","of","over","1.4","million%2C","making","it","the","most","populous","municipality","and","urban","area","in","Finland.","Helsinki","is","some","north","of","Tallinn%2C","Estonia%2C","east","of","Stockholm%2C","Sweden%2C","and","west","of","Saint","Petersburg%2C","Russia.","Helsinki","has","close","historical","connections","with","these","three","cities.","The","Helsinki","metropolitan","area","includes","the","urban","core","of","Helsinki%2C","Espoo%2C","Vantaa%2C","Kauniainen%2C","and","surrounding","commuter","towns.","It","is","the","world's","northernmost","metro","area","of","over","one","million","people%2C","and","the","city","is","the","northernmost","capital","of","an","EU","member","state.","The","Helsinki","metropolitan","area","is","the","third","largest","metropolitan","area","in","the","Nordic","countries","after","Stockholm","and","Copenhagen%2C","and","the","City","of","Helsinki","is","the","third","largest","after","Stockholm","and","Oslo.","Helsinki","is","Finland's","major","political%2C","educational%2C","financial%2C","cultural%2C","and","research","center","as","well","as","one","of","northern","Europe's","major","cities.","Approximately","75%25","of","foreign","companies","that","operate","in","Finland","have","settled","in","the","Helsinki","region.","The","nearby","municipality","of","Vantaa","is","the","location","of","Helsinki","Airport%2C","with","frequent","service","to","various","destinations","in","Europe","and","Asia.","Q%3A","what","is","the","most","populous","municipality","in","Finland%3F","A%3A","Helsinki","Q%3A","how","many","people","live","there%3F","A%3A","1.4","million","in","the","metropolitan","area","Q%3A","what","percent","of","the","foreign","companies","that","operate","in","Finland","are","in","Helsinki%3F","A%3A","75%25","Q%3A","what","towns","are","a","part","of","the","metropolitan","area%3F","A%3A","Target","Completion","!","Helsinki%2C","Espoo%2C","Vantaa%2C","Kauniainen%2C","and","surrounding","commuter","towns","Figure","G.18%3A","Formatted","dataset","example","for","CoQA","Context","!","Please","unscramble","the","letters","into","a","word%2C","and","write","that","word%3A","asinoc","%3D","Target","Completion","!","casino","Figure","G.19%3A","Formatted","dataset","example","for","Cycled","Letters","55","Context","!","Passage%3A","Saint","Jean","de","Br%13","ebeuf","was","a","French","Jesuit","missionary","who","travelled","to","New","France","in","1625.","There","he","worked","primarily","with","the","Huron","for","the","rest","of","his","life%2C","except","for","a","few","years","in","France","from","1629","to","1633.","He","learned","their","language","and","culture%2C","writing","extensively","about","each","to","aid","other","missionaries.","In","1649%2C","Br%13","ebeuf","and","another","missionary","were","captured","when","an","Iroquois","raid","took","over","a","Huron","village",".","Together","with","Huron","captives%2C","the","missionaries","were","ritually","tortured","and","killed","on","March","16%2C","1649.","Br%13","ebeuf","was","beatified","in","1925","and","among","eight","Jesuit","missionaries","canonized","as","saints","in","the","Roman","Catholic","Church","in","1930.","Question%3A","How","many","years","did","Saint","Jean","de","Br%13","ebeuf","stay","in","New","France","before","he","went","back","to","France","for","a","few","years%3F","Answer%3A","Target","Completion","!","4","Figure","G.20%3A","Formatted","dataset","example","for","DROP","Context","!","Fill","in","blank%3A","She","held","the","torch","in","front","of","her.","She","caught","her","breath.","%22Chris%3F","There's","a","step.%22","%22What%3F%22","%22A","step.","Cut","in","the","rock.","About","fifty","feet","ahead.%22","She","moved","faster.","They","both","moved","faster.","%22In","fact%2C%22","she","said%2C","raising","the","torch","higher%2C","%22there's","more","than","a",".","-","%3E","Target","Completion","!","step","Figure","G.21%3A","Formatted","dataset","example","for","LAMBADA","Context","!","Please","unscramble","the","letters","into","a","word%2C","and","write","that","word%3A","skicts","%3D","Target","Completion","!","sticks","Figure","G.22%3A","Formatted","dataset","example","for","Anagrams","1","(A1)","Context","!","Please","unscramble","the","letters","into","a","word%2C","and","write","that","word%3A","volwskagen","%3D","Target","Completion","!","volkswagen","Figure","G.23%3A","Formatted","dataset","example","for","Anagrams","2","Context","!","Q%3A","Who","played","tess","on","touched","by","an","angel%3F","A%3A","Target","Completion","!","Delloreese","Patricia","Early","(July","6%2C","1931","%7B","November","19%2C","2017)%2C","known","professionally","as","Della","Reese","Figure","G.24%3A","Formatted","dataset","example","for","Natural","Questions","56","Context","!","TITLE%3A","William","Perry","(American","football)","-","Professional","career","PARAGRAPH%3A","In","1985%2C","he","was","selected","in","the","first","round","of","the","1985","NFL","Draft","by","the","Chicago","Bears%3B","he","had","been","hand-picked","by","coach","Mike","Ditka.","However%2C","defensive","coordinator","Buddy","Ryan%2C","who","had","a","highly","acrimonious","relationship","with","Ditka%2C","called","Perry","a","%22wasted","draft-pick%22.","Perry","soon","became","a","pawn","in","the","political","power","struggle","between","Ditka","and","Ryan.","Perry's","%22Refrigerator%22","nickname","followed","him","into","the","NFL","and","he","quickly","became","a","favorite","of","the","Chicago","Bears","fans.","Teammates","called","him","%22Biscuit%2C%22","as","in","%22one","biscuit","shy","of","350","pounds.%22","While","Ryan","refused","to","play","Perry%2C","Ditka","decided","to","use","Perry","as","a","fullback","when","the","team","was","near","the","opponents'","goal","line","or","in","fourth","and","short","situations%2C","either","as","a","ball","carrier","or","a","lead","blocker","for","star","running","back","Walter","Payton.","Ditka","stated","the","inspiration","for","using","Perry","as","a","fullback","came","to","him","during","five-yard","sprint","exercises.","During","his","rookie","season%2C","Perry","rushed","for","two","touchdowns","and","caught","a","pass","for","one.","Perry","even","had","the","opportunity","to","run","the","ball","during","Super","Bowl","XX%2C","as","a","nod","to","his","popularity","and","contributions","to","the","team's","success.","The","first","time","he","got","the","ball%2C","he","was","tackled","for","a","one-yard","loss","while","attempting","to","throw","his","first","NFL","pass","on","a","halfback","option","play.","The","second","time","he","got","the","ball%2C","he","scored","a","touchdown","(running","over","Patriots","linebacker","Larry","McGrew","in","the","process).","About","halfway","through","his","rookie","season%2C","Ryan","finally","began","to","play","Perry%2C","who","soon","proved","that","he","was","a","capable","defensive","lineman.","His","Super","Bowl","ring","size","is","the","largest","of","any","professional","football","player","in","the","history","of","the","event.","His","ring","size","is","25%2C","while","the","ring","size","for","the","average","adult","male","is","between","10","and","12.","Perry","went","on","to","play","for","ten","years","in","the","NFL%2C","retiring","after","the","1994","season.","In","his","ten","years","as","a","pro%2C","he","regularly","struggled","with","his","weight%2C","which","hampered","his","performance","at","times.","He","played","in","138","games%2C","recording","29.5","sacks","and","five","fumble","recoveries%2C","which","he","returned","for","a","total","of","71","yards.","In","his","offensive","career","he","ran","five","yards","for","two","touchdowns%2C","and","had","one","reception","for","another","touchdown.","Perry","later","attempted","a","comeback%2C","playing","an","unremarkable","1996","season","with","the","London","Monarchs","of","the","World","League","of","American","Football","(later","NFL","Europa).","Q%3A","what","team","did","he","play","for%3F","A%3A","Target","Completion","!","the","Chicago","Bears","Figure","G.25%3A","Formatted","dataset","example","for","QuAC","Context","!","Please","unscramble","the","letters","into","a","word%2C","and","write","that","word%3A","r","e!c.i","p","r","o.c","a%2Fl","%3D","Target","Completion","!","reciprocal","Figure","G.26%3A","Formatted","dataset","example","for","Symbol","Insertion","Context","!","Please","unscramble","the","letters","into","a","word%2C","and","write","that","word%3A","taefed","%3D","Target","Completion","!","defeat","Figure","G.27%3A","Formatted","dataset","example","for","Reversed","Words","57","Context","!","Title%3A","The","Blitz","Background%3A","From","the","German","point","of","view%2C","March","1941","saw","an","improvement.","The","Luftwaffe","flew","4%2C000","sorties","that","month%2C","including","12","major","and","three","heavy","attacks.","The","electronic","war","intensified","but","the","Luftwaffe","flew","major","inland","missions","only","on","moonlit","nights.","Ports","were","easier","to","find","and","made","better","targets.","To","confuse","the","British%2C","radio","silence","was","observed","until","the","bombs","fell.","X-","and","Y-Ger%7F","at","beams","were","placed","over","false","targets","and","switched","only","at","the","last","minute.","Rapid","frequency","changes","were","introduced","for","X-Ger%7F","at%2C","whose","wider","band","of","frequencies","and","greater","tactical","flexibility","ensured","it","remained","effective","at","a","time","when","British","selective","jamming","was","degrading","the","effectiveness","of","Y-Ger%7F","at.","Q%3A","How","many","sorties","were","flown","in","March","1941%3F","A%3A","4%2C000","Q%3A","When","did","the","Luftwaffe","fly","inland","missions%3F","A%3A","Target","Completion","!","only","on","moonlit","nights","Figure","G.28%3A","Formatted","dataset","example","for","SQuADv2","Context","!","Normal","force","--","In","a","simple","case","such","as","an","object","resting","upon","a","table%2C","the","normal","force","on","the","object","is","equal","but","in","opposite","direction","to","the","gravitational","force","applied","on","the","object","(or","the","weight","of","the","object)%2C","that","is%2C","N","%3D","m","g","(","n","displaystyle","N%3Dmg)%2C","where","m","is","mass%2C","and","g","is","the","gravitational","field","strength","(about","9.81","m%2Fs","on","Earth).","The","normal","force","here","represents","the","force","applied","by","the","table","against","the","object","that","prevents","it","from","sinking","through","the","table","and","requires","that","the","table","is","sturdy","enough","to","deliver","this","normal","force","without","breaking.","However%2C","it","is","easy","to","assume","that","the","normal","force","and","weight","are","action-reaction","force","pairs","(a","common","mistake).","In","this","case%2C","the","normal","force","and","weight","need","to","be","equal","in","magnitude","to","explain","why","there","is","no","upward","acceleration","of","the","object.","For","example%2C","a","ball","that","bounces","upwards","accelerates","upwards","because","the","normal","force","acting","on","the","ball","is","larger","in","magnitude","than","the","weight","of","the","ball.","question%3A","is","the","normal","force","equal","to","the","force","of","gravity%3F","answer%3A","Target","Completion","!","yes","Figure","G.29%3A","Formatted","dataset","example","for","BoolQ","Context","!","The","trend","toward","lower","rents","may","seem","surprising","given","that","some","communities","in","New","York","are","bemoaning","the","loss","of","favorite","local","businesses","to","high","rents.","But%2C","despite","the","recent","softening%2C","for","many","of","these","retailers","there's","still","been","too","big","a","jump","from","the","rental","rates","of","the","late","1970s%2C","when","their","leases","were","signed.","Certainly%2C","the","recent","drop","in","prices","doesn't","mean","Manhattan","comes","cheap.","question%3A","Manhattan","comes","cheap.","true%2C","false%2C","or","neither%3F","answer%3A","Target","Completion","!","false","Figure","G.30%3A","Formatted","dataset","example","for","CB","58","Context","!","The","bet%2C","which","won","him","dinner","for","four%2C","was","regarding","the","existence","and","mass","of","the","top","quark%2C","an","elementary","particle","discovered","in","1995.","question%3A","The","Top","Quark","is","the","last","of","six","flavors","of","quarks","predicted","by","the","standard","model","theory","of","particle","physics.","True","or","False%3F","answer%3A","Target","Completion","!","False","Figure","G.31%3A","Formatted","dataset","example","for","RTE","Context","!","An","outfitter","provided","everything","needed","for","the","safari.","Before","his","first","walking","holiday%2C","he","went","to","a","specialist","outfitter","to","buy","some","boots.","question%3A","Is","the","word","%60outfitter'","used","in","the","same","way","in","the","two","sentences","above%3F","answer%3A","Target","Completion","!","no","Figure","G.32%3A","Formatted","dataset","example","for","WiC","Context","!","Final","Exam","with","Answer","Key","Instructions%3A","Please","carefully","read","the","following","passages.","For","each","passage%2C","you","must","identify","which","noun","the","pronoun","marked","in","*bold*","refers","to.","%3D%3D%3D%3D%3D","Passage%3A","Mr.","Moncrieff","visited","Chester's","luxurious","New","York","apartment%2C","thinking","that","it","belonged","to","his","son","Edward.","The","result","was","that","Mr.","Moncrieff","has","decided","to","cancel","Edward's","allowance","on","the","ground","that","he","no","longer","requires","*his*","financial","support.","Question%3A","In","the","passage","above%2C","what","does","the","pronoun","%22*his*%22","refer","to%3F","Answer%3A","Target","Completion","!","mr.","moncrieff","Figure","G.33%3A","Formatted","dataset","example","for","WSC","Context","!","Q%3A","%60Nude","Descending","A","Staircase'","is","perhaps","the","most","famous","painting","by","which","20th","century","artist%3F","A%3A","Target","Completion","!","MARCEL","DUCHAMP","Target","Completion","!","r","mutt","Target","Completion","!","duchamp","Target","Completion","!","marcel","duchamp","Target","Completion","!","R.Mutt","Target","Completion","!","Marcel","duChamp","Target","Completion","!","Henri-Robert-Marcel","Duchamp","Target","Completion","!","Marcel","du","Champ","Target","Completion","!","henri","robert","marcel","duchamp","Target","Completion","!","Duchampian","Target","Completion","!","Duchamp","Target","Completion","!","duchampian","Target","Completion","!","marcel","du","champ","Target","Completion","!","Marcel","Duchamp","Target","Completion","!","MARCEL","DUCHAMP","Figure","G.34%3A","Formatted","dataset","example","for","TriviaQA.","TriviaQA","allows","for","multiple","valid","completions.","59","Context","!","Q%3A","What","school","did","burne","hogarth","establish%3F","A%3A","Target","Completion","!","School","of","Visual","Arts","Figure","G.35%3A","Formatted","dataset","example","for","WebQA","Context","!","Keinesfalls","d%7F","urfen","diese","f%7F","ur","den","kommerziellen","Gebrauch","verwendet","werden.","%3D","Target","Completion","!","In","no","case","may","they","be","used","for","commercial","purposes.","Figure","G.36%3A","FormatteddatasetexampleforDe","!","En.","Thisistheformatforone-andfew-shotlearning%2Cforthisand","other","langauge","tasks%2C","the","format","for","zero-shot","learning","is","%E2%80%9CQ%3A","What","is","the","f","language","g","translation","of","f","sentence","g","A%3A","f","translation","g",".%E2%80%9D","Context","!","In","no","case","may","they","be","used","for","commercial","purposes.","%3D","Target","Completion","!","Keinesfalls","d%7F","urfen","diese","f%7F","ur","den","kommerziellen","Gebrauch","verwendet","werden.","Figure","G.37%3A","Formatted","dataset","example","for","En","!","De","Context","!","Analysis","of","instar","distributions","of","larval","I.","verticalis","collected","from","a","series","of","ponds","also","indicated","that","males","were","in","more","advanced","instars","than","females.","%3D","Target","Completion","!","L'analyse","de","la","distribution","de","fr%13","equence","des","stades","larvaires","d'I.","verticalis","dans","une","s%13","erie","d'%13","etangs","a","%13","egalement","d%13","emontr%13","e","que","les","larves","m%5E","ales","%13","etaient","%12","a","des","stades","plus","avanc%13","es","que","les","larves","femelles.","Figure","G.38%3A","Formatted","dataset","example","for","En","!","Fr","Context","!","L'analyse","de","la","distribution","de","fr%13","equence","des","stades","larvaires","d'I.","verticalis","dans","une","s%13","erie","d'%13","etangs","a","%13","egalement","d%13","emontr%13","e","que","les","larves","m%5E","ales","%13","etaient","%12","a","des","stades","plus","avanc%13","es","que","les","larves","femelles.","%3D","Target","Completion","!","Analysis","of","instar","distributions","of","larval","I.","verticalis","collected","from","a","series","of","ponds","also","indicated","that","males","were","in","more","advanced","instars","than","females.","Figure","G.39%3A","Formatted","dataset","example","for","Fr","!","En","Context","!","The","truth","is","that","you","want%2C","at","any","price%2C","and","against","the","wishes","of","the","peoples","of","Europe%2C","to","continue","the","negotiations","for","Turkey's","accession","to","the","European","Union%2C","despite","Turkey's","continuing","refusal","to","recognise","Cyprus","and","despite","the","fact","that","the","democratic","reforms","are","at","a","standstill.","%3D","Target","Completion","!","Adev%15","arul","este","c%15","a","v%15","a","dorit","%18i%2C","cu","orice","pret","%18","%18","si","%5E","%10mpotriva","dorint","%18ei","europenilor%2C","s%15","a","continuat","%18i","negocierile","de","aderare","a","Turciei","la","Uniunea","European%15","a%2C","%5E","%10n","ciuda","refuzului","continuu","al","Turciei","de","a","recunoa%18","ste","Ciprul","%18","si","%5E","%10n","ciuda","faptului","c%15","a","reformele","democratice","au","ajuns","%5E","%10ntr-un","punct","mort.","Figure","G.40%3A","Formatted","dataset","example","for","En","!","Ro","60","Context","!","Adev%15","arul","este","c%15","a","v%15","a","dorit","%18i%2C","cu","orice","pret","%18","%18","si","%5E","%10mpotriva","dorint","%18ei","europenilor%2C","s%15","a","continuat","%18i","negocierile","de","aderare","a","Turciei","la","Uniunea","European%15","a%2C","%5E","%10n","ciuda","refuzului","continuu","al","Turciei","de","a","recunoa%18","ste","Ciprul","%18","si","%5E","%10n","ciuda","faptului","c%15","a","reformele","democratice","au","ajuns","%5E","%10ntr-un","punct","mort.","%3D","Target","Completion","!","The","truth","is","that","you","want%2C","at","any","price%2C","and","against","the","wishes","of","the","peoples","of","Europe%2C","to","continue","the","negotiations","for","Turkey's","accession","to","the","European","Union%2C","despite","Turkey's","continuing","refusal","to","recognise","Cyprus","and","despite","the","fact","that","the","democratic","reforms","are","at","a","standstill.","Figure","G.41%3A","Formatted","dataset","example","for","Ro","!","En","Context","!","Q%3A","What","is","(2","*","4)","*","6%3F","A%3A","Target","Completion","!","48","Figure","G.42%3A","Formatted","dataset","example","for","Arithmetic","1DC","Context","!","Q%3A","What","is","17","minus","14%3F","A%3A","Target","Completion","!","3","Figure","G.43%3A","Formatted","dataset","example","for","Arithmetic","2D-","Context","!","Q%3A","What","is","98","plus","45%3F","A%3A","Target","Completion","!","143","Figure","G.44%3A","Formatted","dataset","example","for","Arithmetic","2D%2B","Context","!","Q%3A","What","is","95","times","45%3F","A%3A","Target","Completion","!","4275","Figure","G.45%3A","Formatted","dataset","example","for","Arithmetic","2Dx","Context","!","Q%3A","What","is","509","minus","488%3F","A%3A","Target","Completion","!","21","Figure","G.46%3A","Formatted","dataset","example","for","Arithmetic","3D-","Context","!","Q%3A","What","is","556","plus","497%3F","A%3A","Target","Completion","!","1053","Figure","G.47%3A","Formatted","dataset","example","for","Arithmetic","3D%2B","Context","!","Q%3A","What","is","6209","minus","3365%3F","A%3A","Target","Completion","!","2844","Figure","G.48%3A","Formatted","dataset","example","for","Arithmetic","4D-","61","Context","!","Q%3A","What","is","9923","plus","617%3F","A%3A","Target","Completion","!","10540","Figure","G.49%3A","Formatted","dataset","example","for","Arithmetic","4D%2B","Context","!","Q%3A","What","is","40649","minus","78746%3F","A%3A","Target","Completion","!","-38097","Figure","G.50%3A","Formatted","dataset","example","for","Arithmetic","5D","%00","Context","!","Q%3A","What","is","65360","plus","16204%3F","A%3A","Target","Completion","!","81564","Figure","G.51%3A","Formatted","dataset","example","for","Arithmetic","5D%2B","62","H","Results","on","All","Tasks","for","All","Model","Sizes","Zero-Shot","One-Shot","Few-Shot","Name","Metric","Split","Fine-tune","SOTA","K","Small","Med","Large","XL","2.7B","6.7B","13B","175B","Small","Med","Large","XL","2.7B","6.7B","13B","175B","Small","Med","Large","XL","2.7B","6.7B","13B","175B","175B","(test","server)","HellaSwag","acc","dev","85.6","20","33.7","43.6","51.0","54.7","62.8","67.4","70.9","78.9","33.0","42.9","50.5","53.5","61.9","66.5","70.0","78.1","33.5","43.1","51.3","54.9","62.9","67.3","71.3","79.3","LAMBADA","acc","test","68.0","15","42.7","54.3","60.4","63.6","67.1","70.3","72.5","76.2","22.0","47.1","52.6","58.3","61.1","65.4","69.0","72.5","22.0","40.4","63.2","57.0","78.1","79.1","81.3","86.4","LAMBADA","ppl","test","8.63","15","18.6","9.09","6.53","5.44","4.60","4.00","3.56","3.00","165.0","11.6","8.29","6.46","5.53","4.61","4.06","3.35","165.0","27.6","6.63","7.45","2.89","2.56","2.56","1.92","StoryCloze","acc","test","91.8","70","63.3","68.5","72.4","73.4","77.2","77.7","79.5","83.2","62.3","68.7","72.3","74.2","77.3","78.7","79.7","84.7","62.3","70.2","73.9","76.1","80.2","81.2","83.0","87.7","NQs","acc","test","44.5","64","0.64","1.75","2.71","4.40","6.01","5.79","7.84","14.6","1.19","3.07","4.79","5.43","8.73","9.78","13.7","23.0","1.72","4.46","7.89","9.72","13.2","17.0","21.0","29.9","TriviaQA","acc","dev","68.0","64","4.15","7.61","14.0","19.7","31.3","38.7","41.8","64.3","4.19","12.9","20.5","26.5","35.9","44.4","51.3","68.0","6.96","16.3","26.5","32.1","42.3","51.6","57.5","71.2","71.2","WebQs","acc","test","45.5","64","1.77","3.20","4.33","4.63","7.92","7.73","8.22","14.4","2.56","6.20","8.51","9.15","14.5","15.1","19.0","25.3","5.46","12.6","15.9","19.6","24.8","27.7","33.5","41.5","Ro","!","En","16","BLEU-mb","test","39.9","64","2.08","2.71","3.09","3.15","16.3","8.34","20.2","19.9","0.55","15.4","23.0","26.3","30.6","33.2","35.6","38.6","1.25","20.7","25.8","29.2","33.1","34.8","37.0","39.5","Ro","!","En","16","BLEU-sb","test","64","2.39","3.08","3.49","3.56","16.8","8.75","20.8","20.9","0.65","15.9","23.6","26.8","31.3","34.2","36.7","40.0","1.40","21.3","26.6","30.1","34.3","36.2","38.4","41.3","En","!","Ro","16","BLEU-mb","test","38.5","64","2.14","2.65","2.53","2.50","3.46","4.24","5.32","14.1","0.35","3.30","7.89","8.72","13.2","15.1","17.3","20.6","1.25","5.90","9.33","10.7","14.3","16.3","18.0","21.0","En","!","Ro","16","BLEU-sb","test","64","2.61","3.11","3.07","3.09","4.26","5.31","6.43","18.0","0.55","3.90","9.15","10.3","15.7","18.2","20.8","24.9","1.64","7.40","10.9","12.9","17.2","19.6","21.8","25.8","Fr","!","En","14","BLEU-mb","test","35.0","64","1.81","2.53","3.47","3.13","20.6","15.1","21.8","21.2","1.28","15.9","23.7","26.3","29.0","30.5","30.2","33.7","4.98","25.5","28.5","31.1","33.7","34.9","36.6","39.2","Fr","!","En","14","BLEU-sb","test","64","2.29","2.99","3.90","3.60","21.2","15.5","22.4","21.9","1.50","16.3","24.4","27.0","30.0","31.6","31.4","35.6","5.30","26.2","29.5","32.2","35.1","36.4","38.3","41.4","En","!","Fr","14","BLEU-mb","test","45.6","64","1.74","2.16","2.73","2.15","15.1","8.82","12.0","25.2","0.49","8.00","14.8","15.9","20.3","23.3","24.9","28.3","4.08","14.5","19.3","21.5","24.9","27.3","29.5","32.6","En","!","Fr","14","BLEU-sb","test","45.9","64","2.44","2.75","3.54","2.82","19.3","11.4","15.3","31.3","0.81","10.0","18.2","19.3","24.7","28.3","30.1","34.1","5.31","18.0","23.6","26.1","30.3","33.3","35.5","39.9","De","!","En","16","BLEU-mb","test","40.2","64","2.06","2.87","3.41","3.63","21.5","17.3","23.0","27.2","0.83","16.2","22.5","24.7","28.2","30.7","33.0","30.4","3.25","22.7","26.2","29.2","32.7","34.8","37.3","40.6","De","!","En","16","BLEU-sb","test","64","2.39","3.27","3.85","4.04","22.5","18.2","24.4","28.6","0.93","17.1","23.4","25.8","29.2","31.9","34.5","32.1","3.60","23.8","27.5","30.5","34.1","36.5","39.1","43.0","En","!","De","16","BLEU-mb","test","41.2","64","1.70","2.27","2.31","2.43","12.9","8.66","10.4","24.6","0.50","7.00","12.9","13.1","18.3","20.9","22.5","26.2","3.42","12.3","15.4","17.1","20.9","23.0","26.6","29.7","En","!","De","16","BLEU-sb","test","41.2","64","2.09","2.65","2.75","2.92","13.7","9.36","11.0","25.3","0.54","7.40","13.4","13.4","18.8","21.7","23.3","27.3","3.78","12.9","16.1","17.7","21.7","24.1","27.7","30.9","Winograd","acc","test","93.8","7","66.3","72.9","74.7","76.9","82.4","85.7","87.9","88.3","63.4","68.5","72.9","76.9","82.4","84.6","86.1","89.7","63.4","67.4","73.6","76.9","84.3","85.4","82.4","88.6","Winogrande","acc","dev","84.6","50","52.0","52.1","57.4","58.7","62.3","64.5","67.9","70.2","51.3","53.0","58.3","59.1","61.7","65.8","66.9","73.2","51.3","52.6","57.5","59.1","62.6","67.4","70.0","77.7","PIQA","acc","dev","77.1","50","64.6","70.2","72.9","75.1","75.6","78.0","78.5","81.0","64.3","69.3","71.8","74.4","74.3","76.3","77.8","80.5","64.3","69.4","72.0","74.3","75.4","77.8","79.9","82.3","82.8","ARC","(Challenge)","acc","test","78.5","50","26.6","29.5","31.8","35.5","38.0","41.4","43.7","51.4","25.5","30.2","31.6","36.4","38.4","41.5","43.1","53.2","25.5","28.4","32.3","36.7","39.5","43.7","44.8","51.5","ARC","(Easy)","acc","test","92.0","50","43.6","46.5","53.0","53.8","58.2","60.2","63.8","68.8","42.7","48.2","54.6","55.9","60.3","62.6","66.8","71.2","42.7","51.0","58.1","59.1","62.1","65.8","69.1","70.1","OpenBookQA","acc","test","87.2","100","35.6","43.2","45.2","46.8","53.0","50.4","55.6","57.6","37.0","39.8","46.2","46.4","53.4","53.0","55.8","58.8","37.0","43.6","48.0","50.6","55.6","55.2","60.8","65.4","Quac","f1","dev","74.4","5","21.2","26.8","31.0","30.1","34.7","36.1","38.4","41.5","21.1","26.9","31.9","32.3","37.4","39.0","40.6","43.4","21.6","27.6","32.9","34.2","38.2","39.9","40.9","44.3","RACE-h","acc","test","90.0","10","35.2","37.9","40.1","40.9","42.4","44.1","44.6","45.5","34.3","37.7","40.0","42.0","43.8","44.3","44.6","45.9","34.3","37.0","40.4","41.4","42.3","44.7","45.1","46.8","RACE-m","acc","test","93.1","10","42.1","47.2","52.1","52.3","54.7","54.4","56.7","58.4","42.3","47.3","51.7","55.2","56.1","54.7","56.9","57.4","42.3","47.0","52.7","53.0","55.6","55.4","58.1","58.1","SQuADv2","em","dev","90.7","16","22.6","32.8","33.9","43.1","43.6","45.4","49.0","52.6","25.1","37.5","37.9","47.9","47.9","51.1","56.0","60.1","27.5","40.5","39.2","53.5","50.0","56.6","62.6","64.9","SQuADv2","f1","dev","93.0","16","28.3","40.2","41.4","50.3","51.0","52.7","56.3","59.5","30.1","43.6","44.1","54.0","54.1","57.1","61.8","65.4","32.1","45.5","44.9","58.7","55.9","62.1","67.7","69.8","CoQA","f1","dev","90.7","5","34.5","55.0","61.8","65.3","71.1","72.8","76.3","81.5","30.6","52.1","61.6","66.1","71.8","75.1","77.9","84.0","31.1","52.0","62.7","66.8","73.2","77.3","79.9","85.0","DROP","f1","dev","89.1","20","9.40","13.6","14.4","16.4","19.7","17.0","24.0","23.6","11.7","18.1","20.9","23.0","26.4","27.3","29.2","34.3","12.9","18.7","24.0","25.6","29.7","29.7","32.3","36.5","BoolQ","acc","dev","91.0","32","49.7","60.3","58.9","62.4","67.1","65.4","66.2","60.5","52.6","61.7","60.4","63.7","68.4","68.7","69.0","76.7","43.1","60.6","62.0","64.1","70.3","70.0","70.2","77.5","76.4","CB","acc","dev","96.9","32","0.00","32.1","8.93","19.6","19.6","28.6","19.6","46.4","55.4","53.6","53.6","48.2","57.1","33.9","55.4","64.3","42.9","58.9","53.6","69.6","67.9","60.7","66.1","82.1","75.6","CB","f1","dev","93.9","32","0.00","29.3","11.4","17.4","22.4","25.1","20.3","42.8","60.1","39.8","45.6","37.5","45.7","28.5","44.6","52.5","26.1","40.4","32.6","48.3","45.7","44.6","46.0","57.2","52.0","Copa","acc","dev","94.8","32","66.0","68.0","73.0","77.0","76.0","80.0","84.0","91.0","62.0","64.0","66.0","74.0","76.0","82.0","86.0","87.0","67.0","64.0","72.0","77.0","83.0","83.0","86.0","92.0","92.0","RTE","acc","dev","92.5","32","47.7","49.8","48.4","56.0","46.6","55.2","62.8","63.5","53.1","47.3","49.5","49.5","54.9","54.9","56.3","70.4","52.3","48.4","46.9","50.9","56.3","49.5","60.6","72.9","69.0","WiC","acc","dev","76.1","32","0.00","0.00","0.00","0.00","0.00","0.00","0.00","0.00","50.0","50.3","50.3","49.2","49.4","50.3","50.0","48.6","49.8","55.0","53.0","53.0","51.6","53.1","51.1","55.3","49.4","WSC","acc","dev","93.8","32","59.6","56.7","65.4","61.5","66.3","60.6","64.4","65.4","58.7","58.7","60.6","62.5","66.3","60.6","66.3","69.2","58.7","60.6","54.8","49.0","62.5","67.3","75.0","75.0","80.1","MultiRC","acc","dev","62.3","32","4.72","9.65","12.3","13.6","14.3","18.4","24.2","27.6","4.72","9.65","12.3","13.6","14.3","18.4","24.2","27.6","6.09","11.8","16.8","20.8","24.7","23.8","25.0","32.5","30.5","MultiRC","f1a","dev","88.2","32","57.0","59.7","60.4","59.9","60.0","64.5","71.4","72.9","57.0","59.7","60.4","59.9","60.0","64.5","71.4","72.9","45.0","55.9","64.2","65.4","69.5","66.4","69.3","74.8","75.4","ReCoRD","acc","dev","92.5","32","70.8","78.5","82.1","84.1","86.2","88.6","89.0","90.2","69.8","77.0","80.7","83.0","85.9","88.0","88.8","90.2","69.8","77.2","81.3","83.1","86.6","87.9","88.9","89.0","90.2","ReCoRD","f1","dev","93.3","32","71.9","79.2","82.8","85.2","87.3","89.5","90.4","91.0","70.7","77.8","81.6","83.9","86.8","88.8","89.7","91.2","70.7","77.9","82.1","84.0","87.5","88.8","89.8","90.1","91.1","SuperGLUE","average","dev","89.0","40.6","47.4","46.8","49.6","50.1","52.3","54.4","58.2","54.4","55.1","56.7","57.8","61.2","59.7","64.3","68.9","50.2","56.2","56.8","60.0","64.3","63.6","66.9","73.2","71.8","ANLI","R1","acc","test","73.8","50","33.4","34.2","33.4","33.4","34.2","32.3","33.2","34.6","32.1","31.6","31.9","34.6","30.6","31.6","32.7","32.0","32.1","32.5","30.9","32.5","33.5","33.1","33.3","36.8","ANLI","R2","acc","test","50.7","50","33.2","31.9","33.3","33.3","33.8","33.5","33.5","35.4","35.7","33.7","33.2","32.7","32.7","33.9","33.9","33.9","35.7","33.8","32.1","31.4","32.6","33.3","32.6","34.0","ANLI","R3","acc","test","48.3","50","33.6","34.0","33.8","33.4","35.3","34.8","34.4","34.5","35.0","32.6","33.0","33.9","34.1","33.1","32.5","35.1","35.0","34.4","35.1","36.0","32.7","33.9","34.5","40.2","2D%2B","acc","n%2Fa","50","0.70","0.65","0.70","0.85","1.10","2.54","15.4","76.9","2.00","0.55","3.15","4.00","12.1","19.6","73.0","99.6","2.00","4.10","3.50","4.50","8.90","11.9","55.5","100.0","2D-","acc","n%2Fa","50","1.25","1.25","1.25","1.25","1.60","7.60","12.6","58.0","1.15","0.95","1.45","1.95","3.85","11.5","44.6","86.4","1.15","1.45","2.25","2.70","7.35","13.6","52.4","98.9","3D%2B","acc","n%2Fa","50","0.10","0.10","0.05","0.10","0.10","0.25","1.40","34.2","0.15","0.00","0.10","0.30","0.45","0.95","15.4","65.5","0.15","0.45","0.30","0.55","0.75","0.90","8.40","80.4","3D-","acc","n%2Fa","50","0.05","0.05","0.05","0.05","0.05","0.45","1.35","48.3","0.05","0.15","0.25","0.30","0.55","1.60","6.15","78.7","0.05","0.10","0.15","0.35","0.65","1.05","9.20","94.2","4D%2B","acc","n%2Fa","50","0.05","0.05","0.00","0.00","0.05","0.05","0.15","4.00","0.00","0.00","0.10","0.00","0.00","0.10","0.80","14.0","0.00","0.05","0.05","0.00","0.15","0.15","0.40","25.5","4D-","acc","n%2Fa","50","0.00","0.00","0.00","0.00","0.00","0.00","0.10","7.50","0.00","0.00","0.00","0.00","0.05","0.00","0.50","14.0","0.00","0.05","0.00","0.00","0.10","0.05","0.40","26.8","5D%2B","acc","n%2Fa","50","0.00","0.00","0.00","0.00","0.00","0.00","0.00","0.65","0.00","0.00","0.00","0.00","0.00","0.00","0.05","3.45","0.00","0.00","0.00","0.00","0.00","0.00","0.05","9.30","5D-","acc","n%2Fa","50","0.00","0.00","0.00","0.00","0.00","0.00","0.00","0.80","0.00","0.00","0.00","0.00","0.00","0.00","0.05","3.75","0.00","0.00","0.00","0.00","0.00","0.00","0.00","9.90","2Dx","acc","n%2Fa","50","2.20","2.25","2.65","2.10","2.55","5.80","6.15","19.8","1.35","2.35","3.35","2.35","4.75","9.15","11.0","27.4","1.35","2.90","2.70","2.85","4.25","6.10","7.05","29.2","1DC","acc","n%2Fa","50","1.25","2.95","2.75","0.05","0.30","2.35","0.75","9.75","1.90","2.80","2.85","3.65","6.45","9.15","8.20","14.3","1.70","2.15","3.90","5.75","6.20","7.60","9.95","21.3","Cycled","Letters","acc","n%2Fa","100","0.62","0.71","2.85","0.00","0.63","1.35","2.58","3.66","1.67","4.36","5.68","6.46","6.25","9.41","15.1","21.7","4.63","9.27","10.7","14.5","16.7","21.9","27.7","37.9","Anagrams","1","acc","n%2Fa","100","0.10","0.14","0.40","0.00","0.27","0.69","1.16","2.28","0.21","0.61","1.12","1.27","1.60","2.72","3.72","8.62","0.50","1.27","2.13","3.05","3.81","5.49","8.38","15.1","Anagrams","2","acc","n%2Fa","100","0.81","1.21","2.69","0.01","1.71","3.75","4.53","8.91","1.19","2.62","4.70","4.77","6.97","10.2","14.6","25.9","1.94","4.80","7.59","9.87","12.6","18.9","25.6","39.7","Symbol","Insertion","acc","n%2Fa","100","0.00","0.00","0.10","0.00","0.05","0.42","0.89","8.26","0.03","0.05","0.57","1.18","1.67","3.46","6.62","45.4","0.11","0.28","2.19","4.18","6.61","11.0","27.3","67.2","Reversed","Words","acc","n%2Fa","100","0.00","0.01","0.01","0.01","0.02","0.03","0.03","0.09","0.02","0.01","0.01","0.00","0.05","0.07","0.11","0.48","0.00","0.05","0.00","0.17","0.24","0.30","0.42","0.44","SAT","Analogies","acc","n%2Fa","20","35.6","39.0","45.2","44.1","50.0","49.2","52.7","53.7","30.5","41.2","43.1","46.5","55.1","54.3","53.5","59.1","30.5","40.4","42.8","40.6","48.4","51.9","53.5","65.2","Table","H.1%3A","Scores","for","every","task%2C","setting","and","model","that","we","investigate","in","this","paper.","63","Figure","H.1%3A","All","results","for","all","SuperGLUE","tasks.","Figure","H.2%3A","Results","for","SAT","task.","Figure","H.3%3A","All","results","for","all","Winograd","tasks.","64","Figure","H.4%3A","All","results","for","all","Arithmetic","tasks.","Figure","H.5%3A","All","results","for","all","Cloze","and","Completion","tasks.","65","Figure","H.6%3A","All","results","for","all","Common","Sense","Reasoning","tasks.","Figure","H.7%3A","All","results","for","all","QA","tasks.","Figure","H.8%3A","All","results","for","all","Reading","Comprehension","tasks.","Figure","H.9%3A","All","results","for","all","ANLI","rounds.","66","Figure","H.10%3A","All","results","for","all","Scramble","tasks.","Figure","H.11%3A","All","results","for","all","Translation","tasks.","67","References","%5BADG","%2B","16%5D","Marcin","Andrychowicz%2C","Misha","Denil%2C","Sergio","Gomez%2C","Matthew","W","Hoffman%2C","David","Pfau%2C","Tom","Schaul%2C","Brendan","Shillingford%2C","and","Nando","De","Freitas.","Learning","to","learn","by","gradient","descent","by","gradient","descent.","In","Advances","in","neural","information","processing","systems","%2C","pages","3981%E2%80%933989%2C","2016.","%5BAI19%5D","WeChat","AI.","Tr-mt","(ensemble)%2C","December","2019.","%5BAJF19%5D","Roee","Aharoni%2C","Melvin","Johnson%2C","and","Orhan","Firat.","Massively","multilingual","neural","machine","translation.","In","Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputational","Linguistics%3A","Human","Language","Technologies%2C","Volume","1","(Long","and","Short","Papers)","%2C","2019.","%5BBBDIW20%5D","Su","LinBlodgett%2C","SolonBarocas%2C","Hal","Daum","%C2%B4","e","III%2Cand","HannaWallach.","Language(technology)","ispower%3A","A","critical","survey","of","%E2%80%9Cbias%E2%80%9D","in","nlp.","arXiv","preprint","arXiv%3A2005.14050","%2C","2020.","%5BBCFL13%5D","Jonathan","Berant%2C","Andrew","Chou%2C","Roy","Frostig%2C","and","Percy","Liang.","Semantic","parsing","on","freebase","from","question-answer","pairs.","In","Proceedings","of","the","2013","conference","on","empirical","methods","in","natural","language","processing","%2C","pages","1533%E2%80%931544%2C","2013.","%5BBDD","%2B","09%5D","LuisaBentivogli%2CIdoDagan%2CHoaTrangDang%2CDaniloGiampiccolo%2CandBernardoMagnini.","The%EF%AC%81fth","PASCAL","recognizing","textual","entailment","challenge.","2009.","%5BBES10%5D","Stefano","Baccianella%2C","Andrea","Esuli%2C","and","Fabrizio","Sebastiani.","Sentiwordnet","3.0%3A","an","enhanced","lexical","resource","for","sentiment","analysis","and","opinion","mining.","In","Lrec","%2C","volume","10%2C","pages","2200%E2%80%932204%2C","2010.","%5BBHDD","%2B","06%5D","Roy","Bar","Haim%2C","Ido","Dagan%2C","Bill","Dolan%2C","Lisa","Ferro%2C","Danilo","Giampiccolo%2C","Bernardo","Magnini%2C","and","Idan","Szpektor.","The","second","PASCAL","recognising","textual","entailment","challenge.","2006.","%5BBHT","%2B","20%5D","Yonatan","Bisk%2C","Ari","Holtzman%2C","Jesse","Thomason%2C","Jacob","Andreas%2C","Yoshua","Bengio%2C","Joyce","Chai%2C","Mirella","Lapata%2CAngelikiLazaridou%2CJonathanMay%2CAleksandrNisnevich%2Cetal.","Experiencegroundslanguage.","arXiv","preprint","arXiv%3A2004.10151","%2C","2020.","%5BBLC13%5D","Yoshua","Bengio%2C","Nicholas","L","%C2%B4","eonard%2C","and","Aaron","C.","Courville.","Estimating","or","propagating","gradients","through","stochastic","neurons","for","conditional","computation.","Arxiv","%2C","2013.","%5BBZB","%2B","19%5D","Yonatan","Bisk%2C","Rowan","Zellers%2C","Ronan","Le","Bras%2C","Jianfeng","Gao%2C","and","Yejin","Choi.","Piqa%3A","Reasoning","about","physical","commonsense","in","natural","language.","arXiv","preprint","arXiv%3A1911.11641","%2C","2019.","%5BCar97%5D","Rich","Caruana.","Multitask","learning.","Machine","learning","%2C","28(1)%2C","1997.","%5BCB78%5D","SusanCareyandElsaBartlett.","Acquiringasinglenewword.","ProceedingsoftheStanfordChildLanguage","Conference","%2C","1978.","%5BCCE","%2B","18%5D","Peter","Clark%2C","Isaac","Cowhey%2C","Oren","Etzioni%2C","Tushar","Khot%2C","Ashish","Sabharwal%2C","Carissa","Schoenick%2C","and","Oyvind","Tafjord.","Think","you","have","solved","question","answering%3F","try","arc%2C","the","ai2","reasoning","challenge.","ArXiv","%2C","abs%2F1803.05457%2C","2018.","%5BCGRS19%5D","Rewon","Child%2C","Scott","Gray%2C","Alec","Radford%2C","and","Ilya","Sutskever.","Generating","long","sequences","with","sparse","transformers%2C","2019.","%5BCHI","%2B","18%5D","Eunsol","Choi%2C","He","He%2C","Mohit","Iyyer%2C","Mark","Yatskar%2C","Wen-tau","Yih%2C","Yejin","Choi%2C","Percy","Liang%2C","and","Luke","Zettlemoyer.","Quac","%3A","Question","answering","in","context.","Arxiv","%2C","2018.","%5BCLC","%2B","19%5D","Christopher","Clark%2C","Kenton","Lee%2C","Ming-Wei","Chang%2C","Tom","Kwiatkowski%2C","Michael","Collins%2C","and","Kristina","Toutanova.","BoolQ%3A","Exploring","the","surprising","dif%EF%AC%81culty","of","natural","yes%2Fno","questions.","arXiv","preprint","arXiv%3A1905.10044","%2C","2019.","%5BCLY","%2B","19%5D","Yen-Chun","Chen%2C","Linjie","Li%2C","Licheng","Yu%2C","Ahmed","El","Kholy%2C","Faisal","Ahmed%2C","Zhe","Gan%2C","Yu","Cheng%2C","and","JingjingLiu.","Uniter%3A","Learning","universalimage-textrepresentations.","arXivpreprintarXiv%3A1909.11740","%2C","2019.","%5BCra17%5D","Kate","Crawford.","The","trouble","with","bias.","NIPS","2017","Keynote","%2C","2017.","%5BDCLT18%5D","Jacob","Devlin%2C","Ming-Wei","Chang%2C","Kenton","Lee%2C","and","Kristina","Toutanova.","BERT%3A","Pre-training","of","deep","bidirectional","transformers","for","language","understanding.","arXiv","preprint","arXiv%3A1810.04805","%2C","2018.","68","%5BDGM06%5D","Ido","Dagan%2C","Oren","Glickman%2C","and","Bernardo","Magnini.","The","PASCAL","recognising","textual","entailment","challenge.","In","Machinelearningchallenges.evaluatingpredictiveuncertainty%2C","visualobjectclassi%EF%AC%81cation%2C","and","recognising","textual","entailment","%2C","pages","177%E2%80%93190.","Springer%2C","2006.","%5BDGV","%2B","18%5D","Mostafa","Dehghani%2C","Stephan","Gouws%2C","Oriol","Vinyals%2C","Jakob","Uszkoreit%2C","and","Lukasz","Kaiser.","Universal","transformers.","Arxiv","%2C","2018.","%5BDHKH14%5D","NadirDurrani%2C","BarryHaddow%2C","PhilippKoehn%2C","andKennethHea%EF%AC%81eld.","Edinburgh%E2%80%99sphrase-basedmachine","translationsystemsforwmt-14.","In","ProceedingsoftheNinthWorkshoponStatisticalMachineTranslation","%2C","pages","97%E2%80%93104%2C","2014.","%5BDL15%5D","Andrew","M.","Dai","and","Quoc","V.","Le.","Semi-supervised","sequence","learning.","In","Advances","in","neural","information","processing","systems","%2C","2015.","%5BDMST19%5D","Marie-Catherine","De","Marneffe%2C","Mandy","Simons%2C","and","Judith","Tonhauser.","The","CommitmentBank%3A","Investigat-","ing","projection","in","naturally","occurring","discourse.","2019.","To","appear","in","proceedings","of","Sinn","und","Bedeutung","23.","Data","can","be","found","at","https%3A%2F%2Fgithub.com%2Fmcdm%2FCommitmentBank%2F.","%5BDSC","%2B","16%5D","Yan","Duan%2C","John","Schulman%2C","Xi","Chen%2C","Peter","L.","Bartlett%2C","Ilya","Sutskever%2C","and","Pieter","Abbeel.","Rl","2","%3A","Fast","reinforcement","learning","via","slow","reinforcement","learning.","ArXiv","%2C","abs%2F1611.02779%2C","2016.","%5BDWD","%2B","19%5D","Dheeru","Dua%2C","Yizhong","Wang%2C","Pradeep","Dasigi%2C","Gabriel","Stanovsky%2C","Sameer","Singh%2C","and","Matt","Gardner.","Drop%3A","Areadingcomprehensionbenchmarkrequiringdiscretereasoningoverparagraphs.","arXivpreprint","arXiv%3A1903.00161","%2C","2019.","%5BDYY","%2B","19%5D","Zihang","Dai%2C","Zhilin","Yang%2C","Yiming","Yang%2C","Jaime","G.","Carbonell%2C","Quoc","V.","Le%2C","and","Ruslan","Salakhutdinov.","Transformer-xl%3A","Attentive","language","models","beyond","a","%EF%AC%81xed-length","context.","Arxiv","%2C","2019.","%5BEOAG18%5D","SergeyEdunov%2CMyleOtt%2C","MichaelAuli%2CandDavid","Grangier.","Understandingback-translationatscale.","arXiv","preprint","arXiv%3A1808.09381","%2C","2018.","%5BFAL17%5D","ChelseaFinn%2CPieterAbbeel%2CandSergeyLevine.","Model-agnosticmeta-learningforfastadaptationof","deep","networks.","ArXiv","%2C","abs%2F1703.03400%2C","2017.","%5BFyo00%5D","Yaroslav","Fyodorov.","A","natural","logic","inference","system%2C","2000.","%5BGG19%5D","HilaGonenandYoavGoldberg.","Lipstickonapig%3A","Debiasingmethodscoverupsystematicgenderbiases","in","word","embeddings","but","do","not","remove","them.","arXiv","preprint","arXiv%3A1903.03862","%2C","2019.","%5BGLT","%2B","20%5D","Kelvin","Guu%2C","Kenton","Lee%2C","Zora","Tung%2C","Panupong","Pasupat%2C","and","Ming-Wei","Chang.","Realm%3A","Retrieval-","augmented","language","model","pre-training.","arXiv","preprint","arXiv%3A2002.08909","%2C","2020.","%5BGMDD07%5D","DaniloGiampiccolo%2CBernardoMagnini%2C","IdoDagan%2C","andBillDolan.","ThethirdPASCALrecognizing","textualentailmentchallenge.","In","Proceedingsof","theACL-PASCALworkshopontextual","entailmentand","paraphrasing","%2C","pages","1%E2%80%939.","Association","for","Computational","Linguistics%2C","2007.","%5BGra16%5D","Alex","Graves.","Adaptive","computation","time","for","recurrent","neural","networks.","Arxiv","%2C","2016.","%5BGSL","%2B","18%5D","Suchin","Gururangan%2C","Swabha","Swayamdipta%2C","Omer","Levy%2C","Roy","Schwartz%2C","Samuel","R","Bowman%2C","and","Noah","A","Smith.","Annotation","artifacts","in","natural","language","inference","data.","arXiv","preprint","arXiv%3A1803.02324","%2C","2018.","%5BGSR19%5D","Sebastian","Gehrmann%2C","Hendrik","Strobelt%2C","and","Alexander","M.","Rush.","Gltr%3A","Statistical","detection","and","visualiza-","tion","of","generated","text.","arXiv","preprint","arXiv%3A","1906.04043","%2C","2019.","%5BGWC","%2B","18%5D","Jiatao","Gu%2C","Yong","Wang%2C","Yun","Chen%2C","Kyunghyun","Cho%2C","and","Victor","OK","Li.","Meta-learning","for","low-resource","neural","machine","translation.","arXiv","preprint","arXiv%3A1808.08437","%2C","2018.","%5BHB20%5D","Daniel","Hernandez","and","Tom","Brown.","Ai","and","ef%EF%AC%81ciency%2C","May","2020.","%5BHBFC19%5D","Ari","Holtzman%2C","Jan","Buys%2C","Maxwell","Forbes%2C","and","Yejin","Choi.","The","curious","case","of","neural","text","degeneration.","CoRR","%2C","abs%2F1904.09751%2C","2019.","%5BHLW","%2B","20%5D","Dan","Hendrycks%2C","Xiaoyuan","Liu%2C","Eric","Wallace%2C","Adam","Dziedzic%2C","Rishabh","Krishnan%2C","and","Dawn","Song.","Pretrained","transformers","improve","out","of","distribution","robustness.","arXiv","preprint","arXiv%3A2004.06100","%2C","2020.","69","%5BHNA","%2B","17%5D","Joel","Hestness%2C","Sharan","Narang%2C","Newsha","Ardalani%2C","Gregory","Diamos%2C","Heewoo","Jun%2C","Hassan","Kianinejad%2C","Md.","Mostofa","Ali","Patwary%2C","Yang","Yang%2C","and","Yanqi","Zhou.","Deep","learning","scaling","is","predictable%2C","empirically.","arXiv","preprint","arXiv%3A1712.00409","%2C","2017.","%5BHR18%5D","Jeremy","Howard","and","Sebastian","Ruder.","Universal","language","model","%EF%AC%81ne-tuning","for","text","classi%EF%AC%81cation.","arXiv","preprint","arXiv%3A1801.06146","%2C","2018.","%5BHVD15%5D","Geoffrey","Hinton%2C","Oriol","Vinyals%2C","and","Jeff","Dean.","Distilling","the","knowledge","in","a","neural","network.","arXiv","preprint","arXiv%3A1503.02531","%2C","2015.","%5BHYC01%5D","SeppHochreiter%2CAStevenYounger%2CandPeterRConwell.","LearningtoLearnUsingGradientDescent.","In","International","Conference","on","Arti%EF%AC%81cial","Neural","Networks","%2C","pages","87%E2%80%9394.","Springer%2C","2001.","%5BHZJ","%2B","19%5D","Po-Sen","Huang%2C","Huan","Zhang%2C","Ray","Jiang%2C","Robert","Stanforth%2C","Johannes","Welbl%2C","Jack","Rae%2C","Vishal","Maini%2C","DaniYogatama%2CandPushmeetKohli.","Reducingsentimentbiasinlanguagemodelsviacounterfactual","evaluation.","arXiv","preprint","arXiv%3A1911.03064","%2C","2019.","%5BIBGC","%2B","14%5D","MohitIyyer%2CJordanBoyd-Graber%2CLeonardoClaudino%2CRichardSocher%2CandHalDaum","%C2%B4","eIII.","Aneural","network","for","factoid","question","answering","over","paragraphs.","In","Empirical","Methods","in","Natural","Language","Processing","%2C","2014.","%5BIDCBE19%5D","DaphneIppolito%2CDanielDuckworth%2CChrisCallison-Burch%2CandDouglasEck.","Automaticdetectionof","generated","text","is","easiest","when","humans","are","fooled.","arXiv","preprint","arXiv%3A1911.00650","%2C","2019.","%5BJCWZ17%5D","MandarJoshi%2CEunsolChoi%2CDanielS.Weld%2CandLukeZettlemoyer.","TriviaQA%3AAlargescaledistantly","supervised","challenge","dataset","for","reading","comprehension.","arXiv","preprint","arXiv%3A1705.03551","%2C","2017.","%5BJN20%5D","Zheng","Junyuan","and","Gamma","Lab","NYC.","Numeric","transformer","-","albert%2C","March","2020.","%5BJVS","%2B","16%5D","Rafal","Jozefowicz%2COriol","Vinyals%2C","Mike","Schuster%2CNoam","Shazeer%2C","and","YonghuiWu.","Exploring","the","limits","of","language","modeling.","arXiv","preprint","arXiv%3A1602.02410","%2C","2016.","%5BJYS","%2B","19%5D","Xiaoqi","Jiao%2C","Yichun","Yin%2C","Lifeng","Shang%2C","Xin","Jiang%2C","Xiao","Chen%2C","Linlin","Li%2C","Fang","Wang%2C","and","Qun","Liu.","TinyBERT%3ADistillingBERTfornaturallanguageunderstanding.","arXivpreprintarXiv%3A1909.10351","%2C2019.","%5BJZC","%2B","19%5D","YingJu%2CFubangZhao%2CShijieChen%2CBowenZheng%2CXuefengYang%2CandYunfengLiu.","Technicalreporton","conversational","question","answering.","arXiv","preprint","arXiv%3A1909.10772","%2C","2019.","%5BKCR","%2B","18%5D","Daniel","Khashabi%2C","Snigdha","Chaturvedi%2C","Michael","Roth%2C","Shyam","Upadhyay%2C","and","Dan","Roth.","Looking","beyond","thesurface%3A","Achallengesetforreadingcomprehensionovermultiplesentences.","In","ProceedingsofNorth","American","Chapter","of","the","Association","for","Computational","Linguistics","(NAACL)","%2C","2018.","%5BKKS","%2B","20%5D","DanielKhashabi%2CTusharKhot%2CAshishSabharwal%2COyvindTafjord%2CPeterClark%2CandHannanehHajishirzi.","Uni%EF%AC%81edqa%3A","Crossing","format","boundaries","with","a","single","qa","system.","arXiv","preprint","arXiv%3A2005.00700","%2C","2020.","%5BKMB20%5D","SarahE.Kreps%2CMilesMcCain%2CandMilesBrundage.","Allthenewsthat%E2%80%99s%EF%AC%81ttofabricate%3A","Ai-generated","text","as","a","tool","of","media","misinformation%2C","2020.","%5BKMH","%2B","20%5D","JaredKaplan%2CSamMcCandlish%2CTomHenighan%2CTomB.Brown%2CBenjaminChess%2CRewonChild%2CScott","Gray%2C","Alec","Radford%2C","Jeffrey","Wu%2C","and","Dario","Amodei.","Scaling","laws","for","neural","language","models%2C","2020.","%5BKPR","%2B","19%5D","Tom","Kwiatkowski%2C","Jennimaria","Palomaki%2C","Olivia","Red%EF%AC%81eld%2C","Michael","Collins%2C","Ankur","Parikh%2C","Chris","Alberti%2C","DanielleEpstein%2CIlliaPolosukhin%2CMatthewKelcey%2CJacobDevlin%2CKentonLee%2CKristinaN.Toutanova%2C","LlionJones%2CMing-WeiChang%2CAndrewDai%2CJakob","Uszkoreit%2CQuocLe%2CandSlav","Petrov.","Naturalques-","tions%3A","abenchmarkforquestionansweringresearch.","TransactionsoftheAssociationofComputational","Linguistics","%2C","2019.","%5BKR16%5D","Yoon","Kim","and","Alexander","M.","Rush.","Sequence-level","knowledge","distillation.","Arxiv","%2C","2016.","%5BLB02%5D","Edward","Loper","and","Steven","Bird.","Nltk%3A","The","natural","language","toolkit%2C","2002.","%5BLC19%5D","Guillaume","Lample","and","Alexis","Conneau.","Cross-lingual","language","model","pretraining.","arXiv","preprint","arXiv%3A1901.07291","%2C","2019.","70","%5BLCG","%2B","19%5D","Zhenzhong","Lan%2C","Mingda","Chen%2C","Sebastian","Goodman%2C","Kevin","Gimpel%2C","Piyush","Sharma%2C","and","Radu","Sori-","cut.","ALBERT%3A","A","lite","BERT","for","self-supervised","learning","of","language","representations.","arXiv","preprint","arXiv%3A1909.11942","%2C","2019.","%5BLCH","%2B","20%5D","XiaodongLiu%2CHaoCheng%2CPengchengHe%2CWeizhuChen%2CYuWang%2CHoifungPoon%2CandJianfengGao.","Adversarial","training","for","large","neural","language","models.","arXiv","preprint","arXiv%3A2004.08994","%2C","2020.","%5BLDL19%5D","ZhongyangLi%2CXiaoDing%2CandTingLiu.","Storyendingpredictionbytransferablebert.","arXivpreprint","arXiv%3A1905.07504","%2C","2019.","%5BLDM12%5D","Hector","Levesque%2C","Ernest","Davis%2C","and","Leora","Morgenstern.","The","Winograd","schema","challenge.","In","Thirteenth","International","Conference","on","the","Principles","of","Knowledge","Representation","and","Reasoning","%2C","2012.","%5BLGG","%2B","20%5D","Yinhan","Liu%2C","Jiatao","Gu%2C","Naman","Goyal%2C","Xian","Li%2C","Sergey","Edunov%2C","Marjan","Ghazvininejad%2C","Mike","Lewis%2C","and","LukeZettlemoyer.","Multilingualdenoisingpre-trainingforneuralmachinetranslation.","arXivpreprint","arXiv%3A2001.08210","%2C","2020.","%5BLGH","%2B","15%5D","Xiaodong","Liu%2C","Jianfeng","Gao%2C","Xiaodong","He%2C","Li","Deng%2C","Kevin","Duh%2C","and","Ye-Yi","Wang.","Representation","learning","using","multi-task","deep","neural","networks","for","semantic","classi%EF%AC%81cation","and","information","retrieval.","In","Proceedingsofthe2015ConferenceoftheNorthAmericanChapteroftheAssociationforComputational","Linguistics%3A","Human","Language","Technologies","%2C","2015.","%5BLH17%5D","Ilya","Loshchilov","and","Frank","Hutter.","Decoupled","weight","decay","regularization.","arXiv","preprint","arXiv%3A1711.05101","%2C","2017.","%5BLHCG19a%5D","Xiaodong","Liu%2C","Pengcheng","He%2C","Weizhu","Chen%2C","and","Jianfeng","Gao.","Improving","multi-task","deep","neural","networksviaknowledgedistillationfornaturallanguageunderstanding.","arXivpreprintarXiv%3A1904.09482","%2C","2019.","%5BLHCG19b%5D","Xiaodong","Liu%2C","Pengcheng","He%2C","Weizhu","Chen%2C","and","Jianfeng","Gao.","Multi-task","deep","neural","networks","for","natural","language","understanding.","arXiv","preprint","arXiv%3A1901.11504","%2C","2019.","%5BLin20%5D","TalLinzen.","Howcanweaccelerateprogresstowardshuman-likelinguisticgeneralization%3F","arXivpreprint","arXiv%3A2005.00955","%2C","2020.","%5BLLG","%2B","19%5D","MikeLewis%2CYinhanLiu%2CNamanGoyal%2CMarjanGhazvininejad%2CAbdelrahmanMohamed%2COmerLevy%2C","Ves","Stoyanov%2C","and","Luke","Zettlemoyer.","Bart%3A","Denoising","sequence-to-sequence","pre-training","for","natural","language","generation%2C","translation%2C","and","comprehension.","arXiv","preprint","arXiv%3A1910.13461","%2C","2019.","%5BLM17%5D","Ke","Li","and","Jitendra","Malik.","Learning","to","optimize","neural","nets.","arXiv","preprint","arXiv%3A1703.00441","%2C","2017.","%5BLOG","%2B","19%5D","YinhanLiu%2CMyleOtt%2CNamanGoyal%2CJingfeiDu%2CMandarJoshi%2CDanqiChen%2COmerLevy%2CMikeLewis%2C","Luke","Zettlemoyer%2C","and","Veselin","Stoyanov.","RoBERTa%3A","A","robustly","optimized","BERT","pretraining","approach.","arXiv","preprint","arXiv%3A1907.11692","%2C","2019.","%5BLPP","%2B","20%5D","Patrick","Lewis%2C","Ethan","Perez%2C","Aleksandra","Piktus%2C","Fabio","Petroni%2C","Vladimir","Karpukhin%2C","Naman","Goyal%2C","Heinrich","K","%C2%A8","uttler%2C","Mike","Lewis%2C","Wen-tau","Yih%2C","Tim","Rockt","%C2%A8","aschel%2C","Sebastian","Riedel%2C","and","Kiela","Douwe.","Retrieval-augmentedgenerationforknowledge-intensivenlptasks.","arXivpreprintarXiv%3A2005.11401","%2C","2020.","%5BLSP","%2B","18%5D","PeterJ.Liu%2CMohammadSaleh%2CEtiennePot%2CBenGoodrich%2CRyanSepassi%2CLukaszKaiser%2CandNoam","Shazeer.","GeneratingWikipediabysummarizinglongsequences.","arXivpreprintarXiv%3A1801.10198","%2C2018.","%5BLWS","%2B","20%5D","ZhuohanLi%2CEricWallace%2CShengShen%2CKevinLin%2CKurtKeutzer%2CDanKlein%2CandJosephE.Gonzalez.","Trainlarge%2Cthencompress%3A","Rethinkingmodelsizeforef%EF%AC%81cienttrainingandinferenceoftransformers%2C","2020.","%5BLXL","%2B","17%5D","Guokun","Lai%2C","Qizhe","Xie%2C","Hanxiao","Liu%2C","Yiming","Yang%2C","and","Eduard","Hovy.","Race%3A","Large-scale","reading","comprehension","dataset","from","examinations.","arXiv","preprint","arXiv%3A1704.04683","%2C","2017.","%5BLYN","%2B","20%5D","Sheng-Chieh","Lin%2C","Jheng-Hong","Yang%2C","Rodrigo","Nogueira%2C","Ming-Feng","Tsai%2C","Chuan-Ju","Wang%2C","and","Jimmy","Lin.","Tttttackling","winogrande","schemas.","arXiv","preprint","arXiv%3A2003.08380","%2C","2020.","%5BMac92%5D","David.MacKay.","Information-basedobjectivefunctionsforactivedataselection.","NeuralComputation","%2C","1992.","71","%5BMBXS17%5D","Bryan","McCann%2C","James","Bradbury%2C","Caiming","Xiong%2C","and","Richard","Socher.","Learned","in","translation%3A","Con-","textualized","word","vectors.","In","Advances","in","Neural","Information","Processing","Systems","%2C","pages","6294%E2%80%936305%2C","2017.","%5BMCCD13%5D","Tomas","Mikolov%2C","Kai","Chen%2C","Greg","Corrado%2C","and","Jeffrey","Dean.","Ef%EF%AC%81cient","estimation","of","word","representations","in","vector","space.","arXiv","preprint","arXiv%3A1301.3781","%2C","2013.","%5BMCH","%2B","16%5D","NasrinMostafazadeh%2CNathanaelChambers%2CXiaodongHe%2CDeviParikh%2CDhruvBatra%2CLucyVanderwende%2C","Pushmeet","Kohli%2C","and","James","Allen.","A","corpus","and","evaluation","framework","for","deeper","understanding","of","commonsense","stories.","arXiv","preprint","arXiv%3A1604.01696","%2C","2016.","%5BMCKS18%5D","TodorMihaylov%2CPeterClark%2CTusharKhot%2CandAshishSabharwal.Canasuitofarmorconductelectricity%3F","a","new","dataset","for","open","book","question","answering.","ArXiv","%2C","abs%2F1809.02789%2C","2018.","%5BMKAT18%5D","Sam","McCandlish%2C","Jared","Kaplan%2C","Dario","Amodei%2C","and","OpenAI","Dota","Team.","An","empirical","model","of","large-batch","training%2C","2018.","%5BMKM","%2B","94%5D","MitchellMarcus%2CGraceKim%2CMaryAnnMarcinkiewicz%2CRobertMacIntyre%2CAnnBies%2CMarkFerguson%2C","Karen","Katz%2C","and","Britta","Schasberger.","The","penn","treebank%3A","annotating","predicate","argument","structure.","In","Proceedings","of","the","workshop","on","Human","Language","Technology","%2C","pages","114%E2%80%93119.","Association","for","Computational","Linguistics%2C","1994.","%5BMKXS18%5D","Bryan","McCann%2C","Nitish","Shirish","Keskar%2C","Caiming","Xiong%2C","and","Richard","Socher.","The","natural","language","decathlon%3A","Multitask","learning","as","question","answering.","arXiv","preprint","arXiv%3A1806.08730","%2C","2018.","%5BMPL19%5D","R","Thomas","McCoy%2C","Ellie","Pavlick%2C","and","Tal","Linzen.","Right","for","the","wrong","reasons%3A","Diagnosing","syntactic","heuristics","in","natural","language","inference.","arXiv","preprint","arXiv%3A1902.01007","%2C","2019.","%5BMWZ","%2B","18%5D","Margaret","Mitchell%2C","Simone","Wu%2C","Andrew","Zaldivar%2C","Parker","Barnes%2C","Lucy","Vasserman%2C","Ben","Hutchinson%2C","Elena","Spitzer%2C","Inioluwa","Deborah","Raji%2C","and","Timnit","Gebru.","Model","cards","for","model","reporting%2C","2018.","%5BNBR20%5D","Moin","Nadeem%2C","Anna","Bethke%2C","and","Siva","Reddy.","Stereoset%3A","Measuring","stereotypical","bias","in","pretrained","language","models.","arXiv","preprint","arXiv%3A2004.09456","%2C","2020.","%5BNK19%5D","TimothyNivenandHung-YuKao.","Probingneuralnetworkcomprehensionofnaturallanguagearguments.","arXiv","preprint","arXiv%3A1907.07355","%2C","2019.","%5BNor09%5D","Peter","Norvig.","Natural","language","corpus","data%2C","2009.","%5BNvNvdG19%5D","Malvina","Nissim%2C","Rik","van","Noord%2C","and","Rob","van","der","Goot.","Fair","is","better","than","sensational%3A","Man","is","to","doctor","as","woman","is","to","doctor.","arXiv","preprint","arXiv%3A1905.09866","%2C","2019.","%5BNWD","%2B","19%5D","YixinNie%2CAdinaWilliams%2CEmilyDinan%2CMohitBansal%2CJasonWeston%2CandDouweKiela.","Adversarial","nli%3A","A","new","benchmark","for","natural","language","understanding.","arXiv","preprint","arXiv%3A1910.14599","%2C","2019.","%5BoR16%5D","University","of","Regensburg.","Fascha%2C","2016.","%5BPCC18%5D","Mohammad","Taher","Pilehvar","and","Jose","Camacho-Collados.","WIC%3A","10%2C000","example","pairs","for","evaluating","context-sensitive","representations.","arXiv","preprint","arXiv%3A1808.09121","%2C","2018.","%5BPFB18%5D","JasonPhang%2CThibaultF","%C2%B4","evry%2CandSamuelR.Bowman.","SentenceencodersonSTILTs%3A","Supplementary","training","on","intermediate","labeled-data","tasks.","arXiv","preprint","arXiv%3A1811.01088","%2C","2018.","%5BPHR","%2B","18%5D","Adam","Poliak%2C","Aparajita","Haldar%2C","Rachel","Rudinger%2C","J.","Edward","Hu%2C","Ellie","Pavlick%2C","Aaron","Steven","White%2C","and","BenjaminVanDurme.","Collectingdiversenaturallanguageinferenceproblemsforsentencerepresentation","evaluation.","In","Proceedings","of","EMNLP","%2C","2018.","%5BPKL","%2B","16%5D","Denis","Paperno%2C","Germ","%C2%B4","an","Kruszewski%2C","Angeliki","Lazaridou%2C","Quan","Ngoc","Pham%2C","Raffaella","Bernardi%2C","Sandro","Pezzelle%2C","Marco","Baroni%2C","Gemma","Boleda%2C","and","Raquel","Fern","%C2%B4","andez.","The","lambada","dataset%3A","Word","prediction","requiring","a","broad","discourse","context.","arXiv","preprint","arXiv%3A1606.06031","%2C","2016.","%5BPNZtY18%5D","MatthewE.Peters%2CMarkNeumann%2CLukeZettlemoyer%2CandWentauYih.","Dissectingcontextualword","embeddings%3A","Architecture","and","representation%2C","2018.","%5BPos18%5D","Matt","Post.","A","call","for","clarity","in","reporting","BLEU","scores.","arXiv","preprint","arXiv%3A1804.08771","%2C","2018.","72","%5BPSM14%5D","Jeffrey","Pennington%2C","Richard","Socher%2C","and","Christopher","Manning.","GloVe%3A","Global","vectors","for","word","representation.","In","Proceedings","of","the","2014","conference","on","empirical","methods","in","natural","language","processing","(EMNLP)","%2C","2014.","%5BQIA20%5D","QIANXIN.","Sa-net","on","albert","(ensemble)%2C","April","2020.","%5BQMZH19%5D","Yusu","Qian%2C","Urwa","Muaz%2C","Ben","Zhang%2C","and","Jae","Won","Hyun.","Reducing","gender","bias","in","word-level","language","models","with","a","gender-equalizing","loss","function.","arXiv","preprint","arXiv%3A1905.12801","%2C","2019.","%5BRBG11%5D","Melissa","Roemmele%2C","Cosmin","Adrian","Bejan%2C","and","Andrew","S","Gordon.","Choice","of","plausible","alternatives%3A","An","evaluation","of","commonsense","causal","reasoning.","In","2011","AAAI","Spring","Symposium","Series","%2C","2011.","%5BRCM19%5D","Siva","Reddy%2C","Danqi","Chen%2C","and","Christopher","D","Manning.","Coqa%3A","A","conversational","question","answering","challenge.","Transactions","of","the","Association","for","Computational","Linguistics","%2C","7%3A249%E2%80%93266%2C","2019.","%5BRCP","%2B","17%5D","Scott","Reed%2C","Yutian","Chen%2C","Thomas","Paine%2C","A","%C2%A8","aron","van","den","Oord%2C","SM","Eslami%2C","Danilo","Rezende%2C","Oriol","Vinyals%2CandNandodeFreitas.","Few-shotautoregressivedensityestimation%3A","Towardslearningtolearn","distributions.","arXiv","preprint","arXiv%3A1710.10304","%2C","2017.","%5BRJL18%5D","Pranav","Rajpurkar%2C","Robin","Jia%2C","and","Percy","Liang.","Know","what","you","don%E2%80%99t","know%3A","Unanswerable","questions","for","squad.","arXiv","preprint","arXiv%3A1806.03822","%2C","2018.","%5BRL16%5D","SachinRaviandHugoLarochelle.","Optimizationasamodelforfew-shotlearning.","ICLR2017(oral)","%2C","2016.","%5BRLL","%2B","19%5D","QiuRan%2CYankaiLin%2CPengLi%2CJieZhou%2CandZhiyuanLiu.","NumNet%3A","Machinereadingcomprehension","with","numerical","reasoning.","In","Proceedings","of","EMNLP","%2C","2019.","%5BRNLVD18%5D","Rachel","Rudinger%2C","Jason","Naradowsky%2C","Brian","Leonard%2C","and","Benjamin","Van","Durme.","Gender","bias","in","coreference","resolution.","arXiv","preprint","arXiv%3A1804.09301","%2C","2018.","%5BRNSS18%5D","AlecRadford%2CKarthikNarasimhan%2CTimSalimans%2CandIlyaSutskever.","Improvinglanguageunderstanding","by","generative","pre-training%2C","2018.","%5BRos12%5D","R.S.","Ross.","Guide","for","conducting","risk","assessments.","NIST","Special","Publication","%2C","2012.","%5BRRBS19%5D","Jonathan","S.","Rosenfeld%2C","Amir","Rosenfeld%2C","Yonatan","Belinkov%2C","and","Nir","Shavit.","A","constructive","prediction","of","the","generalization","error","across","scales%2C","2019.","%5BRRS20%5D","AdamRoberts%2CColinRaffel%2CandNoamShazeer.","Howmuchknowledgecanyoupackintotheparameters","of","a","language","model%3F","arXiv","preprint","arXiv%3A2002.08910","%2C","2020.","%5BRSR","%2B","19%5D","Colin","Raffel%2C","Noam","Shazeer%2C","Adam","Roberts%2C","Katherine","Lee%2C","Sharan","Narang%2C","Michael","Matena%2C","Yanqi","Zhou%2C","Wei","Li%2C","and","Peter","J.","Liu.","Exploring","the","limits","of","transfer","learning","with","a","uni%EF%AC%81ed","text-to-text","transformer%2C","2019.","%5BRWC","%2B","19%5D","Alec","Radford%2C","Jeffrey","Wu%2C","Rewon","Child%2C","David","Luan%2C","Dario","Amodei%2C","and","Ilya","Sutskever.","Language","models","are","unsupervised","multitask","learners%2C","2019.","%5BSBBC19%5D","Keisuke","Sakaguchi%2C","Ronan","Le","Bras%2C","Chandra","Bhagavatula%2C","and","Yejin","Choi.","Winogrande%3A","An","adversarial","winograd","schema","challenge","at","scale%2C","2019.","%5BSBC","%2B","19%5D","IreneSolaiman%2CMilesBrundage%2CJackClark%2CAmandaAskell%2CArielHerbert-Voss%2CJeffWu%2CAlecRadford%2C","Gretchen","Krueger%2C","Jong","Wook","Kim%2C","Sarah","Kreps%2C","Miles","McCain%2C","Alex","Newhouse%2C","Jason","Blazakis%2C","Kris","McGuf%EF%AC%81e%2C","and","Jasmine","Wang.","Release","strategies","and","the","social","impacts","of","language","models%2C","2019.","%5BSCNP19%5D","Emily","Sheng%2C","Kai-Wei","Chang%2C","Premkumar","Natarajan%2C","and","Nanyun","Peng.","The","woman","worked","as","a","babysitter%3A","On","biases","in","language","generation.","arXiv","preprint","arXiv%3A1909.01326","%2C","2019.","%5BSDCW19%5D","VictorSanh%2CLysandreDebut%2CJulienChaumond%2CandThomasWolf.","DistilBERT%2Cadistilledversionof","BERT%3A","smaller%2C","faster%2C","cheaper","and","lighter.","arXiv","preprint","arXiv%3A1910.01108","%2C","2019.","%5BSDSE19%5D","Roy","Schwartz%2C","Jesse","Dodge%2C","Noah","A.","Smith%2C","and","Oren","Etzioni.","Green","AI.","CoRR","%2C","abs%2F1907.10597%2C","2019.","%5BSHB15%5D","Rico","Sennrich%2C","Barry","Haddow%2C","and","Alexandra","Birch.","Improving","neural","machine","translation","models","with","monolingual","data.","arXiv","preprint","arXiv%3A1511.06709","%2C","2015.","73","%5BSMM","%2B","17%5D","Noam","Shazeer%2C","Azalia","Mirhoseini%2C","Krzysztof","Maziarz%2C","Andy","Davis%2C","Quoc","Le%2C","Geoffrey","Hinton%2C","and","Jeff","Dean.","Outrageously","large","neural","networks%3A","The","sparsely-gated","mixture-of-experts","layer.","arXiv","preprint","arXiv%3A1701.06538","%2C","2017.","%5BSPP","%2B","19%5D","MohammadShoeybi%2CMostofaPatwary%2CRaulPuri%2CPatrickLeGresley%2CJaredCasper%2CandBryanCatanzaro.","Megatron-lm%3A","Training","multi-billion","parameter","language","models","using","model","parallelism%2C","2019.","%5BSS20%5D","Timo","Schick","and","Hinrich","Sch","%C2%A8","utze.","Exploiting","cloze","questions","for","few-shot","text","classi%EF%AC%81cation","and","natural","language","inference.","arXiv","preprint","arXiv%3A2001.07676","%2C","2020.","%5BSTQ","%2B","19%5D","Kaitao","Song%2C","Xu","Tan%2C","Tao","Qin%2C","Jianfeng","Lu%2C","and","Tie-Yan","Liu.","MASS%3A","Masked","sequence","to","sequence","pre-training","for","language","generation.","arXiv","preprint","arXiv%3A1905.02450","%2C","2019.","%5BTFR","%2B","17%5D","JoshTobin%2CRachelFong%2CAlexRay%2CJonasSchneider%2CWojciechZaremba%2CandPieterAbbeel.","Domain","randomizationfortransferringdeepneuralnetworksfromsimulationtotherealworld.","In","2017IEEE%2FRSJ","international","conference","on","intelligent","robots","and","systems","(IROS)","%2C","pages","23%E2%80%9330.","IEEE%2C","2017.","%5BTL05%5D","Peter","D.","Turney","and","Michael","L.","Littman.","Corpus-based","learning","of","analogies","and","semantic","relations.","CoRR","%2C","abs%2Fcs%2F0508103%2C","2005.","%5BTL18%5D","Trieu","H.","Trinh","and","Quoc","V.","Le.","A","simple","method","for","commonsense","reasoning.","arXiv","preprint","arXiv%3A1806.02847","%2C","2018.","%5BTLBS03%5D","Peter","D.","Turney%2C","Michael","L.","Littman%2C","Jeffrey","Bigham%2C","and","Victor","Shnayder.","Combining","independent","modules","to","solve","multiple-choice","synonym","and","analogy","problems.","CoRR","%2C","cs.CL%2F0309035%2C","2003.","%5BTur20%5D","Project","Turing.","Microsoft","research","blog%2C","Feb","2020.","%5BVBL","%2B","16%5D","OriolVinyals%2CCharlesBlundell%2CTimothyLillicrap%2CDaanWierstra%2Cetal.","MatchingNetworksforOne","Shot","Learning.","In","Advances","in","neural","information","processing","systems","%2C","pages","3630%E2%80%933638%2C","2016.","%5BVSP","%2B","17%5D","AshishVaswani%2C","NoamShazeer%2C","NikiParmar%2C","JakobUszkoreit%2C","LlionJones%2C","AidanN.Gomez%2C","%C5%81","ukasz","Kaiser%2CandIlliaPolosukhin.","Attentionisallyouneed.","In","Advancesinneuralinformationprocessing","systems","%2C","2017.","%5BWPN","%2B","19%5D","AlexWang%2C","YadaPruksachatkun%2C","NikitaNangia%2C","AmanpreetSingh%2C","JulianMichael%2C","FelixHill%2C","Omer","Levy%2C","and","Samuel","Bowman.","Superglue%3A","A","stickier","benchmark","for","general-purpose","language","understand-","ing","systems.","In","Advances","in","Neural","Information","Processing","Systems","%2C","pages","3261%E2%80%933275%2C","2019.","%5BWXH","%2B","18%5D","YirenWang%2CYingceXia%2CTianyuHe%2CFeiTian%2CTaoQin%2CChengXiangZhai%2CandTie-YanLiu.","Multi-agent","dual","learning.","ICLR","2019","%2C","2018.","%5BXDH","%2B","19%5D","Qizhe","Xie%2C","Zihang","Dai%2C","Eduard","Hovy%2C","Minh-Thang","Luong%2C","and","Quoc","V.","Le.","Unsupervised","data","augmentation","for","consistency","training%2C","2019.","%5BYdC","%2B","19%5D","Dani","Yogatama%2C","Cyprien","de","Masson","d%E2%80%99Autume%2C","Jerome","Connor%2C","Tomas","Kocisky%2C","Mike","Chrzanowski%2C","Lingpeng","Kong%2C","Angeliki","Lazaridou%2C","Wang","Ling%2C","Lei","Yu%2C","Chris","Dyer%2C","et","al.","Learning","and","evaluating","general","linguistic","intelligence.","arXiv","preprint","arXiv%3A1901.11373","%2C","2019.","%5BYDY","%2B","19%5D","ZhilinYang%2CZihangDai%2CYimingYang%2CJaimeCarbonell%2CRuslanSalakhutdinov%2CandQuocV.Le.","XLNet%3A","Generalizedautoregressivepretrainingforlanguageunderstanding.","arXivpreprintarXiv%3A1906.08237","%2C","2019.","%5BZHB","%2B","19%5D","RowanZellers%2CAriHoltzman%2CYonatanBisk%2CAliFarhadi%2CandYejinChoi.","Hellaswag%3A","Canamachine","really","%EF%AC%81nish","your","sentence%3F","arXiv","preprint","arXiv%3A1905.07830","%2C","2019.","%5BZHR","%2B","19%5D","RowanZellers%2CAriHoltzman%2CHannahRashkin%2CYonatanBisk%2CAliFarhadi%2CFranziskaRoesner%2CandYejin","Choi.","Defending","against","neural","fake","news.","arXiv","preprint","arXiv%3A1905.12616","%2C","2019.","%5BZLL","%2B","18%5D","Sheng","Zhang%2C","Xiaodong","Liu%2C","Jingjing","Liu%2C","Jianfeng","Gao%2C","Kevin","Duh%2C","and","Benjamin","Van","Durme.","ReCoRD%3ABridgingthegapbetweenhumanand","machinecommonsensereadingcomprehension.","arXiv","preprint","arXiv%3A1810.12885","%2C","2018.","%5BZSW","%2B","19a%5D","Daniel","M.","Ziegler%2C","Nisan","Stiennon%2C","Jeffrey","Wu%2C","Tom","B.","Brown%2C","Alec","Radford%2C","Dario","Amodei%2C","Paul","Christiano%2C","and","Geoffrey","Irving.","Fine-tuning","language","models","from","human","preferences%2C","2019.","74","%5BZSW","%2B","19b%5D","DanielM.Ziegler%2CNisanStiennon%2CJeffreyWu%2CTomB.Brown%2CAlecRadford%2CDarioAmodei%2CPaulChris-","tiano%2CandGeoffreyIrving.Fine-tuninglanguagemodelsfromhumanpreferences.","ArXiv","%2Cabs%2F1909.08593%2C","2019.","75"]